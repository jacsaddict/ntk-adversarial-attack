{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "DATASET = 'mnist'\n",
    "class_num   = 10\n",
    "test_size   = None\n",
    "train_size  = 512\n",
    "image_shape = None\n",
    "\n",
    "if DATASET =='mnist':\n",
    "    image_shape = (28, 28, 1)\n",
    "elif DATASET == 'cifar10':\n",
    "    image_shape = (32, 32, 3)\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'mnist':\n",
    "    eps = 0.3\n",
    "    eps_iter_10 = (eps/10)*1.1\n",
    "    eps_iter_100 = (eps/100)*1.1\n",
    "    eps_iter_1000 = (eps/1000)*1.1\n",
    "    \n",
    "elif DATASET == 'cifar10':\n",
    "    eps = 0.03\n",
    "    eps_iter_10 = (eps/10)*1.1\n",
    "    eps_iter_100 = (eps/100)*1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_all, y_train_all, x_test_all, y_test_all = tuple(onp.array(x) for x in get_dataset(DATASET, None, None, \n",
    "                                                                                  do_flatten_and_normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "seed = 0\n",
    "x_train_all, y_train_all = shaffle(x_train_all, y_train_all, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_all = x_train_all.astype(onp.float32)\n",
    "x_test_all  = x_test_all.astype(onp.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# down sample\n",
    "x_train = x_train_all[:train_size]\n",
    "y_train = y_train_all[:train_size]\n",
    "\n",
    "x_valid = x_train_all[train_size:]\n",
    "y_valid = y_train_all[train_size:]\n",
    "\n",
    "x_test = x_test_all[:test_size]\n",
    "y_test = y_test_all[:test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, x_test = x_train.reshape((-1, *image_shape)), x_valid.reshape((-1, *image_shape)), x_test.reshape((-1, *image_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds = train_ds.shuffle(\n",
    "    100000\n",
    ").batch(\n",
    "    batch_size\n",
    ").prefetch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ds = tf.data.Dataset.from_tensor_slices((x_valid, y_valid)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = tf.keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_input = layers.Input(shape=image_shape)\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same', strides=(2, 2),\n",
    "                  kernel_initializer=tf.keras.initializers.GlorotNormal())(img_input)\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same', strides=(2, 2),\n",
    "                 kernel_initializer=tf.keras.initializers.GlorotNormal())(x)\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same', strides=(2, 2),\n",
    "                 kernel_initializer=tf.keras.initializers.GlorotNormal())(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(64, activation='relu', kernel_initializer=tf.keras.initializers.GlorotNormal())(x)\n",
    "out = layers.Dense(10, kernel_initializer=tf.keras.initializers.GlorotNormal())(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=img_input, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return 1e-2\n",
    "    elif epoch < 20:\n",
    "        return 1e-1\n",
    "    elif epoch < 60:\n",
    "        return 1e-1\n",
    "    else:\n",
    "        return 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.SGD(momentum=0.9),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 14, 14, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 4, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 140,746\n",
      "Trainable params: 140,746\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.fit(x=train_ds, validation_data=valid_ds, epochs=epochs*2, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./model_weights/mnist-simple_cnn_stride_thin_train=512-without-DA_ce.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.2019809935068713, 0.8747]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=x_test, y=y_test_all, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tangent_feature(model, xs):\n",
    "    n = len(xs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(xs)\n",
    "    subgradients = tape.jacobian(output, model.trainable_weights)\n",
    "    flattened = [tf.reshape(sg, [n, -1]) for sg in subgradients]\n",
    "    gradients = tf.concat(flattened, 1)\n",
    "    return gradients\n",
    "\n",
    "@tf.function\n",
    "def kernel(model, x, y):\n",
    "    dot = tf.reduce_sum(tf.multiply(tangent_feature(model, x), tangent_feature(model, y)))\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "@tf.function\n",
    "def targeted_fgsm(x, y_target, model, eps):\n",
    "    with tf.GradientTape() as tp:\n",
    "        tp.watch(x)\n",
    "        y = model(x)\n",
    "        loss = ce_loss(y_target, y)\n",
    "    grad = tp.gradient(loss, x)\n",
    "    return tf.clip_by_value(x - eps * tf.sign(grad), 0, 1)\n",
    "\n",
    "@tf.function\n",
    "def untargeted_fgsm(x, y_true, model, eps):\n",
    "    with tf.GradientTape() as tp:\n",
    "        tp.watch(x)\n",
    "        y = model(x)\n",
    "        loss = ce_loss(y_true, y)\n",
    "    grad = tp.gradient(loss, x)\n",
    "    return tf.clip_by_value(x + eps * tf.sign(grad), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onp.save('k_train_train.npy', k_train_train)\n",
    "# onp.save('k_test_train.npy', k_test_train)\n",
    "\n",
    "# k_train_train = onp.zeros((train_size, train_size), dtype=onp.float32)\n",
    "\n",
    "# for i in tqdm(range(train_size)):\n",
    "#     for j in range(i, train_size):\n",
    "#         k_train_train[i][j] = kernel(model, x_train[i][None], x_train[j][None])\n",
    "#         k_train_train[j][i] = k_train_train[i][j]\n",
    "        \n",
    "# k_train_train = k_train_train.astype(onp.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_train_train = onp.load('k_train_train.npy')\n",
    "k_train_train_inv = onp.load('k_train_train_inv.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def inv(k):\n",
    "#     #inverse with diag_reg\n",
    "#     return onp.linalg.inv(k + 1e-5 * onp.eye(k.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_train_train_inv = inv(k_train_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in range(train_size):\n",
    "i = 0\n",
    "x1 = tf.constant(x_test[i])\n",
    "x1 = tf.reshape(x1, (-1, *image_shape))\n",
    "with tf.GradientTape() as tp:\n",
    "    tp.watch(x1)\n",
    "    k_test_train = tf.stack([kernel(model, x1, x_train[j][None]) for j in range(train_size)],\n",
    "                            axis=-1)\n",
    "    k_test_train = tf.reshape(k_test_train, (1, train_size))\n",
    "    k_test_train = tf.cast(k_test_train, tf.float64)\n",
    "    \n",
    "    xy = tf.matmul(k_train_train_inv, y_train[:train_size].astype(onp.float64))\n",
    "    pred = tf.matmul(k_test_train, xy)\n",
    "    loss = ce_loss(y_test[i][None], pred)\n",
    "grad = tp.gradient(loss, x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = x_test[0][None] + grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f698580d320>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(ans.reshape((28, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=74024, shape=(1, 10), dtype=float32, numpy=\n",
       "array([[-15.940378 ,  -9.592758 ,   2.3942502,  -3.761365 ,  25.623034 ,\n",
       "          1.480119 ,  -3.2154405,  -3.5626764,  -0.9011345,  19.350779 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
