{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.0\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "\n",
    "import numpy as onp\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "\n",
    "from jax import lax, random\n",
    "from jax.api import grad, jit, vmap\n",
    "from jax.config import config\n",
    "from jax.experimental import optimizers\n",
    "from jax.experimental.stax import logsoftmax\n",
    "\n",
    "config.update('jax_enable_x64', True)\n",
    "\n",
    "from neural_tangents import stax\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# Attacking\n",
    "from cleverhans.utils import clip_eta, one_hot\n",
    "\n",
    "# Plotting\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from utils import *\n",
    "\n",
    "sns.set_style(style='white')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "\"\"\"\n",
    "diag_reg:\n",
    "    a scalar representing the strength of the diagonal regularization for\n",
    "    `k_train_train`, i.e. computing `k_train_train + diag_reg * I` during\n",
    "    Cholesky factorization or eigendecomposition.\n",
    "\"\"\"\n",
    "diag_reg = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'cifar10'\n",
    "class_num   = 10\n",
    "image_shape = None\n",
    "\n",
    "train_size = 45000\n",
    "valid_size = 5000\n",
    "test_size = None\n",
    "test_batch_size  = 16\n",
    "eps = 0.03\n",
    "\n",
    "batch_size = 100\n",
    "eps = 0.03\n",
    "epochs = 2000\n",
    "\n",
    "if DATASET =='mnist':\n",
    "    image_shape = (28, 28, 1)\n",
    "elif DATASET == 'cifar10':\n",
    "    image_shape = (32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_all, y_train_all, x_test_all, y_test_all = tuple(onp.array(x) for x in get_dataset(DATASET, None, None, \n",
    "                                                                                  do_flatten_and_normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "seed = 0\n",
    "x_train_all, y_train_all = shaffle(x_train_all, y_train_all, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# down sample\n",
    "x_train = x_train_all[:train_size]\n",
    "y_train = y_train_all[:train_size]\n",
    "\n",
    "x_valid = x_train_all[train_size:train_size + valid_size]\n",
    "y_valid = y_train_all[train_size:train_size + valid_size]\n",
    "\n",
    "x_test = x_test_all[:test_size]\n",
    "y_test = y_test_all[:test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, x_test = x_train.reshape((-1, *image_shape)), x_valid.reshape((-1, *image_shape)), x_test.reshape((-1, *image_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(mean, ys):\n",
    "    return onp.argmax(mean, axis=-1) == onp.argmax(ys, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvBlock(channels, W_std, b_std, strides=(1,1)):\n",
    "    return stax.serial(stax.Conv(out_chan=channels, filter_shape=(3,3), strides=strides, padding='SAME',\n",
    "                                 W_std=W_std, b_std=b_std), \n",
    "                       stax.Relu(do_backprop=True))\n",
    "\n",
    "def ConvGroup(n, channels, stride, W_std, b_std, last_stride=False):\n",
    "    blocks = []\n",
    "    if last_stride:\n",
    "        for i in range(n-1):\n",
    "            blocks += [ConvBlock(channels, W_std, b_std, stride)]\n",
    "        blocks += [ConvBlock(channels, W_std, b_std, (2, 2))]\n",
    "    \n",
    "    else:\n",
    "        for i in range(n):\n",
    "            blocks += [ConvBlock(channels, W_std, b_std, stride)]\n",
    "        \n",
    "    return stax.serial(*blocks)\n",
    "        \n",
    "def VGG19_stride(class_num=class_num):\n",
    "    \n",
    "    return stax.serial(\n",
    "        ConvGroup(n=2, channels=64 , stride=(1,1), W_std=0.1, b_std=0.18, last_stride=True),\n",
    "        ConvGroup(n=2, channels=128, stride=(1,1), W_std=0.1, b_std=0.18, last_stride=True),\n",
    "        ConvGroup(n=4, channels=256, stride=(1,1), W_std=0.1, b_std=0.18, last_stride=True),\n",
    "        ConvGroup(n=4, channels=512, stride=(1,1), W_std=0.1, b_std=0.18, last_stride=True),\n",
    "        ConvGroup(n=4, channels=512, stride=(1,1), W_std=0.1, b_std=0.18, last_stride=True),\n",
    "        stax.Flatten(),\n",
    "        stax.Dense(512), stax.Relu(do_backprop=True),\n",
    "        stax.Dense(class_num))\n",
    "\n",
    "def simple_net(class_num=class_num):\n",
    "    return stax.serial(\n",
    "        ConvGroup(n=3, channels=64 , stride=(1,1), W_std=onp.sqrt(2), b_std=0.0, last_stride=False),\n",
    "        stax.Flatten(),\n",
    "        stax.Dense(512, W_std=onp.sqrt(2)), stax.Relu(do_backprop=True),\n",
    "        stax.Dense(class_num, W_std=onp.sqrt(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = 0.18\n",
    "# W = 1.76\n",
    "\n",
    "# b_std = np.sqrt(b)\n",
    "# W_std = np.sqrt(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_fn, apply_fn, kernel_fn = stax.serial(stax.Conv(64, (5, 5), (2, 2), padding=\"SAME\", W_std=W_std, b_std=b_std), \n",
    "#                                            stax.Relu(do_backprop=True),\n",
    "#                                            stax.Conv(64, (5, 5), (2, 2), padding=\"SAME\", W_std=W_std, b_std=b_std), \n",
    "#                                            stax.Relu(do_backprop=True),\n",
    "#                                            stax.Flatten(),\n",
    "#                                            stax.Dense(384, W_std=W_std, b_std=b_std),\n",
    "#                                            stax.Relu(do_backprop=True),\n",
    "#                                            stax.Dense(192, W_std=W_std, b_std=b_std),\n",
    "#                                            stax.Relu(do_backprop=True),\n",
    "#                                            stax.Dense(class_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def l2_loss_v1(logits, labels, weighting=1):\n",
    "    \"\"\"\n",
    "    Tensorflow version of L2 loss (without sqrt)\n",
    "    \"\"\"\n",
    "    return np.sum(((logits - labels)**2) * weighting) / 2\n",
    "    \n",
    "@jit\n",
    "def l2_loss_v2(logits, lables):\n",
    "    \"\"\"\n",
    "    Normal L2 loss\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(logits - labels)\n",
    "\n",
    "@jit\n",
    "def cross_entropy_loss(logits, lables):\n",
    "    return -np.mean(logsoftmax(logits) * lables)\n",
    "    \n",
    "@jit\n",
    "def mse_loss(logits, lables):\n",
    "    return 0.5 * np.mean((logits - lables) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_fn, apply_fn, kernel_fn = simple_net(class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_fn = jit(apply_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)\n",
    "key, net_key = random.split(key)\n",
    "_, params = init_fn(net_key, (-1, 32, 32, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e0\n",
    "# training_steps = 3200\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.sgd(learning_rate)\n",
    "opt_update = jit(opt_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = jit(lambda params, x, y: -np.mean(logsoftmax(apply_fn(params, x)) * y))\n",
    "loss = lambda params, x, y: 0.5 * np.mean((apply_fn(params, x) - y) ** 2)\n",
    "grad_loss = jit(lambda params, x, y: grad(loss)(params, x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = onp.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_state = opt_init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   0: train loss 0.05752, valid loss 0.05757 train acc 0.11738 valid acc 0.11790\n",
      "epoch   1: train loss 0.04709, valid loss 0.04717 train acc 0.15482 valid acc 0.15461\n",
      "epoch   2: train loss 0.04564, valid loss 0.04571 train acc 0.17729 valid acc 0.17667\n",
      "epoch   3: train loss 0.04494, valid loss 0.04501 train acc 0.19047 valid acc 0.19307\n",
      "epoch   4: train loss 0.04447, valid loss 0.04451 train acc 0.20238 valid acc 0.20489\n",
      "epoch   5: train loss 0.04409, valid loss 0.04412 train acc 0.21222 valid acc 0.21398\n",
      "epoch   6: train loss 0.04380, valid loss 0.04382 train acc 0.22020 valid acc 0.22363\n",
      "epoch   7: train loss 0.04357, valid loss 0.04359 train acc 0.22933 valid acc 0.23001\n",
      "epoch   8: train loss 0.04337, valid loss 0.04338 train acc 0.23764 valid acc 0.23589\n",
      "epoch   9: train loss 0.04320, valid loss 0.04321 train acc 0.24327 valid acc 0.24170\n",
      "epoch  10: train loss 0.04305, valid loss 0.04306 train acc 0.24813 valid acc 0.24651\n",
      "epoch  11: train loss 0.04291, valid loss 0.04292 train acc 0.25387 valid acc 0.25248\n",
      "epoch  12: train loss 0.04278, valid loss 0.04279 train acc 0.25973 valid acc 0.25695\n",
      "epoch  13: train loss 0.04266, valid loss 0.04267 train acc 0.26384 valid acc 0.26000\n",
      "epoch  14: train loss 0.04254, valid loss 0.04256 train acc 0.26958 valid acc 0.26499\n",
      "epoch  15: train loss 0.04243, valid loss 0.04244 train acc 0.27500 valid acc 0.26994\n",
      "epoch  16: train loss 0.04233, valid loss 0.04235 train acc 0.27904 valid acc 0.27472\n",
      "epoch  17: train loss 0.04224, valid loss 0.04225 train acc 0.28320 valid acc 0.27885\n",
      "epoch  18: train loss 0.04214, valid loss 0.04216 train acc 0.28571 valid acc 0.28176\n",
      "epoch  19: train loss 0.04206, valid loss 0.04207 train acc 0.28916 valid acc 0.28477\n",
      "epoch  20: train loss 0.04197, valid loss 0.04199 train acc 0.29416 valid acc 0.28815\n",
      "epoch  21: train loss 0.04189, valid loss 0.04191 train acc 0.29807 valid acc 0.29089\n",
      "epoch  22: train loss 0.04181, valid loss 0.04183 train acc 0.30227 valid acc 0.29390\n",
      "epoch  23: train loss 0.04174, valid loss 0.04176 train acc 0.30471 valid acc 0.29637\n",
      "epoch  24: train loss 0.04166, valid loss 0.04168 train acc 0.30727 valid acc 0.29922\n",
      "epoch  25: train loss 0.04159, valid loss 0.04161 train acc 0.31144 valid acc 0.30218\n",
      "epoch  26: train loss 0.04152, valid loss 0.04154 train acc 0.31362 valid acc 0.30582\n",
      "epoch  27: train loss 0.04145, valid loss 0.04147 train acc 0.31693 valid acc 0.30873\n",
      "epoch  28: train loss 0.04139, valid loss 0.04141 train acc 0.32116 valid acc 0.31232\n",
      "epoch  29: train loss 0.04132, valid loss 0.04135 train acc 0.32380 valid acc 0.31453\n",
      "epoch  30: train loss 0.04126, valid loss 0.04129 train acc 0.32556 valid acc 0.31709\n",
      "epoch  31: train loss 0.04120, valid loss 0.04123 train acc 0.32849 valid acc 0.32004\n",
      "epoch  32: train loss 0.04114, valid loss 0.04117 train acc 0.33062 valid acc 0.32242\n",
      "epoch  33: train loss 0.04108, valid loss 0.04111 train acc 0.33269 valid acc 0.32502\n",
      "epoch  34: train loss 0.04103, valid loss 0.04106 train acc 0.33489 valid acc 0.32610\n",
      "epoch  35: train loss 0.04097, valid loss 0.04100 train acc 0.33687 valid acc 0.32836\n",
      "epoch  36: train loss 0.04092, valid loss 0.04095 train acc 0.33982 valid acc 0.32880\n",
      "epoch  37: train loss 0.04086, valid loss 0.04090 train acc 0.34187 valid acc 0.32990\n",
      "epoch  38: train loss 0.04081, valid loss 0.04085 train acc 0.34422 valid acc 0.33068\n",
      "epoch  39: train loss 0.04076, valid loss 0.04080 train acc 0.34624 valid acc 0.33148\n",
      "epoch  40: train loss 0.04071, valid loss 0.04075 train acc 0.34669 valid acc 0.33257\n",
      "epoch  41: train loss 0.04066, valid loss 0.04070 train acc 0.34962 valid acc 0.33488\n",
      "epoch  42: train loss 0.04062, valid loss 0.04066 train acc 0.35149 valid acc 0.33618\n",
      "epoch  43: train loss 0.04057, valid loss 0.04061 train acc 0.35249 valid acc 0.33770\n",
      "epoch  44: train loss 0.04052, valid loss 0.04056 train acc 0.35409 valid acc 0.33854\n",
      "epoch  45: train loss 0.04048, valid loss 0.04052 train acc 0.35518 valid acc 0.33913\n",
      "epoch  46: train loss 0.04043, valid loss 0.04048 train acc 0.35742 valid acc 0.34095\n",
      "epoch  47: train loss 0.04039, valid loss 0.04043 train acc 0.35869 valid acc 0.34291\n",
      "epoch  48: train loss 0.04035, valid loss 0.04039 train acc 0.36053 valid acc 0.34499\n",
      "epoch  49: train loss 0.04030, valid loss 0.04035 train acc 0.36131 valid acc 0.34678\n",
      "epoch  50: train loss 0.04026, valid loss 0.04031 train acc 0.36367 valid acc 0.34833\n",
      "epoch  51: train loss 0.04022, valid loss 0.04027 train acc 0.36482 valid acc 0.34963\n",
      "epoch  52: train loss 0.04018, valid loss 0.04024 train acc 0.36716 valid acc 0.35054\n",
      "epoch  53: train loss 0.04014, valid loss 0.04020 train acc 0.36742 valid acc 0.35267\n",
      "epoch  54: train loss 0.04010, valid loss 0.04016 train acc 0.36973 valid acc 0.35355\n",
      "epoch  55: train loss 0.04007, valid loss 0.04012 train acc 0.37160 valid acc 0.35414\n",
      "epoch  56: train loss 0.04003, valid loss 0.04008 train acc 0.37176 valid acc 0.35552\n",
      "epoch  57: train loss 0.03999, valid loss 0.04005 train acc 0.37267 valid acc 0.35622\n",
      "epoch  58: train loss 0.03995, valid loss 0.04002 train acc 0.37478 valid acc 0.35671\n",
      "epoch  59: train loss 0.03992, valid loss 0.03998 train acc 0.37629 valid acc 0.35786\n",
      "epoch  60: train loss 0.03988, valid loss 0.03995 train acc 0.37758 valid acc 0.35873\n",
      "epoch  61: train loss 0.03985, valid loss 0.03991 train acc 0.37818 valid acc 0.36011\n",
      "epoch  62: train loss 0.03981, valid loss 0.03988 train acc 0.37896 valid acc 0.36113\n",
      "epoch  63: train loss 0.03978, valid loss 0.03985 train acc 0.38089 valid acc 0.36090\n",
      "epoch  64: train loss 0.03975, valid loss 0.03981 train acc 0.38122 valid acc 0.36236\n",
      "epoch  65: train loss 0.03972, valid loss 0.03978 train acc 0.38204 valid acc 0.36314\n",
      "epoch  66: train loss 0.03968, valid loss 0.03975 train acc 0.38242 valid acc 0.36351\n",
      "epoch  67: train loss 0.03965, valid loss 0.03972 train acc 0.38356 valid acc 0.36433\n",
      "epoch  68: train loss 0.03962, valid loss 0.03969 train acc 0.38469 valid acc 0.36477\n",
      "epoch  69: train loss 0.03959, valid loss 0.03966 train acc 0.38591 valid acc 0.36534\n",
      "epoch  70: train loss 0.03956, valid loss 0.03963 train acc 0.38636 valid acc 0.36627\n",
      "epoch  71: train loss 0.03953, valid loss 0.03960 train acc 0.38709 valid acc 0.36690\n",
      "epoch  72: train loss 0.03950, valid loss 0.03957 train acc 0.38847 valid acc 0.36724\n",
      "epoch  73: train loss 0.03947, valid loss 0.03954 train acc 0.38893 valid acc 0.36804\n",
      "epoch  74: train loss 0.03944, valid loss 0.03952 train acc 0.38940 valid acc 0.36886\n",
      "epoch  75: train loss 0.03941, valid loss 0.03949 train acc 0.39116 valid acc 0.36950\n",
      "epoch  76: train loss 0.03938, valid loss 0.03946 train acc 0.39160 valid acc 0.36988\n",
      "epoch  77: train loss 0.03935, valid loss 0.03944 train acc 0.39176 valid acc 0.37107\n",
      "epoch  78: train loss 0.03933, valid loss 0.03941 train acc 0.39327 valid acc 0.37173\n",
      "epoch  79: train loss 0.03930, valid loss 0.03939 train acc 0.39271 valid acc 0.37212\n",
      "epoch  80: train loss 0.03927, valid loss 0.03936 train acc 0.39462 valid acc 0.37304\n",
      "epoch  81: train loss 0.03924, valid loss 0.03933 train acc 0.39567 valid acc 0.37364\n",
      "epoch  82: train loss 0.03922, valid loss 0.03931 train acc 0.39607 valid acc 0.37463\n",
      "epoch  83: train loss 0.03919, valid loss 0.03928 train acc 0.39627 valid acc 0.37510\n",
      "epoch  84: train loss 0.03916, valid loss 0.03926 train acc 0.39722 valid acc 0.37561\n",
      "epoch  85: train loss 0.03914, valid loss 0.03923 train acc 0.39820 valid acc 0.37625\n",
      "epoch  86: train loss 0.03911, valid loss 0.03921 train acc 0.39944 valid acc 0.37667\n",
      "epoch  87: train loss 0.03909, valid loss 0.03918 train acc 0.39980 valid acc 0.37739\n",
      "epoch  88: train loss 0.03906, valid loss 0.03917 train acc 0.40033 valid acc 0.37779\n",
      "epoch  89: train loss 0.03904, valid loss 0.03914 train acc 0.40162 valid acc 0.37790\n",
      "epoch  90: train loss 0.03901, valid loss 0.03912 train acc 0.40236 valid acc 0.37870\n",
      "epoch  91: train loss 0.03899, valid loss 0.03910 train acc 0.40331 valid acc 0.37876\n",
      "epoch  92: train loss 0.03897, valid loss 0.03907 train acc 0.40378 valid acc 0.37934\n",
      "epoch  93: train loss 0.03894, valid loss 0.03906 train acc 0.40389 valid acc 0.37944\n",
      "epoch  94: train loss 0.03892, valid loss 0.03903 train acc 0.40518 valid acc 0.38059\n",
      "epoch  95: train loss 0.03890, valid loss 0.03901 train acc 0.40449 valid acc 0.38057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  96: train loss 0.03887, valid loss 0.03899 train acc 0.40622 valid acc 0.38131\n",
      "epoch  97: train loss 0.03885, valid loss 0.03897 train acc 0.40684 valid acc 0.38153\n",
      "epoch  98: train loss 0.03883, valid loss 0.03895 train acc 0.40758 valid acc 0.38174\n",
      "epoch  99: train loss 0.03880, valid loss 0.03893 train acc 0.40727 valid acc 0.38236\n",
      "epoch 100: train loss 0.03878, valid loss 0.03891 train acc 0.40840 valid acc 0.38270\n",
      "epoch 101: train loss 0.03876, valid loss 0.03889 train acc 0.40909 valid acc 0.38327\n",
      "epoch 102: train loss 0.03874, valid loss 0.03887 train acc 0.40962 valid acc 0.38317\n",
      "epoch 103: train loss 0.03872, valid loss 0.03885 train acc 0.41007 valid acc 0.38445\n",
      "epoch 104: train loss 0.03870, valid loss 0.03883 train acc 0.41153 valid acc 0.38458\n",
      "epoch 105: train loss 0.03868, valid loss 0.03881 train acc 0.41100 valid acc 0.38481\n",
      "epoch 106: train loss 0.03865, valid loss 0.03879 train acc 0.41231 valid acc 0.38538\n",
      "epoch 107: train loss 0.03864, valid loss 0.03877 train acc 0.41213 valid acc 0.38618\n",
      "epoch 108: train loss 0.03861, valid loss 0.03876 train acc 0.41264 valid acc 0.38661\n",
      "epoch 109: train loss 0.03860, valid loss 0.03874 train acc 0.41289 valid acc 0.38680\n",
      "epoch 110: train loss 0.03857, valid loss 0.03872 train acc 0.41371 valid acc 0.38819\n",
      "epoch 111: train loss 0.03856, valid loss 0.03870 train acc 0.41420 valid acc 0.38849\n",
      "epoch 112: train loss 0.03854, valid loss 0.03869 train acc 0.41509 valid acc 0.38868\n",
      "epoch 113: train loss 0.03852, valid loss 0.03866 train acc 0.41498 valid acc 0.38942\n",
      "epoch 114: train loss 0.03850, valid loss 0.03865 train acc 0.41544 valid acc 0.39039\n",
      "epoch 115: train loss 0.03848, valid loss 0.03863 train acc 0.41604 valid acc 0.39061\n",
      "epoch 116: train loss 0.03846, valid loss 0.03861 train acc 0.41667 valid acc 0.39121\n",
      "epoch 117: train loss 0.03844, valid loss 0.03859 train acc 0.41647 valid acc 0.39244\n",
      "epoch 118: train loss 0.03842, valid loss 0.03858 train acc 0.41720 valid acc 0.39200\n",
      "epoch 119: train loss 0.03840, valid loss 0.03856 train acc 0.41800 valid acc 0.39346\n",
      "epoch 120: train loss 0.03839, valid loss 0.03854 train acc 0.41862 valid acc 0.39341\n",
      "epoch 121: train loss 0.03836, valid loss 0.03853 train acc 0.41942 valid acc 0.39399\n",
      "epoch 122: train loss 0.03835, valid loss 0.03851 train acc 0.41971 valid acc 0.39460\n",
      "epoch 123: train loss 0.03833, valid loss 0.03849 train acc 0.41989 valid acc 0.39463\n",
      "epoch 124: train loss 0.03831, valid loss 0.03848 train acc 0.42031 valid acc 0.39516\n",
      "epoch 125: train loss 0.03829, valid loss 0.03846 train acc 0.42129 valid acc 0.39589\n",
      "epoch 126: train loss 0.03828, valid loss 0.03844 train acc 0.42138 valid acc 0.39632\n",
      "epoch 127: train loss 0.03826, valid loss 0.03843 train acc 0.42227 valid acc 0.39608\n",
      "epoch 128: train loss 0.03824, valid loss 0.03841 train acc 0.42273 valid acc 0.39666\n",
      "epoch 129: train loss 0.03822, valid loss 0.03839 train acc 0.42236 valid acc 0.39662\n",
      "epoch 130: train loss 0.03820, valid loss 0.03838 train acc 0.42233 valid acc 0.39702\n",
      "epoch 131: train loss 0.03819, valid loss 0.03837 train acc 0.42422 valid acc 0.39708\n",
      "epoch 132: train loss 0.03817, valid loss 0.03835 train acc 0.42433 valid acc 0.39765\n",
      "epoch 133: train loss 0.03815, valid loss 0.03833 train acc 0.42356 valid acc 0.39783\n",
      "epoch 134: train loss 0.03814, valid loss 0.03832 train acc 0.42498 valid acc 0.39805\n",
      "epoch 135: train loss 0.03812, valid loss 0.03830 train acc 0.42522 valid acc 0.39891\n",
      "epoch 136: train loss 0.03810, valid loss 0.03829 train acc 0.42527 valid acc 0.39914\n",
      "epoch 137: train loss 0.03809, valid loss 0.03827 train acc 0.42564 valid acc 0.40002\n",
      "epoch 138: train loss 0.03807, valid loss 0.03826 train acc 0.42613 valid acc 0.40016\n",
      "epoch 139: train loss 0.03805, valid loss 0.03824 train acc 0.42691 valid acc 0.40075\n",
      "epoch 140: train loss 0.03804, valid loss 0.03823 train acc 0.42649 valid acc 0.40068\n",
      "epoch 141: train loss 0.03802, valid loss 0.03821 train acc 0.42764 valid acc 0.40172\n",
      "epoch 142: train loss 0.03800, valid loss 0.03820 train acc 0.42833 valid acc 0.40252\n",
      "epoch 143: train loss 0.03798, valid loss 0.03818 train acc 0.42811 valid acc 0.40227\n",
      "epoch 144: train loss 0.03797, valid loss 0.03817 train acc 0.42847 valid acc 0.40373\n",
      "epoch 145: train loss 0.03795, valid loss 0.03815 train acc 0.42807 valid acc 0.40374\n",
      "epoch 146: train loss 0.03794, valid loss 0.03814 train acc 0.42940 valid acc 0.40422\n",
      "epoch 147: train loss 0.03792, valid loss 0.03812 train acc 0.43002 valid acc 0.40473\n",
      "epoch 148: train loss 0.03791, valid loss 0.03811 train acc 0.43071 valid acc 0.40542\n",
      "epoch 149: train loss 0.03789, valid loss 0.03809 train acc 0.43031 valid acc 0.40516\n",
      "epoch 150: train loss 0.03787, valid loss 0.03808 train acc 0.43167 valid acc 0.40603\n",
      "epoch 151: train loss 0.03786, valid loss 0.03806 train acc 0.43124 valid acc 0.40592\n",
      "epoch 152: train loss 0.03784, valid loss 0.03805 train acc 0.43227 valid acc 0.40666\n",
      "epoch 153: train loss 0.03783, valid loss 0.03804 train acc 0.43280 valid acc 0.40670\n",
      "epoch 154: train loss 0.03781, valid loss 0.03802 train acc 0.43200 valid acc 0.40733\n",
      "epoch 155: train loss 0.03780, valid loss 0.03801 train acc 0.43329 valid acc 0.40742\n",
      "epoch 156: train loss 0.03778, valid loss 0.03799 train acc 0.43331 valid acc 0.40856\n",
      "epoch 157: train loss 0.03777, valid loss 0.03798 train acc 0.43424 valid acc 0.40877\n",
      "epoch 158: train loss 0.03775, valid loss 0.03797 train acc 0.43411 valid acc 0.40918\n",
      "epoch 159: train loss 0.03774, valid loss 0.03795 train acc 0.43429 valid acc 0.40947\n",
      "epoch 160: train loss 0.03772, valid loss 0.03794 train acc 0.43480 valid acc 0.40992\n",
      "epoch 161: train loss 0.03771, valid loss 0.03793 train acc 0.43522 valid acc 0.40994\n",
      "epoch 162: train loss 0.03770, valid loss 0.03791 train acc 0.43578 valid acc 0.41060\n",
      "epoch 163: train loss 0.03768, valid loss 0.03789 train acc 0.43511 valid acc 0.41129\n",
      "epoch 164: train loss 0.03767, valid loss 0.03788 train acc 0.43631 valid acc 0.41143\n",
      "epoch 165: train loss 0.03765, valid loss 0.03787 train acc 0.43556 valid acc 0.41183\n",
      "epoch 166: train loss 0.03764, valid loss 0.03786 train acc 0.43736 valid acc 0.41254\n",
      "epoch 167: train loss 0.03762, valid loss 0.03784 train acc 0.43736 valid acc 0.41312\n",
      "epoch 168: train loss 0.03761, valid loss 0.03783 train acc 0.43691 valid acc 0.41369\n",
      "epoch 169: train loss 0.03759, valid loss 0.03782 train acc 0.43713 valid acc 0.41368\n",
      "epoch 170: train loss 0.03758, valid loss 0.03780 train acc 0.43840 valid acc 0.41457\n",
      "epoch 171: train loss 0.03757, valid loss 0.03779 train acc 0.43787 valid acc 0.41498\n",
      "epoch 172: train loss 0.03755, valid loss 0.03778 train acc 0.43896 valid acc 0.41553\n",
      "epoch 173: train loss 0.03754, valid loss 0.03777 train acc 0.43889 valid acc 0.41560\n",
      "epoch 174: train loss 0.03753, valid loss 0.03775 train acc 0.43967 valid acc 0.41639\n",
      "epoch 175: train loss 0.03751, valid loss 0.03774 train acc 0.43931 valid acc 0.41660\n",
      "epoch 176: train loss 0.03750, valid loss 0.03773 train acc 0.43984 valid acc 0.41759\n",
      "epoch 177: train loss 0.03748, valid loss 0.03772 train acc 0.44049 valid acc 0.41700\n",
      "epoch 178: train loss 0.03747, valid loss 0.03770 train acc 0.44104 valid acc 0.41784\n",
      "epoch 179: train loss 0.03746, valid loss 0.03769 train acc 0.44102 valid acc 0.41850\n",
      "epoch 180: train loss 0.03744, valid loss 0.03767 train acc 0.44124 valid acc 0.41858\n",
      "epoch 181: train loss 0.03743, valid loss 0.03766 train acc 0.44098 valid acc 0.41894\n",
      "epoch 182: train loss 0.03742, valid loss 0.03765 train acc 0.44038 valid acc 0.41952\n",
      "epoch 183: train loss 0.03740, valid loss 0.03764 train acc 0.44222 valid acc 0.41970\n",
      "epoch 184: train loss 0.03739, valid loss 0.03763 train acc 0.44311 valid acc 0.42032\n",
      "epoch 185: train loss 0.03738, valid loss 0.03761 train acc 0.44271 valid acc 0.42101\n",
      "epoch 186: train loss 0.03737, valid loss 0.03760 train acc 0.44336 valid acc 0.42100\n",
      "epoch 187: train loss 0.03735, valid loss 0.03759 train acc 0.44293 valid acc 0.42165\n",
      "epoch 188: train loss 0.03734, valid loss 0.03758 train acc 0.44373 valid acc 0.42224\n",
      "epoch 189: train loss 0.03732, valid loss 0.03757 train acc 0.44460 valid acc 0.42238\n",
      "epoch 190: train loss 0.03731, valid loss 0.03755 train acc 0.44478 valid acc 0.42300\n",
      "epoch 191: train loss 0.03730, valid loss 0.03754 train acc 0.44398 valid acc 0.42350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 192: train loss 0.03728, valid loss 0.03753 train acc 0.44429 valid acc 0.42396\n",
      "epoch 193: train loss 0.03727, valid loss 0.03752 train acc 0.44598 valid acc 0.42420\n",
      "epoch 194: train loss 0.03726, valid loss 0.03751 train acc 0.44564 valid acc 0.42444\n",
      "epoch 195: train loss 0.03725, valid loss 0.03749 train acc 0.44660 valid acc 0.42467\n",
      "epoch 196: train loss 0.03723, valid loss 0.03748 train acc 0.44600 valid acc 0.42540\n",
      "epoch 197: train loss 0.03722, valid loss 0.03747 train acc 0.44731 valid acc 0.42529\n",
      "epoch 198: train loss 0.03721, valid loss 0.03746 train acc 0.44636 valid acc 0.42561\n",
      "epoch 199: train loss 0.03719, valid loss 0.03745 train acc 0.44771 valid acc 0.42608\n",
      "epoch 200: train loss 0.03718, valid loss 0.03744 train acc 0.44822 valid acc 0.42660\n",
      "epoch 201: train loss 0.03717, valid loss 0.03743 train acc 0.44827 valid acc 0.42684\n",
      "epoch 202: train loss 0.03716, valid loss 0.03741 train acc 0.44911 valid acc 0.42726\n",
      "epoch 203: train loss 0.03714, valid loss 0.03741 train acc 0.44913 valid acc 0.42712\n",
      "epoch 204: train loss 0.03713, valid loss 0.03739 train acc 0.44942 valid acc 0.42753\n",
      "epoch 205: train loss 0.03712, valid loss 0.03738 train acc 0.44889 valid acc 0.42839\n",
      "epoch 206: train loss 0.03710, valid loss 0.03737 train acc 0.44953 valid acc 0.42812\n",
      "epoch 207: train loss 0.03709, valid loss 0.03736 train acc 0.44991 valid acc 0.42811\n",
      "epoch 208: train loss 0.03708, valid loss 0.03734 train acc 0.45142 valid acc 0.42888\n",
      "epoch 209: train loss 0.03707, valid loss 0.03734 train acc 0.45031 valid acc 0.42866\n",
      "epoch 210: train loss 0.03706, valid loss 0.03732 train acc 0.45080 valid acc 0.42903\n",
      "epoch 211: train loss 0.03704, valid loss 0.03731 train acc 0.45084 valid acc 0.42925\n",
      "epoch 212: train loss 0.03703, valid loss 0.03730 train acc 0.45191 valid acc 0.42982\n",
      "epoch 213: train loss 0.03702, valid loss 0.03729 train acc 0.45231 valid acc 0.43008\n",
      "epoch 214: train loss 0.03701, valid loss 0.03728 train acc 0.45229 valid acc 0.43028\n",
      "epoch 215: train loss 0.03700, valid loss 0.03727 train acc 0.45171 valid acc 0.43007\n",
      "epoch 216: train loss 0.03698, valid loss 0.03726 train acc 0.45287 valid acc 0.43108\n",
      "epoch 217: train loss 0.03697, valid loss 0.03725 train acc 0.45307 valid acc 0.43078\n",
      "epoch 218: train loss 0.03696, valid loss 0.03724 train acc 0.45320 valid acc 0.43142\n",
      "epoch 219: train loss 0.03695, valid loss 0.03722 train acc 0.45353 valid acc 0.43189\n",
      "epoch 220: train loss 0.03693, valid loss 0.03722 train acc 0.45371 valid acc 0.43202\n",
      "epoch 221: train loss 0.03692, valid loss 0.03720 train acc 0.45380 valid acc 0.43217\n",
      "epoch 222: train loss 0.03691, valid loss 0.03719 train acc 0.45469 valid acc 0.43202\n",
      "epoch 223: train loss 0.03690, valid loss 0.03718 train acc 0.45349 valid acc 0.43261\n",
      "epoch 224: train loss 0.03689, valid loss 0.03717 train acc 0.45507 valid acc 0.43280\n",
      "epoch 225: train loss 0.03688, valid loss 0.03716 train acc 0.45524 valid acc 0.43288\n",
      "epoch 226: train loss 0.03687, valid loss 0.03715 train acc 0.45542 valid acc 0.43379\n",
      "epoch 227: train loss 0.03685, valid loss 0.03714 train acc 0.45560 valid acc 0.43376\n",
      "epoch 228: train loss 0.03684, valid loss 0.03713 train acc 0.45578 valid acc 0.43380\n",
      "epoch 229: train loss 0.03683, valid loss 0.03712 train acc 0.45582 valid acc 0.43434\n",
      "epoch 230: train loss 0.03682, valid loss 0.03711 train acc 0.45727 valid acc 0.43450\n",
      "epoch 231: train loss 0.03681, valid loss 0.03710 train acc 0.45658 valid acc 0.43488\n",
      "epoch 232: train loss 0.03679, valid loss 0.03709 train acc 0.45689 valid acc 0.43499\n",
      "epoch 233: train loss 0.03679, valid loss 0.03708 train acc 0.45751 valid acc 0.43542\n",
      "epoch 234: train loss 0.03677, valid loss 0.03707 train acc 0.45782 valid acc 0.43607\n",
      "epoch 235: train loss 0.03676, valid loss 0.03706 train acc 0.45742 valid acc 0.43576\n",
      "epoch 236: train loss 0.03675, valid loss 0.03705 train acc 0.45767 valid acc 0.43603\n",
      "epoch 237: train loss 0.03674, valid loss 0.03704 train acc 0.45818 valid acc 0.43632\n",
      "epoch 238: train loss 0.03673, valid loss 0.03703 train acc 0.45918 valid acc 0.43641\n",
      "epoch 239: train loss 0.03671, valid loss 0.03702 train acc 0.45858 valid acc 0.43712\n",
      "epoch 240: train loss 0.03670, valid loss 0.03701 train acc 0.45829 valid acc 0.43711\n",
      "epoch 241: train loss 0.03669, valid loss 0.03700 train acc 0.45871 valid acc 0.43738\n",
      "epoch 242: train loss 0.03668, valid loss 0.03699 train acc 0.45991 valid acc 0.43807\n",
      "epoch 243: train loss 0.03667, valid loss 0.03698 train acc 0.45942 valid acc 0.43781\n",
      "epoch 244: train loss 0.03666, valid loss 0.03697 train acc 0.46013 valid acc 0.43807\n",
      "epoch 245: train loss 0.03665, valid loss 0.03696 train acc 0.46020 valid acc 0.43840\n",
      "epoch 246: train loss 0.03664, valid loss 0.03695 train acc 0.46038 valid acc 0.43865\n",
      "epoch 247: train loss 0.03663, valid loss 0.03694 train acc 0.46091 valid acc 0.43863\n",
      "epoch 248: train loss 0.03662, valid loss 0.03694 train acc 0.46180 valid acc 0.43959\n",
      "epoch 249: train loss 0.03660, valid loss 0.03693 train acc 0.46173 valid acc 0.43989\n",
      "epoch 250: train loss 0.03659, valid loss 0.03692 train acc 0.46153 valid acc 0.43988\n",
      "epoch 251: train loss 0.03658, valid loss 0.03691 train acc 0.46222 valid acc 0.44022\n",
      "epoch 252: train loss 0.03657, valid loss 0.03690 train acc 0.46251 valid acc 0.44017\n",
      "epoch 253: train loss 0.03656, valid loss 0.03689 train acc 0.46238 valid acc 0.44088\n",
      "epoch 254: train loss 0.03655, valid loss 0.03688 train acc 0.46287 valid acc 0.44096\n",
      "epoch 255: train loss 0.03654, valid loss 0.03687 train acc 0.46291 valid acc 0.44184\n",
      "epoch 256: train loss 0.03653, valid loss 0.03686 train acc 0.46304 valid acc 0.44214\n",
      "epoch 257: train loss 0.03652, valid loss 0.03685 train acc 0.46387 valid acc 0.44193\n",
      "epoch 258: train loss 0.03651, valid loss 0.03685 train acc 0.46493 valid acc 0.44228\n",
      "epoch 259: train loss 0.03650, valid loss 0.03684 train acc 0.46402 valid acc 0.44249\n",
      "epoch 260: train loss 0.03648, valid loss 0.03682 train acc 0.46464 valid acc 0.44295\n",
      "epoch 261: train loss 0.03648, valid loss 0.03682 train acc 0.46553 valid acc 0.44326\n",
      "epoch 262: train loss 0.03646, valid loss 0.03681 train acc 0.46527 valid acc 0.44343\n",
      "epoch 263: train loss 0.03646, valid loss 0.03680 train acc 0.46531 valid acc 0.44427\n",
      "epoch 264: train loss 0.03644, valid loss 0.03679 train acc 0.46520 valid acc 0.44420\n",
      "epoch 265: train loss 0.03643, valid loss 0.03678 train acc 0.46538 valid acc 0.44427\n",
      "epoch 266: train loss 0.03642, valid loss 0.03677 train acc 0.46664 valid acc 0.44475\n",
      "epoch 267: train loss 0.03641, valid loss 0.03676 train acc 0.46622 valid acc 0.44551\n",
      "epoch 268: train loss 0.03640, valid loss 0.03675 train acc 0.46622 valid acc 0.44519\n",
      "epoch 269: train loss 0.03639, valid loss 0.03674 train acc 0.46718 valid acc 0.44583\n",
      "epoch 270: train loss 0.03638, valid loss 0.03674 train acc 0.46673 valid acc 0.44603\n",
      "epoch 271: train loss 0.03637, valid loss 0.03673 train acc 0.46722 valid acc 0.44598\n",
      "epoch 272: train loss 0.03636, valid loss 0.03672 train acc 0.46696 valid acc 0.44638\n",
      "epoch 273: train loss 0.03635, valid loss 0.03671 train acc 0.46744 valid acc 0.44720\n",
      "epoch 274: train loss 0.03634, valid loss 0.03670 train acc 0.46704 valid acc 0.44705\n",
      "epoch 275: train loss 0.03633, valid loss 0.03669 train acc 0.46796 valid acc 0.44729\n",
      "epoch 276: train loss 0.03632, valid loss 0.03668 train acc 0.46829 valid acc 0.44739\n",
      "epoch 277: train loss 0.03631, valid loss 0.03668 train acc 0.46871 valid acc 0.44769\n",
      "epoch 278: train loss 0.03630, valid loss 0.03666 train acc 0.46898 valid acc 0.44828\n",
      "epoch 279: train loss 0.03629, valid loss 0.03666 train acc 0.46924 valid acc 0.44847\n",
      "epoch 280: train loss 0.03628, valid loss 0.03665 train acc 0.46940 valid acc 0.44842\n",
      "epoch 281: train loss 0.03627, valid loss 0.03664 train acc 0.46947 valid acc 0.44904\n",
      "epoch 282: train loss 0.03626, valid loss 0.03663 train acc 0.46929 valid acc 0.44908\n",
      "epoch 283: train loss 0.03625, valid loss 0.03662 train acc 0.46938 valid acc 0.44962\n",
      "epoch 284: train loss 0.03624, valid loss 0.03661 train acc 0.46991 valid acc 0.44907\n",
      "epoch 285: train loss 0.03623, valid loss 0.03660 train acc 0.47033 valid acc 0.44950\n",
      "epoch 286: train loss 0.03622, valid loss 0.03660 train acc 0.46987 valid acc 0.44997\n",
      "epoch 287: train loss 0.03621, valid loss 0.03659 train acc 0.47089 valid acc 0.45003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 288: train loss 0.03620, valid loss 0.03658 train acc 0.47036 valid acc 0.45051\n",
      "epoch 289: train loss 0.03619, valid loss 0.03657 train acc 0.47082 valid acc 0.45037\n",
      "epoch 290: train loss 0.03618, valid loss 0.03656 train acc 0.47160 valid acc 0.45095\n",
      "epoch 291: train loss 0.03617, valid loss 0.03655 train acc 0.47211 valid acc 0.45140\n",
      "epoch 292: train loss 0.03616, valid loss 0.03654 train acc 0.47200 valid acc 0.45156\n",
      "epoch 293: train loss 0.03615, valid loss 0.03654 train acc 0.47249 valid acc 0.45128\n",
      "epoch 294: train loss 0.03614, valid loss 0.03653 train acc 0.47218 valid acc 0.45182\n",
      "epoch 295: train loss 0.03613, valid loss 0.03652 train acc 0.47193 valid acc 0.45212\n",
      "epoch 296: train loss 0.03612, valid loss 0.03651 train acc 0.47249 valid acc 0.45228\n",
      "epoch 297: train loss 0.03611, valid loss 0.03650 train acc 0.47289 valid acc 0.45244\n",
      "epoch 298: train loss 0.03610, valid loss 0.03649 train acc 0.47278 valid acc 0.45245\n",
      "epoch 299: train loss 0.03610, valid loss 0.03648 train acc 0.47340 valid acc 0.45285\n",
      "epoch 300: train loss 0.03608, valid loss 0.03648 train acc 0.47358 valid acc 0.45324\n",
      "epoch 301: train loss 0.03608, valid loss 0.03647 train acc 0.47409 valid acc 0.45288\n",
      "epoch 302: train loss 0.03607, valid loss 0.03646 train acc 0.47362 valid acc 0.45345\n",
      "epoch 303: train loss 0.03606, valid loss 0.03645 train acc 0.47422 valid acc 0.45342\n",
      "epoch 304: train loss 0.03605, valid loss 0.03644 train acc 0.47493 valid acc 0.45387\n",
      "epoch 305: train loss 0.03604, valid loss 0.03643 train acc 0.47529 valid acc 0.45395\n",
      "epoch 306: train loss 0.03603, valid loss 0.03643 train acc 0.47469 valid acc 0.45403\n",
      "epoch 307: train loss 0.03602, valid loss 0.03642 train acc 0.47436 valid acc 0.45448\n",
      "epoch 308: train loss 0.03601, valid loss 0.03641 train acc 0.47558 valid acc 0.45441\n",
      "epoch 309: train loss 0.03600, valid loss 0.03641 train acc 0.47551 valid acc 0.45477\n",
      "epoch 310: train loss 0.03599, valid loss 0.03640 train acc 0.47622 valid acc 0.45492\n",
      "epoch 311: train loss 0.03598, valid loss 0.03639 train acc 0.47624 valid acc 0.45506\n",
      "epoch 312: train loss 0.03597, valid loss 0.03638 train acc 0.47636 valid acc 0.45525\n",
      "epoch 313: train loss 0.03596, valid loss 0.03637 train acc 0.47616 valid acc 0.45535\n",
      "epoch 314: train loss 0.03595, valid loss 0.03637 train acc 0.47760 valid acc 0.45554\n",
      "epoch 315: train loss 0.03595, valid loss 0.03636 train acc 0.47609 valid acc 0.45579\n",
      "epoch 316: train loss 0.03593, valid loss 0.03635 train acc 0.47693 valid acc 0.45572\n",
      "epoch 317: train loss 0.03593, valid loss 0.03634 train acc 0.47749 valid acc 0.45630\n",
      "epoch 318: train loss 0.03592, valid loss 0.03633 train acc 0.47656 valid acc 0.45654\n",
      "epoch 319: train loss 0.03591, valid loss 0.03632 train acc 0.47720 valid acc 0.45680\n",
      "epoch 320: train loss 0.03590, valid loss 0.03632 train acc 0.47864 valid acc 0.45678\n",
      "epoch 321: train loss 0.03589, valid loss 0.03631 train acc 0.47764 valid acc 0.45714\n",
      "epoch 322: train loss 0.03588, valid loss 0.03630 train acc 0.47816 valid acc 0.45765\n",
      "epoch 323: train loss 0.03587, valid loss 0.03629 train acc 0.47904 valid acc 0.45760\n",
      "epoch 324: train loss 0.03586, valid loss 0.03629 train acc 0.47849 valid acc 0.45760\n",
      "epoch 325: train loss 0.03585, valid loss 0.03628 train acc 0.47931 valid acc 0.45766\n",
      "epoch 326: train loss 0.03585, valid loss 0.03627 train acc 0.47869 valid acc 0.45818\n",
      "epoch 327: train loss 0.03584, valid loss 0.03626 train acc 0.47962 valid acc 0.45825\n",
      "epoch 328: train loss 0.03583, valid loss 0.03626 train acc 0.47876 valid acc 0.45835\n",
      "epoch 329: train loss 0.03582, valid loss 0.03625 train acc 0.47996 valid acc 0.45865\n",
      "epoch 330: train loss 0.03581, valid loss 0.03624 train acc 0.48053 valid acc 0.45893\n",
      "epoch 331: train loss 0.03580, valid loss 0.03623 train acc 0.47944 valid acc 0.45922\n",
      "epoch 332: train loss 0.03579, valid loss 0.03623 train acc 0.48071 valid acc 0.45893\n",
      "epoch 333: train loss 0.03578, valid loss 0.03622 train acc 0.48044 valid acc 0.45911\n",
      "epoch 334: train loss 0.03577, valid loss 0.03621 train acc 0.48080 valid acc 0.45961\n",
      "epoch 335: train loss 0.03577, valid loss 0.03620 train acc 0.48067 valid acc 0.46019\n",
      "epoch 336: train loss 0.03576, valid loss 0.03620 train acc 0.48169 valid acc 0.45989\n",
      "epoch 337: train loss 0.03575, valid loss 0.03619 train acc 0.48109 valid acc 0.46030\n",
      "epoch 338: train loss 0.03574, valid loss 0.03618 train acc 0.48124 valid acc 0.46033\n",
      "epoch 339: train loss 0.03573, valid loss 0.03617 train acc 0.48198 valid acc 0.46068\n",
      "epoch 340: train loss 0.03572, valid loss 0.03616 train acc 0.48240 valid acc 0.46089\n",
      "epoch 341: train loss 0.03572, valid loss 0.03616 train acc 0.48176 valid acc 0.46111\n",
      "epoch 342: train loss 0.03571, valid loss 0.03615 train acc 0.48200 valid acc 0.46112\n",
      "epoch 343: train loss 0.03570, valid loss 0.03614 train acc 0.48218 valid acc 0.46145\n",
      "epoch 344: train loss 0.03569, valid loss 0.03614 train acc 0.48300 valid acc 0.46166\n",
      "epoch 345: train loss 0.03568, valid loss 0.03613 train acc 0.48220 valid acc 0.46160\n",
      "epoch 346: train loss 0.03567, valid loss 0.03612 train acc 0.48251 valid acc 0.46150\n",
      "epoch 347: train loss 0.03566, valid loss 0.03612 train acc 0.48269 valid acc 0.46230\n",
      "epoch 348: train loss 0.03565, valid loss 0.03611 train acc 0.48218 valid acc 0.46202\n",
      "epoch 349: train loss 0.03564, valid loss 0.03610 train acc 0.48276 valid acc 0.46237\n",
      "epoch 350: train loss 0.03564, valid loss 0.03609 train acc 0.48362 valid acc 0.46250\n",
      "epoch 351: train loss 0.03563, valid loss 0.03609 train acc 0.48400 valid acc 0.46271\n",
      "epoch 352: train loss 0.03562, valid loss 0.03608 train acc 0.48362 valid acc 0.46314\n",
      "epoch 353: train loss 0.03561, valid loss 0.03607 train acc 0.48413 valid acc 0.46316\n",
      "epoch 354: train loss 0.03561, valid loss 0.03607 train acc 0.48487 valid acc 0.46309\n",
      "epoch 355: train loss 0.03560, valid loss 0.03606 train acc 0.48431 valid acc 0.46379\n",
      "epoch 356: train loss 0.03559, valid loss 0.03605 train acc 0.48456 valid acc 0.46402\n",
      "epoch 357: train loss 0.03558, valid loss 0.03604 train acc 0.48482 valid acc 0.46376\n",
      "epoch 358: train loss 0.03557, valid loss 0.03604 train acc 0.48471 valid acc 0.46371\n",
      "epoch 359: train loss 0.03556, valid loss 0.03603 train acc 0.48516 valid acc 0.46419\n",
      "epoch 360: train loss 0.03555, valid loss 0.03602 train acc 0.48544 valid acc 0.46482\n",
      "epoch 361: train loss 0.03554, valid loss 0.03602 train acc 0.48544 valid acc 0.46490\n",
      "epoch 362: train loss 0.03554, valid loss 0.03601 train acc 0.48636 valid acc 0.46520\n",
      "epoch 363: train loss 0.03553, valid loss 0.03600 train acc 0.48509 valid acc 0.46530\n",
      "epoch 364: train loss 0.03552, valid loss 0.03599 train acc 0.48616 valid acc 0.46534\n",
      "epoch 365: train loss 0.03551, valid loss 0.03599 train acc 0.48660 valid acc 0.46555\n",
      "epoch 366: train loss 0.03550, valid loss 0.03598 train acc 0.48622 valid acc 0.46552\n",
      "epoch 367: train loss 0.03550, valid loss 0.03597 train acc 0.48660 valid acc 0.46596\n",
      "epoch 368: train loss 0.03549, valid loss 0.03596 train acc 0.48702 valid acc 0.46666\n",
      "epoch 369: train loss 0.03548, valid loss 0.03596 train acc 0.48767 valid acc 0.46631\n",
      "epoch 370: train loss 0.03547, valid loss 0.03595 train acc 0.48747 valid acc 0.46664\n",
      "epoch 371: train loss 0.03546, valid loss 0.03594 train acc 0.48818 valid acc 0.46693\n",
      "epoch 372: train loss 0.03545, valid loss 0.03594 train acc 0.48771 valid acc 0.46710\n",
      "epoch 373: train loss 0.03545, valid loss 0.03593 train acc 0.48842 valid acc 0.46740\n",
      "epoch 374: train loss 0.03544, valid loss 0.03592 train acc 0.48800 valid acc 0.46739\n",
      "epoch 375: train loss 0.03543, valid loss 0.03591 train acc 0.48864 valid acc 0.46750\n",
      "epoch 376: train loss 0.03542, valid loss 0.03591 train acc 0.48802 valid acc 0.46779\n",
      "epoch 377: train loss 0.03542, valid loss 0.03590 train acc 0.48793 valid acc 0.46825\n",
      "epoch 378: train loss 0.03541, valid loss 0.03589 train acc 0.48836 valid acc 0.46799\n",
      "epoch 379: train loss 0.03540, valid loss 0.03589 train acc 0.48824 valid acc 0.46837\n",
      "epoch 380: train loss 0.03539, valid loss 0.03588 train acc 0.48902 valid acc 0.46838\n",
      "epoch 381: train loss 0.03538, valid loss 0.03587 train acc 0.48891 valid acc 0.46891\n",
      "epoch 382: train loss 0.03537, valid loss 0.03587 train acc 0.48907 valid acc 0.46919\n",
      "epoch 383: train loss 0.03537, valid loss 0.03586 train acc 0.48976 valid acc 0.46913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 384: train loss 0.03536, valid loss 0.03585 train acc 0.48944 valid acc 0.46947\n",
      "epoch 385: train loss 0.03535, valid loss 0.03585 train acc 0.48976 valid acc 0.46949\n",
      "epoch 386: train loss 0.03535, valid loss 0.03584 train acc 0.48984 valid acc 0.46995\n",
      "epoch 387: train loss 0.03534, valid loss 0.03583 train acc 0.49051 valid acc 0.46992\n",
      "epoch 388: train loss 0.03533, valid loss 0.03582 train acc 0.49142 valid acc 0.47000\n",
      "epoch 389: train loss 0.03532, valid loss 0.03582 train acc 0.49087 valid acc 0.47042\n",
      "epoch 390: train loss 0.03531, valid loss 0.03581 train acc 0.49087 valid acc 0.47027\n",
      "epoch 391: train loss 0.03531, valid loss 0.03581 train acc 0.49033 valid acc 0.47083\n",
      "epoch 392: train loss 0.03530, valid loss 0.03580 train acc 0.49053 valid acc 0.47108\n",
      "epoch 393: train loss 0.03529, valid loss 0.03579 train acc 0.49136 valid acc 0.47188\n",
      "epoch 394: train loss 0.03528, valid loss 0.03579 train acc 0.49227 valid acc 0.47141\n",
      "epoch 395: train loss 0.03528, valid loss 0.03578 train acc 0.49169 valid acc 0.47143\n",
      "epoch 396: train loss 0.03527, valid loss 0.03578 train acc 0.49238 valid acc 0.47235\n",
      "epoch 397: train loss 0.03526, valid loss 0.03577 train acc 0.49176 valid acc 0.47196\n",
      "epoch 398: train loss 0.03525, valid loss 0.03576 train acc 0.49200 valid acc 0.47285\n",
      "epoch 399: train loss 0.03524, valid loss 0.03576 train acc 0.49260 valid acc 0.47255\n",
      "epoch 400: train loss 0.03524, valid loss 0.03575 train acc 0.49233 valid acc 0.47247\n",
      "epoch 401: train loss 0.03523, valid loss 0.03574 train acc 0.49280 valid acc 0.47271\n",
      "epoch 402: train loss 0.03522, valid loss 0.03573 train acc 0.49260 valid acc 0.47330\n",
      "epoch 403: train loss 0.03522, valid loss 0.03573 train acc 0.49318 valid acc 0.47284\n",
      "epoch 404: train loss 0.03521, valid loss 0.03572 train acc 0.49327 valid acc 0.47351\n",
      "epoch 405: train loss 0.03520, valid loss 0.03571 train acc 0.49324 valid acc 0.47377\n",
      "epoch 406: train loss 0.03519, valid loss 0.03571 train acc 0.49324 valid acc 0.47383\n",
      "epoch 407: train loss 0.03519, valid loss 0.03570 train acc 0.49436 valid acc 0.47397\n",
      "epoch 408: train loss 0.03518, valid loss 0.03570 train acc 0.49398 valid acc 0.47433\n",
      "epoch 409: train loss 0.03517, valid loss 0.03569 train acc 0.49400 valid acc 0.47486\n",
      "epoch 410: train loss 0.03516, valid loss 0.03568 train acc 0.49391 valid acc 0.47442\n",
      "epoch 411: train loss 0.03515, valid loss 0.03567 train acc 0.49462 valid acc 0.47458\n",
      "epoch 412: train loss 0.03515, valid loss 0.03567 train acc 0.49442 valid acc 0.47509\n",
      "epoch 413: train loss 0.03514, valid loss 0.03566 train acc 0.49567 valid acc 0.47514\n",
      "epoch 414: train loss 0.03513, valid loss 0.03566 train acc 0.49424 valid acc 0.47533\n",
      "epoch 415: train loss 0.03513, valid loss 0.03565 train acc 0.49547 valid acc 0.47563\n",
      "epoch 416: train loss 0.03512, valid loss 0.03564 train acc 0.49571 valid acc 0.47537\n",
      "epoch 417: train loss 0.03511, valid loss 0.03564 train acc 0.49544 valid acc 0.47543\n",
      "epoch 418: train loss 0.03510, valid loss 0.03563 train acc 0.49529 valid acc 0.47598\n",
      "epoch 419: train loss 0.03509, valid loss 0.03563 train acc 0.49576 valid acc 0.47605\n",
      "epoch 420: train loss 0.03509, valid loss 0.03562 train acc 0.49542 valid acc 0.47619\n",
      "epoch 421: train loss 0.03508, valid loss 0.03561 train acc 0.49600 valid acc 0.47652\n",
      "epoch 422: train loss 0.03507, valid loss 0.03561 train acc 0.49647 valid acc 0.47626\n",
      "epoch 423: train loss 0.03506, valid loss 0.03560 train acc 0.49664 valid acc 0.47647\n",
      "epoch 424: train loss 0.03506, valid loss 0.03560 train acc 0.49640 valid acc 0.47648\n",
      "epoch 425: train loss 0.03505, valid loss 0.03558 train acc 0.49669 valid acc 0.47722\n",
      "epoch 426: train loss 0.03505, valid loss 0.03558 train acc 0.49722 valid acc 0.47734\n",
      "epoch 427: train loss 0.03503, valid loss 0.03558 train acc 0.49778 valid acc 0.47730\n",
      "epoch 428: train loss 0.03503, valid loss 0.03557 train acc 0.49709 valid acc 0.47733\n",
      "epoch 429: train loss 0.03502, valid loss 0.03556 train acc 0.49702 valid acc 0.47724\n",
      "epoch 430: train loss 0.03502, valid loss 0.03556 train acc 0.49793 valid acc 0.47786\n",
      "epoch 431: train loss 0.03501, valid loss 0.03555 train acc 0.49762 valid acc 0.47761\n",
      "epoch 432: train loss 0.03500, valid loss 0.03554 train acc 0.49873 valid acc 0.47780\n",
      "epoch 433: train loss 0.03499, valid loss 0.03554 train acc 0.49764 valid acc 0.47802\n",
      "epoch 434: train loss 0.03499, valid loss 0.03553 train acc 0.49824 valid acc 0.47811\n",
      "epoch 435: train loss 0.03498, valid loss 0.03553 train acc 0.49882 valid acc 0.47840\n",
      "epoch 436: train loss 0.03497, valid loss 0.03552 train acc 0.49851 valid acc 0.47830\n",
      "epoch 437: train loss 0.03497, valid loss 0.03551 train acc 0.49891 valid acc 0.47867\n",
      "epoch 438: train loss 0.03496, valid loss 0.03551 train acc 0.49740 valid acc 0.47888\n",
      "epoch 439: train loss 0.03495, valid loss 0.03550 train acc 0.49856 valid acc 0.47842\n",
      "epoch 440: train loss 0.03494, valid loss 0.03550 train acc 0.49902 valid acc 0.47908\n",
      "epoch 441: train loss 0.03494, valid loss 0.03549 train acc 0.49924 valid acc 0.47866\n",
      "epoch 442: train loss 0.03493, valid loss 0.03548 train acc 0.49880 valid acc 0.47924\n",
      "epoch 443: train loss 0.03492, valid loss 0.03548 train acc 0.49958 valid acc 0.47916\n",
      "epoch 444: train loss 0.03492, valid loss 0.03547 train acc 0.49916 valid acc 0.47934\n",
      "epoch 445: train loss 0.03491, valid loss 0.03546 train acc 0.49978 valid acc 0.47935\n",
      "epoch 446: train loss 0.03490, valid loss 0.03546 train acc 0.50062 valid acc 0.47965\n",
      "epoch 447: train loss 0.03490, valid loss 0.03545 train acc 0.50038 valid acc 0.47986\n",
      "epoch 448: train loss 0.03489, valid loss 0.03545 train acc 0.50036 valid acc 0.47965\n",
      "epoch 449: train loss 0.03488, valid loss 0.03544 train acc 0.50113 valid acc 0.48002\n",
      "epoch 450: train loss 0.03487, valid loss 0.03544 train acc 0.50024 valid acc 0.47992\n",
      "epoch 451: train loss 0.03487, valid loss 0.03543 train acc 0.50107 valid acc 0.47993\n",
      "epoch 452: train loss 0.03486, valid loss 0.03542 train acc 0.50067 valid acc 0.48019\n",
      "epoch 453: train loss 0.03485, valid loss 0.03542 train acc 0.50082 valid acc 0.48048\n",
      "epoch 454: train loss 0.03485, valid loss 0.03541 train acc 0.50136 valid acc 0.48029\n",
      "epoch 455: train loss 0.03484, valid loss 0.03541 train acc 0.50067 valid acc 0.48072\n",
      "epoch 456: train loss 0.03483, valid loss 0.03540 train acc 0.50109 valid acc 0.48043\n",
      "epoch 457: train loss 0.03483, valid loss 0.03539 train acc 0.50196 valid acc 0.48095\n",
      "epoch 458: train loss 0.03482, valid loss 0.03539 train acc 0.50178 valid acc 0.48106\n",
      "epoch 459: train loss 0.03481, valid loss 0.03538 train acc 0.50282 valid acc 0.48082\n",
      "epoch 460: train loss 0.03480, valid loss 0.03538 train acc 0.50178 valid acc 0.48113\n",
      "epoch 461: train loss 0.03480, valid loss 0.03537 train acc 0.50220 valid acc 0.48111\n",
      "epoch 462: train loss 0.03479, valid loss 0.03536 train acc 0.50282 valid acc 0.48140\n",
      "epoch 463: train loss 0.03479, valid loss 0.03536 train acc 0.50233 valid acc 0.48119\n",
      "epoch 464: train loss 0.03478, valid loss 0.03535 train acc 0.50324 valid acc 0.48166\n",
      "epoch 465: train loss 0.03477, valid loss 0.03535 train acc 0.50349 valid acc 0.48135\n",
      "epoch 466: train loss 0.03477, valid loss 0.03534 train acc 0.50320 valid acc 0.48197\n",
      "epoch 467: train loss 0.03476, valid loss 0.03533 train acc 0.50336 valid acc 0.48183\n",
      "epoch 468: train loss 0.03475, valid loss 0.03533 train acc 0.50353 valid acc 0.48240\n",
      "epoch 469: train loss 0.03474, valid loss 0.03532 train acc 0.50351 valid acc 0.48237\n",
      "epoch 470: train loss 0.03474, valid loss 0.03532 train acc 0.50331 valid acc 0.48238\n",
      "epoch 471: train loss 0.03473, valid loss 0.03531 train acc 0.50349 valid acc 0.48228\n",
      "epoch 472: train loss 0.03473, valid loss 0.03531 train acc 0.50347 valid acc 0.48235\n",
      "epoch 473: train loss 0.03472, valid loss 0.03530 train acc 0.50387 valid acc 0.48266\n",
      "epoch 474: train loss 0.03471, valid loss 0.03530 train acc 0.50451 valid acc 0.48275\n",
      "epoch 475: train loss 0.03470, valid loss 0.03529 train acc 0.50424 valid acc 0.48270\n",
      "epoch 476: train loss 0.03470, valid loss 0.03528 train acc 0.50424 valid acc 0.48316\n",
      "epoch 477: train loss 0.03469, valid loss 0.03528 train acc 0.50478 valid acc 0.48294\n",
      "epoch 478: train loss 0.03469, valid loss 0.03527 train acc 0.50476 valid acc 0.48357\n",
      "epoch 479: train loss 0.03468, valid loss 0.03527 train acc 0.50558 valid acc 0.48345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 480: train loss 0.03467, valid loss 0.03526 train acc 0.50576 valid acc 0.48399\n",
      "epoch 481: train loss 0.03467, valid loss 0.03525 train acc 0.50567 valid acc 0.48354\n",
      "epoch 482: train loss 0.03466, valid loss 0.03525 train acc 0.50571 valid acc 0.48394\n",
      "epoch 483: train loss 0.03465, valid loss 0.03524 train acc 0.50582 valid acc 0.48383\n",
      "epoch 484: train loss 0.03464, valid loss 0.03524 train acc 0.50660 valid acc 0.48387\n",
      "epoch 485: train loss 0.03464, valid loss 0.03523 train acc 0.50647 valid acc 0.48408\n",
      "epoch 486: train loss 0.03463, valid loss 0.03523 train acc 0.50622 valid acc 0.48399\n",
      "epoch 487: train loss 0.03462, valid loss 0.03522 train acc 0.50689 valid acc 0.48457\n",
      "epoch 488: train loss 0.03462, valid loss 0.03522 train acc 0.50598 valid acc 0.48452\n",
      "epoch 489: train loss 0.03461, valid loss 0.03521 train acc 0.50682 valid acc 0.48423\n",
      "epoch 490: train loss 0.03461, valid loss 0.03520 train acc 0.50698 valid acc 0.48465\n",
      "epoch 491: train loss 0.03460, valid loss 0.03520 train acc 0.50607 valid acc 0.48470\n",
      "epoch 492: train loss 0.03459, valid loss 0.03519 train acc 0.50673 valid acc 0.48504\n",
      "epoch 493: train loss 0.03459, valid loss 0.03519 train acc 0.50760 valid acc 0.48509\n",
      "epoch 494: train loss 0.03458, valid loss 0.03518 train acc 0.50793 valid acc 0.48497\n",
      "epoch 495: train loss 0.03458, valid loss 0.03518 train acc 0.50731 valid acc 0.48528\n",
      "epoch 496: train loss 0.03456, valid loss 0.03517 train acc 0.50796 valid acc 0.48549\n",
      "epoch 497: train loss 0.03456, valid loss 0.03516 train acc 0.50784 valid acc 0.48571\n",
      "epoch 498: train loss 0.03455, valid loss 0.03516 train acc 0.50827 valid acc 0.48571\n",
      "epoch 499: train loss 0.03455, valid loss 0.03515 train acc 0.50767 valid acc 0.48558\n",
      "epoch 500: train loss 0.03454, valid loss 0.03515 train acc 0.50824 valid acc 0.48591\n",
      "epoch 501: train loss 0.03454, valid loss 0.03514 train acc 0.50858 valid acc 0.48609\n",
      "epoch 502: train loss 0.03453, valid loss 0.03514 train acc 0.50887 valid acc 0.48609\n",
      "epoch 503: train loss 0.03452, valid loss 0.03513 train acc 0.50904 valid acc 0.48627\n",
      "epoch 504: train loss 0.03452, valid loss 0.03512 train acc 0.50851 valid acc 0.48615\n",
      "epoch 505: train loss 0.03451, valid loss 0.03512 train acc 0.50976 valid acc 0.48636\n",
      "epoch 506: train loss 0.03450, valid loss 0.03511 train acc 0.50942 valid acc 0.48669\n",
      "epoch 507: train loss 0.03450, valid loss 0.03511 train acc 0.50931 valid acc 0.48650\n",
      "epoch 508: train loss 0.03449, valid loss 0.03510 train acc 0.50991 valid acc 0.48668\n",
      "epoch 509: train loss 0.03448, valid loss 0.03510 train acc 0.50973 valid acc 0.48655\n",
      "epoch 510: train loss 0.03448, valid loss 0.03509 train acc 0.50962 valid acc 0.48687\n",
      "epoch 511: train loss 0.03447, valid loss 0.03509 train acc 0.51040 valid acc 0.48707\n",
      "epoch 512: train loss 0.03447, valid loss 0.03508 train acc 0.51058 valid acc 0.48726\n",
      "epoch 513: train loss 0.03446, valid loss 0.03507 train acc 0.51091 valid acc 0.48721\n",
      "epoch 514: train loss 0.03445, valid loss 0.03507 train acc 0.51020 valid acc 0.48747\n",
      "epoch 515: train loss 0.03445, valid loss 0.03506 train acc 0.50984 valid acc 0.48739\n",
      "epoch 516: train loss 0.03444, valid loss 0.03506 train acc 0.51098 valid acc 0.48729\n",
      "epoch 517: train loss 0.03443, valid loss 0.03506 train acc 0.51042 valid acc 0.48773\n",
      "epoch 518: train loss 0.03443, valid loss 0.03505 train acc 0.51060 valid acc 0.48781\n",
      "epoch 519: train loss 0.03442, valid loss 0.03504 train acc 0.51064 valid acc 0.48784\n",
      "epoch 520: train loss 0.03441, valid loss 0.03504 train acc 0.51109 valid acc 0.48779\n",
      "epoch 521: train loss 0.03441, valid loss 0.03503 train acc 0.51191 valid acc 0.48833\n",
      "epoch 522: train loss 0.03440, valid loss 0.03503 train acc 0.51187 valid acc 0.48819\n",
      "epoch 523: train loss 0.03440, valid loss 0.03502 train acc 0.51231 valid acc 0.48846\n",
      "epoch 524: train loss 0.03439, valid loss 0.03501 train acc 0.51140 valid acc 0.48865\n",
      "epoch 525: train loss 0.03438, valid loss 0.03501 train acc 0.51173 valid acc 0.48866\n",
      "epoch 526: train loss 0.03438, valid loss 0.03501 train acc 0.51333 valid acc 0.48880\n",
      "epoch 527: train loss 0.03437, valid loss 0.03500 train acc 0.51264 valid acc 0.48910\n",
      "epoch 528: train loss 0.03436, valid loss 0.03499 train acc 0.51278 valid acc 0.48927\n",
      "epoch 529: train loss 0.03436, valid loss 0.03499 train acc 0.51238 valid acc 0.48943\n",
      "epoch 530: train loss 0.03435, valid loss 0.03498 train acc 0.51258 valid acc 0.48938\n",
      "epoch 531: train loss 0.03435, valid loss 0.03498 train acc 0.51253 valid acc 0.48959\n",
      "epoch 532: train loss 0.03434, valid loss 0.03497 train acc 0.51311 valid acc 0.48995\n",
      "epoch 533: train loss 0.03433, valid loss 0.03497 train acc 0.51373 valid acc 0.48987\n",
      "epoch 534: train loss 0.03433, valid loss 0.03496 train acc 0.51349 valid acc 0.49008\n",
      "epoch 535: train loss 0.03432, valid loss 0.03495 train acc 0.51336 valid acc 0.49027\n",
      "epoch 536: train loss 0.03432, valid loss 0.03495 train acc 0.51442 valid acc 0.49008\n",
      "epoch 537: train loss 0.03431, valid loss 0.03494 train acc 0.51442 valid acc 0.49056\n",
      "epoch 538: train loss 0.03430, valid loss 0.03494 train acc 0.51460 valid acc 0.49070\n",
      "epoch 539: train loss 0.03430, valid loss 0.03494 train acc 0.51409 valid acc 0.49075\n",
      "epoch 540: train loss 0.03429, valid loss 0.03493 train acc 0.51460 valid acc 0.49104\n",
      "epoch 541: train loss 0.03429, valid loss 0.03492 train acc 0.51464 valid acc 0.49095\n",
      "epoch 542: train loss 0.03428, valid loss 0.03492 train acc 0.51440 valid acc 0.49141\n",
      "epoch 543: train loss 0.03427, valid loss 0.03491 train acc 0.51444 valid acc 0.49126\n",
      "epoch 544: train loss 0.03427, valid loss 0.03491 train acc 0.51493 valid acc 0.49186\n",
      "epoch 545: train loss 0.03426, valid loss 0.03490 train acc 0.51458 valid acc 0.49199\n",
      "epoch 546: train loss 0.03425, valid loss 0.03490 train acc 0.51522 valid acc 0.49193\n",
      "epoch 547: train loss 0.03425, valid loss 0.03489 train acc 0.51549 valid acc 0.49220\n",
      "epoch 548: train loss 0.03424, valid loss 0.03489 train acc 0.51527 valid acc 0.49242\n",
      "epoch 549: train loss 0.03423, valid loss 0.03488 train acc 0.51633 valid acc 0.49241\n",
      "epoch 550: train loss 0.03423, valid loss 0.03488 train acc 0.51551 valid acc 0.49256\n",
      "epoch 551: train loss 0.03422, valid loss 0.03487 train acc 0.51576 valid acc 0.49293\n",
      "epoch 552: train loss 0.03422, valid loss 0.03486 train acc 0.51680 valid acc 0.49291\n",
      "epoch 553: train loss 0.03421, valid loss 0.03486 train acc 0.51558 valid acc 0.49309\n",
      "epoch 554: train loss 0.03420, valid loss 0.03485 train acc 0.51644 valid acc 0.49341\n",
      "epoch 555: train loss 0.03420, valid loss 0.03485 train acc 0.51656 valid acc 0.49338\n",
      "epoch 556: train loss 0.03419, valid loss 0.03484 train acc 0.51607 valid acc 0.49334\n",
      "epoch 557: train loss 0.03419, valid loss 0.03484 train acc 0.51589 valid acc 0.49363\n",
      "epoch 558: train loss 0.03418, valid loss 0.03483 train acc 0.51682 valid acc 0.49383\n",
      "epoch 559: train loss 0.03418, valid loss 0.03483 train acc 0.51682 valid acc 0.49375\n",
      "epoch 560: train loss 0.03417, valid loss 0.03482 train acc 0.51689 valid acc 0.49406\n",
      "epoch 561: train loss 0.03416, valid loss 0.03482 train acc 0.51613 valid acc 0.49409\n",
      "epoch 562: train loss 0.03416, valid loss 0.03481 train acc 0.51704 valid acc 0.49435\n",
      "epoch 563: train loss 0.03415, valid loss 0.03481 train acc 0.51704 valid acc 0.49420\n",
      "epoch 564: train loss 0.03414, valid loss 0.03480 train acc 0.51780 valid acc 0.49455\n",
      "epoch 565: train loss 0.03414, valid loss 0.03479 train acc 0.51702 valid acc 0.49446\n",
      "epoch 566: train loss 0.03413, valid loss 0.03479 train acc 0.51671 valid acc 0.49500\n",
      "epoch 567: train loss 0.03413, valid loss 0.03478 train acc 0.51791 valid acc 0.49500\n",
      "epoch 568: train loss 0.03412, valid loss 0.03478 train acc 0.51824 valid acc 0.49519\n",
      "epoch 569: train loss 0.03412, valid loss 0.03477 train acc 0.51816 valid acc 0.49507\n",
      "epoch 570: train loss 0.03411, valid loss 0.03477 train acc 0.51827 valid acc 0.49523\n",
      "epoch 571: train loss 0.03410, valid loss 0.03476 train acc 0.51911 valid acc 0.49553\n",
      "epoch 572: train loss 0.03410, valid loss 0.03476 train acc 0.51873 valid acc 0.49545\n",
      "epoch 573: train loss 0.03409, valid loss 0.03475 train acc 0.51880 valid acc 0.49568\n",
      "epoch 574: train loss 0.03409, valid loss 0.03475 train acc 0.51876 valid acc 0.49599\n",
      "epoch 575: train loss 0.03408, valid loss 0.03475 train acc 0.51922 valid acc 0.49594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 576: train loss 0.03408, valid loss 0.03474 train acc 0.51907 valid acc 0.49604\n",
      "epoch 577: train loss 0.03407, valid loss 0.03474 train acc 0.51900 valid acc 0.49642\n",
      "epoch 578: train loss 0.03406, valid loss 0.03473 train acc 0.51996 valid acc 0.49623\n",
      "epoch 579: train loss 0.03406, valid loss 0.03472 train acc 0.51944 valid acc 0.49646\n",
      "epoch 580: train loss 0.03405, valid loss 0.03472 train acc 0.52018 valid acc 0.49639\n",
      "epoch 581: train loss 0.03404, valid loss 0.03472 train acc 0.51882 valid acc 0.49643\n",
      "epoch 582: train loss 0.03404, valid loss 0.03471 train acc 0.52022 valid acc 0.49669\n",
      "epoch 583: train loss 0.03403, valid loss 0.03471 train acc 0.51960 valid acc 0.49716\n",
      "epoch 584: train loss 0.03403, valid loss 0.03470 train acc 0.51953 valid acc 0.49689\n",
      "epoch 585: train loss 0.03402, valid loss 0.03469 train acc 0.52067 valid acc 0.49718\n",
      "epoch 586: train loss 0.03402, valid loss 0.03469 train acc 0.52076 valid acc 0.49735\n",
      "epoch 587: train loss 0.03401, valid loss 0.03468 train acc 0.52073 valid acc 0.49748\n",
      "epoch 588: train loss 0.03401, valid loss 0.03468 train acc 0.52051 valid acc 0.49746\n",
      "epoch 589: train loss 0.03400, valid loss 0.03467 train acc 0.52118 valid acc 0.49779\n",
      "epoch 590: train loss 0.03399, valid loss 0.03467 train acc 0.52131 valid acc 0.49804\n",
      "epoch 591: train loss 0.03399, valid loss 0.03466 train acc 0.52064 valid acc 0.49760\n",
      "epoch 592: train loss 0.03398, valid loss 0.03466 train acc 0.52060 valid acc 0.49820\n",
      "epoch 593: train loss 0.03398, valid loss 0.03465 train acc 0.52093 valid acc 0.49792\n",
      "epoch 594: train loss 0.03397, valid loss 0.03465 train acc 0.52067 valid acc 0.49835\n",
      "epoch 595: train loss 0.03396, valid loss 0.03464 train acc 0.52258 valid acc 0.49828\n",
      "epoch 596: train loss 0.03396, valid loss 0.03464 train acc 0.52118 valid acc 0.49856\n",
      "epoch 597: train loss 0.03395, valid loss 0.03463 train acc 0.52200 valid acc 0.49906\n",
      "epoch 598: train loss 0.03394, valid loss 0.03463 train acc 0.52236 valid acc 0.49868\n",
      "epoch 599: train loss 0.03394, valid loss 0.03462 train acc 0.52262 valid acc 0.49945\n",
      "epoch 600: train loss 0.03393, valid loss 0.03462 train acc 0.52296 valid acc 0.49911\n",
      "epoch 601: train loss 0.03393, valid loss 0.03461 train acc 0.52233 valid acc 0.49959\n",
      "epoch 602: train loss 0.03392, valid loss 0.03460 train acc 0.52158 valid acc 0.49951\n",
      "epoch 603: train loss 0.03392, valid loss 0.03460 train acc 0.52229 valid acc 0.49962\n",
      "epoch 604: train loss 0.03391, valid loss 0.03460 train acc 0.52302 valid acc 0.49956\n",
      "epoch 605: train loss 0.03391, valid loss 0.03459 train acc 0.52264 valid acc 0.49971\n",
      "epoch 606: train loss 0.03390, valid loss 0.03459 train acc 0.52333 valid acc 0.49977\n",
      "epoch 607: train loss 0.03389, valid loss 0.03459 train acc 0.52313 valid acc 0.50003\n",
      "epoch 608: train loss 0.03389, valid loss 0.03458 train acc 0.52338 valid acc 0.50028\n",
      "epoch 609: train loss 0.03388, valid loss 0.03457 train acc 0.52367 valid acc 0.50030\n",
      "epoch 610: train loss 0.03388, valid loss 0.03457 train acc 0.52407 valid acc 0.50015\n",
      "epoch 611: train loss 0.03387, valid loss 0.03456 train acc 0.52316 valid acc 0.50070\n",
      "epoch 612: train loss 0.03387, valid loss 0.03456 train acc 0.52344 valid acc 0.50051\n",
      "epoch 613: train loss 0.03386, valid loss 0.03455 train acc 0.52396 valid acc 0.50078\n",
      "epoch 614: train loss 0.03385, valid loss 0.03455 train acc 0.52344 valid acc 0.50089\n",
      "epoch 615: train loss 0.03385, valid loss 0.03454 train acc 0.52480 valid acc 0.50078\n",
      "epoch 616: train loss 0.03384, valid loss 0.03454 train acc 0.52427 valid acc 0.50138\n",
      "epoch 617: train loss 0.03384, valid loss 0.03453 train acc 0.52353 valid acc 0.50147\n",
      "epoch 618: train loss 0.03383, valid loss 0.03453 train acc 0.52422 valid acc 0.50160\n",
      "epoch 619: train loss 0.03383, valid loss 0.03452 train acc 0.52458 valid acc 0.50135\n",
      "epoch 620: train loss 0.03382, valid loss 0.03452 train acc 0.52387 valid acc 0.50201\n",
      "epoch 621: train loss 0.03381, valid loss 0.03451 train acc 0.52507 valid acc 0.50186\n",
      "epoch 622: train loss 0.03381, valid loss 0.03451 train acc 0.52420 valid acc 0.50224\n",
      "epoch 623: train loss 0.03380, valid loss 0.03451 train acc 0.52518 valid acc 0.50225\n",
      "epoch 624: train loss 0.03380, valid loss 0.03450 train acc 0.52471 valid acc 0.50211\n",
      "epoch 625: train loss 0.03379, valid loss 0.03450 train acc 0.52496 valid acc 0.50259\n",
      "epoch 626: train loss 0.03379, valid loss 0.03449 train acc 0.52536 valid acc 0.50245\n",
      "epoch 627: train loss 0.03378, valid loss 0.03448 train acc 0.52453 valid acc 0.50279\n",
      "epoch 628: train loss 0.03378, valid loss 0.03448 train acc 0.52573 valid acc 0.50306\n",
      "epoch 629: train loss 0.03377, valid loss 0.03447 train acc 0.52502 valid acc 0.50318\n",
      "epoch 630: train loss 0.03376, valid loss 0.03447 train acc 0.52553 valid acc 0.50297\n",
      "epoch 631: train loss 0.03376, valid loss 0.03447 train acc 0.52636 valid acc 0.50354\n",
      "epoch 632: train loss 0.03375, valid loss 0.03446 train acc 0.52618 valid acc 0.50356\n",
      "epoch 633: train loss 0.03375, valid loss 0.03446 train acc 0.52624 valid acc 0.50368\n",
      "epoch 634: train loss 0.03374, valid loss 0.03445 train acc 0.52669 valid acc 0.50399\n",
      "epoch 635: train loss 0.03373, valid loss 0.03445 train acc 0.52740 valid acc 0.50368\n",
      "epoch 636: train loss 0.03373, valid loss 0.03444 train acc 0.52678 valid acc 0.50402\n",
      "epoch 637: train loss 0.03372, valid loss 0.03444 train acc 0.52653 valid acc 0.50416\n",
      "epoch 638: train loss 0.03372, valid loss 0.03443 train acc 0.52647 valid acc 0.50437\n",
      "epoch 639: train loss 0.03371, valid loss 0.03443 train acc 0.52716 valid acc 0.50471\n",
      "epoch 640: train loss 0.03371, valid loss 0.03442 train acc 0.52711 valid acc 0.50509\n",
      "epoch 641: train loss 0.03370, valid loss 0.03442 train acc 0.52713 valid acc 0.50431\n",
      "epoch 642: train loss 0.03370, valid loss 0.03442 train acc 0.52780 valid acc 0.50530\n",
      "epoch 643: train loss 0.03369, valid loss 0.03441 train acc 0.52760 valid acc 0.50528\n",
      "epoch 644: train loss 0.03368, valid loss 0.03441 train acc 0.52798 valid acc 0.50499\n",
      "epoch 645: train loss 0.03368, valid loss 0.03440 train acc 0.52633 valid acc 0.50521\n",
      "epoch 646: train loss 0.03367, valid loss 0.03440 train acc 0.52827 valid acc 0.50557\n",
      "epoch 647: train loss 0.03367, valid loss 0.03439 train acc 0.52829 valid acc 0.50592\n",
      "epoch 648: train loss 0.03366, valid loss 0.03439 train acc 0.52838 valid acc 0.50567\n",
      "epoch 649: train loss 0.03366, valid loss 0.03438 train acc 0.52798 valid acc 0.50591\n",
      "epoch 650: train loss 0.03365, valid loss 0.03438 train acc 0.52809 valid acc 0.50575\n",
      "epoch 651: train loss 0.03365, valid loss 0.03437 train acc 0.52862 valid acc 0.50619\n",
      "epoch 652: train loss 0.03364, valid loss 0.03437 train acc 0.52862 valid acc 0.50659\n",
      "epoch 653: train loss 0.03364, valid loss 0.03436 train acc 0.52833 valid acc 0.50622\n",
      "epoch 654: train loss 0.03363, valid loss 0.03436 train acc 0.52851 valid acc 0.50662\n",
      "epoch 655: train loss 0.03362, valid loss 0.03435 train acc 0.52938 valid acc 0.50659\n",
      "epoch 656: train loss 0.03362, valid loss 0.03435 train acc 0.52884 valid acc 0.50656\n",
      "epoch 657: train loss 0.03362, valid loss 0.03434 train acc 0.52898 valid acc 0.50712\n",
      "epoch 658: train loss 0.03361, valid loss 0.03434 train acc 0.52984 valid acc 0.50697\n",
      "epoch 659: train loss 0.03360, valid loss 0.03433 train acc 0.53129 valid acc 0.50700\n",
      "epoch 660: train loss 0.03359, valid loss 0.03434 train acc 0.52971 valid acc 0.50708\n",
      "epoch 661: train loss 0.03359, valid loss 0.03433 train acc 0.52911 valid acc 0.50765\n",
      "epoch 662: train loss 0.03359, valid loss 0.03432 train acc 0.52987 valid acc 0.50760\n",
      "epoch 663: train loss 0.03358, valid loss 0.03432 train acc 0.53020 valid acc 0.50766\n",
      "epoch 664: train loss 0.03357, valid loss 0.03431 train acc 0.52989 valid acc 0.50744\n",
      "epoch 665: train loss 0.03357, valid loss 0.03431 train acc 0.52884 valid acc 0.50797\n",
      "epoch 666: train loss 0.03356, valid loss 0.03430 train acc 0.52960 valid acc 0.50795\n",
      "epoch 667: train loss 0.03356, valid loss 0.03430 train acc 0.53056 valid acc 0.50803\n",
      "epoch 668: train loss 0.03355, valid loss 0.03429 train acc 0.52973 valid acc 0.50816\n",
      "epoch 669: train loss 0.03355, valid loss 0.03429 train acc 0.53053 valid acc 0.50837\n",
      "epoch 670: train loss 0.03354, valid loss 0.03429 train acc 0.53036 valid acc 0.50847\n",
      "epoch 671: train loss 0.03354, valid loss 0.03428 train acc 0.53158 valid acc 0.50836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 672: train loss 0.03353, valid loss 0.03428 train acc 0.53204 valid acc 0.50865\n",
      "epoch 673: train loss 0.03353, valid loss 0.03427 train acc 0.53053 valid acc 0.50879\n",
      "epoch 674: train loss 0.03352, valid loss 0.03427 train acc 0.53120 valid acc 0.50878\n",
      "epoch 675: train loss 0.03351, valid loss 0.03426 train acc 0.53104 valid acc 0.50883\n",
      "epoch 676: train loss 0.03351, valid loss 0.03426 train acc 0.53204 valid acc 0.50892\n",
      "epoch 677: train loss 0.03350, valid loss 0.03425 train acc 0.53140 valid acc 0.50918\n",
      "epoch 678: train loss 0.03350, valid loss 0.03425 train acc 0.53084 valid acc 0.50887\n",
      "epoch 679: train loss 0.03350, valid loss 0.03424 train acc 0.53171 valid acc 0.50920\n",
      "epoch 680: train loss 0.03349, valid loss 0.03424 train acc 0.53267 valid acc 0.50950\n",
      "epoch 681: train loss 0.03348, valid loss 0.03424 train acc 0.53213 valid acc 0.50938\n",
      "epoch 682: train loss 0.03348, valid loss 0.03423 train acc 0.53280 valid acc 0.50931\n",
      "epoch 683: train loss 0.03347, valid loss 0.03423 train acc 0.53309 valid acc 0.50961\n",
      "epoch 684: train loss 0.03347, valid loss 0.03422 train acc 0.53276 valid acc 0.50989\n",
      "epoch 685: train loss 0.03347, valid loss 0.03422 train acc 0.53300 valid acc 0.51002\n",
      "epoch 686: train loss 0.03346, valid loss 0.03421 train acc 0.53251 valid acc 0.51009\n",
      "epoch 687: train loss 0.03345, valid loss 0.03421 train acc 0.53280 valid acc 0.51007\n",
      "epoch 688: train loss 0.03345, valid loss 0.03420 train acc 0.53296 valid acc 0.51030\n",
      "epoch 689: train loss 0.03344, valid loss 0.03420 train acc 0.53327 valid acc 0.51017\n",
      "epoch 690: train loss 0.03343, valid loss 0.03420 train acc 0.53320 valid acc 0.51032\n",
      "epoch 691: train loss 0.03343, valid loss 0.03419 train acc 0.53360 valid acc 0.51047\n",
      "epoch 692: train loss 0.03342, valid loss 0.03418 train acc 0.53229 valid acc 0.51070\n",
      "epoch 693: train loss 0.03342, valid loss 0.03418 train acc 0.53353 valid acc 0.51044\n",
      "epoch 694: train loss 0.03341, valid loss 0.03418 train acc 0.53416 valid acc 0.51066\n",
      "epoch 695: train loss 0.03340, valid loss 0.03417 train acc 0.53380 valid acc 0.51088\n",
      "epoch 696: train loss 0.03340, valid loss 0.03417 train acc 0.53344 valid acc 0.51091\n",
      "epoch 697: train loss 0.03340, valid loss 0.03416 train acc 0.53387 valid acc 0.51113\n",
      "epoch 698: train loss 0.03339, valid loss 0.03416 train acc 0.53393 valid acc 0.51111\n",
      "epoch 699: train loss 0.03339, valid loss 0.03415 train acc 0.53400 valid acc 0.51126\n",
      "epoch 700: train loss 0.03338, valid loss 0.03415 train acc 0.53444 valid acc 0.51140\n",
      "epoch 701: train loss 0.03338, valid loss 0.03415 train acc 0.53447 valid acc 0.51123\n",
      "epoch 702: train loss 0.03337, valid loss 0.03414 train acc 0.53496 valid acc 0.51160\n",
      "epoch 703: train loss 0.03337, valid loss 0.03414 train acc 0.53418 valid acc 0.51153\n",
      "epoch 704: train loss 0.03336, valid loss 0.03413 train acc 0.53400 valid acc 0.51157\n",
      "epoch 705: train loss 0.03335, valid loss 0.03413 train acc 0.53540 valid acc 0.51201\n",
      "epoch 706: train loss 0.03335, valid loss 0.03413 train acc 0.53467 valid acc 0.51190\n",
      "epoch 707: train loss 0.03335, valid loss 0.03412 train acc 0.53447 valid acc 0.51189\n",
      "epoch 708: train loss 0.03334, valid loss 0.03412 train acc 0.53562 valid acc 0.51237\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_accuracy = []\n",
    "\n",
    "valid_losses = []\n",
    "valid_accuracy = []\n",
    "\n",
    "# valid = (x_valid, y_valid)\n",
    "steps_per_epoch = train_size//batch_size\n",
    "\n",
    "for i in range(epochs):\n",
    "    train_epoch_losses = []\n",
    "    train_epoch_accuracy = []\n",
    "    \n",
    "    valid_epoch_losses = []\n",
    "    valid_epoch_accuracy = []\n",
    "    \n",
    "    x_train, y_train = shaffle(x_train, y_train)\n",
    "    \n",
    "    for batch in range(steps_per_epoch):\n",
    "        \n",
    "        _x_train = x_train[batch*batch_size:(batch+1)*batch_size]\n",
    "        _y_train = y_train[batch*batch_size:(batch+1)*batch_size]\n",
    "        \n",
    "        params = get_params(opt_state)\n",
    "        opt_state = opt_update(i*steps_per_epoch + batch, grad_loss(params, _x_train, _y_train), opt_state)\n",
    "        \n",
    "        train_epoch_losses.append(loss(params, _x_train, _y_train))\n",
    "        valid_epoch_losses.append(loss(params, x_valid, y_valid))\n",
    "        \n",
    "        train_correctness = onp.argmax(apply_fn(params, _x_train), 1) == onp.argmax(_y_train, 1)\n",
    "        train_epoch_accuracy.append(onp.average(train_correctness))\n",
    "        \n",
    "        valid_correctness = onp.argmax(apply_fn(params, x_valid), 1) == onp.argmax(y_valid, 1)\n",
    "        valid_epoch_accuracy.append(onp.average(valid_correctness))\n",
    "    \n",
    "    print(\"epoch %3d: train loss %3.5f, valid loss %3.5f train acc %.5f valid acc %.5f\"%\\\n",
    "          (i, \n",
    "           onp.average(train_epoch_losses), \n",
    "           onp.average(valid_epoch_losses), \n",
    "           onp.average(train_epoch_accuracy), \n",
    "           onp.average(valid_epoch_accuracy)))\n",
    "    \n",
    "    train_losses.append(onp.average(train_epoch_losses))\n",
    "    train_accuracy.append(onp.average(train_epoch_accuracy))\n",
    "    \n",
    "    valid_losses.append(onp.average(valid_epoch_losses))\n",
    "    valid_accuracy.append(onp.average(valid_epoch_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attack algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, x_train=None, y_train=None, x_test=None, \n",
    "                         y=None, t=None, loss_weighting=None, fx_train_0=0., fx_test_0=0., eps=0.3, \n",
    "                         norm=np.inf, clip_min=None, clip_max=None, targeted=False):\n",
    "    if norm not in [np.inf, 2]:\n",
    "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
    "        \n",
    "    x = x_test\n",
    "        \n",
    "    # test independent\n",
    "    if obj_fn == 'untargeted':\n",
    "        grads = grads_fn(x_train, x_test, y_train, y, kernel_fn, t)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Objective function must be either train(ntk_train_train) or test(predict_fn)\")\n",
    "\n",
    "    axis = list(range(1, len(grads.shape)))\n",
    "    eps_div = 1e-12\n",
    "    \n",
    "    if norm == np.inf:\n",
    "        perturbation = eps * np.sign(grads)\n",
    "    elif norm == 1:\n",
    "        raise NotImplementedError(\"L_1 norm has not been implemented yet.\")\n",
    "    elif norm == 2:\n",
    "        square = np.maximum(eps_div, np.sum(np.square(grads), axis=axis, keepdims=True))\n",
    "        perturbation = grads / np.sqrt(square)\n",
    "    \n",
    "    # TODO\n",
    "    adv_x = x + perturbation\n",
    "    \n",
    "    # If clipping is needed, reset all values outside of [clip_min, clip_max]\n",
    "    if (clip_min is not None) or (clip_max is not None):\n",
    "        # We don't currently support one-sided clipping\n",
    "        assert clip_min is not None and clip_max is not None\n",
    "        adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "    \n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, x_train=None, y_train=None,\n",
    "                               x_test=None, y=None, t=None, loss_weighting=None, fx_train_0=0., fx_test_0=0., \n",
    "                               eps=0.3, eps_iter=0.03, nb_iter=10, norm=np.inf, clip_min=None, clip_max=None, \n",
    "                               targeted=False, rand_init=None, rand_minmax=0.3):\n",
    "\n",
    "    assert eps_iter <= eps, (eps_iter, eps)\n",
    "    if norm == 1:\n",
    "        raise NotImplementedError(\"It's not clear that FGM is a good inner loop\"\n",
    "                                  \" step for PGD when norm=1, because norm=1 FGM \"\n",
    "                                  \" changes only one pixel at a time. We need \"\n",
    "                                  \" to rigorously test a strong norm=1 PGD \"\n",
    "                                  \"before enabling this feature.\")\n",
    "    if norm not in [np.inf, 2]:\n",
    "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
    "        \n",
    "    x = x_test\n",
    "    \n",
    "    # Initialize loop variables\n",
    "    if rand_init:\n",
    "        rand_minmax = eps\n",
    "        eta = random.uniform(new_key, x.shape, minval=-rand_minmax, maxval=rand_minmax)\n",
    "    else:\n",
    "        eta = np.zeros_like(x)\n",
    "\n",
    "    # Clip eta\n",
    "    eta = clip_eta(eta, norm, eps)\n",
    "    adv_x = x + eta\n",
    "    if clip_min is not None or clip_max is not None:\n",
    "        adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "        \n",
    "    for i in range(nb_iter):\n",
    "        adv_x = fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, x_train, y_train, adv_x, \n",
    "                                        y, t, loss_weighting, fx_train_0, fx_test_0, eps_iter, norm, \n",
    "                                        clip_min, clip_max, targeted)\n",
    "\n",
    "        # Clipping perturbation eta to norm norm ball\n",
    "        eta = adv_x - x\n",
    "        eta = clip_eta(eta, norm, eps)\n",
    "        adv_x = x + eta\n",
    "\n",
    "        # Redo the clipping.\n",
    "        # FGM already did it, but subtracting and re-adding eta can add some\n",
    "        # small numerical error.\n",
    "        if clip_min is not None or clip_max is not None:\n",
    "            adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "    \n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'mnist':\n",
    "    eps = 0.3\n",
    "    eps_iter_10 = (eps/10)*1.1\n",
    "    eps_iter_100 = (eps/100)*1.1\n",
    "    eps_iter_1000 = (eps/1000)*1.1\n",
    "    \n",
    "elif DATASET == 'cifar10':\n",
    "    eps = 0.03\n",
    "    eps_iter_10 = (eps/10)*1.1\n",
    "    eps_iter_100 = (eps/100)*1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(x_train, x_test, y_test, model_fn, kernel_fn, t=None, attack_type=None, ntk_train_train=None):\n",
    "    \n",
    "    y_train_predict, y_test_predict = model_fn(kernel_fn, x_train, x_test, \n",
    "                                               t=t, ntk_train_train=ntk_train_train)\n",
    "    \n",
    "    selected_table = correct(y_test_predict, y_test)\n",
    "    print(\"Accuray({:s}): {:.2f}\".format(attack_type, onp.mean(selected_table)))\n",
    "    \n",
    "    return selected_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_robustness(x_train, x_test, y_test, model_fn, kernel_fn, selected_table, t=None, \n",
    "                        attack_type=None, ntk_train_train=None):\n",
    "    \n",
    "    y_train_predict, y_test_predict = model_fn(kernel_fn, x_train, x_test,\n",
    "                                               t=t, ntk_train_train=ntk_train_train)\n",
    "    \n",
    "    y_test_predict = onp.asarray(y_test_predict)\n",
    "    y_test_predict_select = y_test_predict[onp.asarray(selected_table)]\n",
    "    y_test_select = y_test[onp.asarray(selected_table)]\n",
    "    print(\"Robustness({:s}): {:.2f}\".format(attack_type, onp.mean(correct(y_test_predict_select, y_test_select))))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adv_x generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
