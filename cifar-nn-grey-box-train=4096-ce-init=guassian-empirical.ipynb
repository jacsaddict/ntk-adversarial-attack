{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.0\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "\n",
    "import numpy as onp\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "\n",
    "from jax import lax, random\n",
    "from jax.api import grad, jit, vmap\n",
    "from jax.config import config\n",
    "from jax.experimental import optimizers\n",
    "from jax.experimental.stax import logsoftmax\n",
    "\n",
    "config.update('jax_enable_x64', True)\n",
    "\n",
    "from neural_tangents import stax\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# Attacking\n",
    "from cleverhans.utils import clip_eta, one_hot\n",
    "\n",
    "# Plotting\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from utils import *\n",
    "\n",
    "sns.set_style(style='white')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "\"\"\"\n",
    "diag_reg:\n",
    "    a scalar representing the strength of the diagonal regularization for\n",
    "    `k_train_train`, i.e. computing `k_train_train + diag_reg * I` during\n",
    "    Cholesky factorization or eigendecomposition.\n",
    "\"\"\"\n",
    "diag_reg = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'cifar10'\n",
    "class_num   = 10\n",
    "image_shape = None\n",
    "\n",
    "train_size = 4096\n",
    "valid_size = 512\n",
    "test_size = 128\n",
    "test_batch_size  = 16\n",
    "eps = 0.03\n",
    "\n",
    "batch_size = 256\n",
    "eps = 0.03\n",
    "epochs = 1000\n",
    "\n",
    "if DATASET =='mnist':\n",
    "    image_shape = (28, 28, 1)\n",
    "elif DATASET == 'cifar10':\n",
    "    image_shape = (32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_all, y_train_all, x_test_all, y_test_all = tuple(onp.array(x) for x in get_dataset(DATASET, None, None, \n",
    "                                                                                  do_flatten_and_normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "seed = 0\n",
    "x_train_all, y_train_all = shaffle(x_train_all, y_train_all, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# down sample\n",
    "x_train = x_train_all[:train_size]\n",
    "y_train = y_train_all[:train_size]\n",
    "\n",
    "x_valid = x_train_all[train_size:train_size + valid_size]\n",
    "y_valid = y_train_all[train_size:train_size + valid_size]\n",
    "\n",
    "x_test = x_test_all[:test_size]\n",
    "y_test = y_test_all[:test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, x_test = x_train.reshape((-1, *image_shape)), x_valid.reshape((-1, *image_shape)), x_test.reshape((-1, *image_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(mean, ys):\n",
    "    return onp.argmax(mean, axis=-1) == onp.argmax(ys, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvBlock(channels, W_std, b_std, strides=(1,1)):\n",
    "    return stax.serial(stax.Conv(out_chan=channels, filter_shape=(3,3), strides=strides, padding='SAME',\n",
    "                                 W_std=W_std, b_std=b_std), \n",
    "                       stax.Relu(do_backprop=True))\n",
    "\n",
    "def ConvGroup(n, channels, stride, W_std, b_std, last_stride=False):\n",
    "    blocks = []\n",
    "    if last_stride:\n",
    "        for i in range(n-1):\n",
    "            blocks += [ConvBlock(channels, W_std, b_std, stride)]\n",
    "        blocks += [ConvBlock(channels, W_std, b_std, (2, 2))]\n",
    "    \n",
    "    else:\n",
    "        for i in range(n):\n",
    "            blocks += [ConvBlock(channels, W_std, b_std, stride)]\n",
    "        \n",
    "    return stax.serial(*blocks)\n",
    "        \n",
    "def VGG19_stride(class_num=class_num):\n",
    "    \n",
    "    return stax.serial(\n",
    "        ConvGroup(n=2, channels=64 , stride=(1,1), W_std=0.1, b_std=0.18, last_stride=True),\n",
    "        ConvGroup(n=2, channels=128, stride=(1,1), W_std=0.1, b_std=0.18, last_stride=True),\n",
    "        ConvGroup(n=4, channels=256, stride=(1,1), W_std=0.1, b_std=0.18, last_stride=True),\n",
    "        ConvGroup(n=4, channels=512, stride=(1,1), W_std=0.1, b_std=0.18, last_stride=True),\n",
    "        ConvGroup(n=4, channels=512, stride=(1,1), W_std=0.1, b_std=0.18, last_stride=True),\n",
    "        stax.Flatten(),\n",
    "        stax.Dense(4096), stax.Relu(do_backprop=True),\n",
    "        stax.Dense(4096), stax.Relu(do_backprop=True),\n",
    "        stax.Dense(class_num))\n",
    "\n",
    "def simple_net(class_num=class_num):\n",
    "    return stax.serial(\n",
    "        ConvGroup(n=3, channels=64 , stride=(1,1), W_std=onp.sqrt(2), b_std=0.0, last_stride=False),\n",
    "        stax.Flatten(),\n",
    "        stax.Dense(4096, W_std=onp.sqrt(2)), stax.Relu(do_backprop=True),\n",
    "        stax.Dense(class_num, W_std=onp.sqrt(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def l2_loss_v1(logits, labels, weighting=1):\n",
    "    \"\"\"\n",
    "    Tensorflow version of L2 loss (without sqrt)\n",
    "    \"\"\"\n",
    "    return np.sum(((logits - labels)**2) * weighting) / 2\n",
    "    \n",
    "@jit\n",
    "def l2_loss_v2(logits, lables):\n",
    "    \"\"\"\n",
    "    Normal L2 loss\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(logits - labels)\n",
    "\n",
    "@jit\n",
    "def cross_entropy_loss(logits, lables):\n",
    "    return -np.mean(logsoftmax(logits) * lables)\n",
    "    \n",
    "@jit\n",
    "def mse_loss(logits, lables):\n",
    "    return 0.5 * np.mean((logits - lables) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_fn, apply_fn, kernel_fn = simple_net(class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_fn = jit(apply_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(88888)\n",
    "key, net_key = random.split(key)\n",
    "_, params = init_fn(net_key, (-1, 32, 32, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "# training_steps = 3200\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.sgd(learning_rate)\n",
    "opt_update = jit(opt_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = jit(lambda params, x, y: -np.mean(logsoftmax(apply_fn(params, x)) * y))\n",
    "grad_loss = jit(lambda state, x, y: grad(loss)(get_params(state), x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = onp.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_state = opt_init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   0: train loss 0.24208, valid loss 0.24338 train acc 0.09473 valid acc 0.08008\n",
      "epoch   1: train loss 0.24208, valid loss 0.24337 train acc 0.09473 valid acc 0.08008\n",
      "epoch   2: train loss 0.24207, valid loss 0.24337 train acc 0.09473 valid acc 0.08008\n",
      "epoch   3: train loss 0.24206, valid loss 0.24336 train acc 0.09473 valid acc 0.08008\n",
      "epoch   4: train loss 0.24206, valid loss 0.24335 train acc 0.09473 valid acc 0.08008\n",
      "epoch   5: train loss 0.24205, valid loss 0.24335 train acc 0.09473 valid acc 0.08008\n",
      "epoch   6: train loss 0.24204, valid loss 0.24334 train acc 0.09473 valid acc 0.08008\n",
      "epoch   7: train loss 0.24204, valid loss 0.24333 train acc 0.09473 valid acc 0.08008\n",
      "epoch   8: train loss 0.24203, valid loss 0.24333 train acc 0.09497 valid acc 0.08008\n",
      "epoch   9: train loss 0.24202, valid loss 0.24332 train acc 0.09497 valid acc 0.08008\n",
      "epoch  10: train loss 0.24202, valid loss 0.24331 train acc 0.09497 valid acc 0.08008\n",
      "epoch  11: train loss 0.24201, valid loss 0.24331 train acc 0.09497 valid acc 0.08008\n",
      "epoch  12: train loss 0.24200, valid loss 0.24330 train acc 0.09497 valid acc 0.08008\n",
      "epoch  13: train loss 0.24200, valid loss 0.24329 train acc 0.09497 valid acc 0.08008\n",
      "epoch  14: train loss 0.24199, valid loss 0.24329 train acc 0.09497 valid acc 0.08008\n",
      "epoch  15: train loss 0.24198, valid loss 0.24328 train acc 0.09497 valid acc 0.08008\n",
      "epoch  16: train loss 0.24198, valid loss 0.24327 train acc 0.09497 valid acc 0.08008\n",
      "epoch  17: train loss 0.24197, valid loss 0.24327 train acc 0.09497 valid acc 0.08008\n",
      "epoch  18: train loss 0.24196, valid loss 0.24326 train acc 0.09521 valid acc 0.08008\n",
      "epoch  19: train loss 0.24196, valid loss 0.24325 train acc 0.09521 valid acc 0.08008\n",
      "epoch  20: train loss 0.24195, valid loss 0.24325 train acc 0.09521 valid acc 0.08008\n",
      "epoch  21: train loss 0.24194, valid loss 0.24324 train acc 0.09521 valid acc 0.08008\n",
      "epoch  22: train loss 0.24193, valid loss 0.24323 train acc 0.09521 valid acc 0.08008\n",
      "epoch  23: train loss 0.24193, valid loss 0.24323 train acc 0.09521 valid acc 0.08008\n",
      "epoch  24: train loss 0.24192, valid loss 0.24322 train acc 0.09521 valid acc 0.08008\n",
      "epoch  25: train loss 0.24191, valid loss 0.24321 train acc 0.09521 valid acc 0.08008\n",
      "epoch  26: train loss 0.24191, valid loss 0.24321 train acc 0.09521 valid acc 0.08008\n",
      "epoch  27: train loss 0.24190, valid loss 0.24320 train acc 0.09521 valid acc 0.08008\n",
      "epoch  28: train loss 0.24189, valid loss 0.24319 train acc 0.09521 valid acc 0.08008\n",
      "epoch  29: train loss 0.24189, valid loss 0.24319 train acc 0.09546 valid acc 0.08008\n",
      "epoch  30: train loss 0.24188, valid loss 0.24318 train acc 0.09521 valid acc 0.08008\n",
      "epoch  31: train loss 0.24187, valid loss 0.24317 train acc 0.09521 valid acc 0.08008\n",
      "epoch  32: train loss 0.24187, valid loss 0.24317 train acc 0.09521 valid acc 0.08008\n",
      "epoch  33: train loss 0.24186, valid loss 0.24316 train acc 0.09521 valid acc 0.08008\n",
      "epoch  34: train loss 0.24185, valid loss 0.24315 train acc 0.09521 valid acc 0.08008\n",
      "epoch  35: train loss 0.24185, valid loss 0.24315 train acc 0.09521 valid acc 0.08008\n",
      "epoch  36: train loss 0.24184, valid loss 0.24314 train acc 0.09521 valid acc 0.08008\n",
      "epoch  37: train loss 0.24183, valid loss 0.24313 train acc 0.09521 valid acc 0.08008\n",
      "epoch  38: train loss 0.24183, valid loss 0.24313 train acc 0.09521 valid acc 0.08008\n",
      "epoch  39: train loss 0.24182, valid loss 0.24312 train acc 0.09521 valid acc 0.08008\n",
      "epoch  40: train loss 0.24181, valid loss 0.24312 train acc 0.09521 valid acc 0.08008\n",
      "epoch  41: train loss 0.24181, valid loss 0.24311 train acc 0.09521 valid acc 0.08008\n",
      "epoch  42: train loss 0.24180, valid loss 0.24310 train acc 0.09521 valid acc 0.08008\n",
      "epoch  43: train loss 0.24180, valid loss 0.24310 train acc 0.09521 valid acc 0.08008\n",
      "epoch  44: train loss 0.24179, valid loss 0.24309 train acc 0.09521 valid acc 0.08008\n",
      "epoch  45: train loss 0.24178, valid loss 0.24308 train acc 0.09521 valid acc 0.08008\n",
      "epoch  46: train loss 0.24178, valid loss 0.24308 train acc 0.09521 valid acc 0.08008\n",
      "epoch  47: train loss 0.24177, valid loss 0.24307 train acc 0.09521 valid acc 0.08008\n",
      "epoch  48: train loss 0.24176, valid loss 0.24306 train acc 0.09521 valid acc 0.08008\n",
      "epoch  49: train loss 0.24176, valid loss 0.24306 train acc 0.09521 valid acc 0.08008\n",
      "epoch  50: train loss 0.24175, valid loss 0.24305 train acc 0.09521 valid acc 0.08008\n",
      "epoch  51: train loss 0.24174, valid loss 0.24304 train acc 0.09521 valid acc 0.08008\n",
      "epoch  52: train loss 0.24174, valid loss 0.24304 train acc 0.09521 valid acc 0.08008\n",
      "epoch  53: train loss 0.24173, valid loss 0.24303 train acc 0.09521 valid acc 0.08008\n",
      "epoch  54: train loss 0.24172, valid loss 0.24302 train acc 0.09521 valid acc 0.08008\n",
      "epoch  55: train loss 0.24172, valid loss 0.24302 train acc 0.09521 valid acc 0.08008\n",
      "epoch  56: train loss 0.24171, valid loss 0.24301 train acc 0.09521 valid acc 0.08008\n",
      "epoch  57: train loss 0.24170, valid loss 0.24301 train acc 0.09521 valid acc 0.08008\n",
      "epoch  58: train loss 0.24170, valid loss 0.24300 train acc 0.09521 valid acc 0.08008\n",
      "epoch  59: train loss 0.24169, valid loss 0.24299 train acc 0.09521 valid acc 0.08008\n",
      "epoch  60: train loss 0.24168, valid loss 0.24299 train acc 0.09521 valid acc 0.08008\n",
      "epoch  61: train loss 0.24168, valid loss 0.24298 train acc 0.09521 valid acc 0.08008\n",
      "epoch  62: train loss 0.24167, valid loss 0.24297 train acc 0.09521 valid acc 0.08008\n",
      "epoch  63: train loss 0.24166, valid loss 0.24297 train acc 0.09521 valid acc 0.08008\n",
      "epoch  64: train loss 0.24166, valid loss 0.24296 train acc 0.09521 valid acc 0.08008\n",
      "epoch  65: train loss 0.24165, valid loss 0.24295 train acc 0.09521 valid acc 0.08008\n",
      "epoch  66: train loss 0.24164, valid loss 0.24295 train acc 0.09521 valid acc 0.08008\n",
      "epoch  67: train loss 0.24164, valid loss 0.24294 train acc 0.09521 valid acc 0.08008\n",
      "epoch  68: train loss 0.24163, valid loss 0.24293 train acc 0.09521 valid acc 0.08008\n",
      "epoch  69: train loss 0.24162, valid loss 0.24293 train acc 0.09521 valid acc 0.08008\n",
      "epoch  70: train loss 0.24162, valid loss 0.24292 train acc 0.09521 valid acc 0.08008\n",
      "epoch  71: train loss 0.24161, valid loss 0.24292 train acc 0.09521 valid acc 0.08008\n",
      "epoch  72: train loss 0.24161, valid loss 0.24291 train acc 0.09521 valid acc 0.08008\n",
      "epoch  73: train loss 0.24160, valid loss 0.24290 train acc 0.09521 valid acc 0.08008\n",
      "epoch  74: train loss 0.24159, valid loss 0.24290 train acc 0.09521 valid acc 0.08008\n",
      "epoch  75: train loss 0.24159, valid loss 0.24289 train acc 0.09521 valid acc 0.08008\n",
      "epoch  76: train loss 0.24158, valid loss 0.24288 train acc 0.09521 valid acc 0.08008\n",
      "epoch  77: train loss 0.24157, valid loss 0.24288 train acc 0.09521 valid acc 0.08008\n",
      "epoch  78: train loss 0.24157, valid loss 0.24287 train acc 0.09521 valid acc 0.08008\n",
      "epoch  79: train loss 0.24156, valid loss 0.24286 train acc 0.09521 valid acc 0.08008\n",
      "epoch  80: train loss 0.24155, valid loss 0.24286 train acc 0.09521 valid acc 0.08008\n",
      "epoch  81: train loss 0.24155, valid loss 0.24285 train acc 0.09521 valid acc 0.08008\n",
      "epoch  82: train loss 0.24154, valid loss 0.24285 train acc 0.09521 valid acc 0.08008\n",
      "epoch  83: train loss 0.24153, valid loss 0.24284 train acc 0.09521 valid acc 0.08008\n",
      "epoch  84: train loss 0.24153, valid loss 0.24283 train acc 0.09521 valid acc 0.08008\n",
      "epoch  85: train loss 0.24152, valid loss 0.24283 train acc 0.09521 valid acc 0.08008\n",
      "epoch  86: train loss 0.24152, valid loss 0.24282 train acc 0.09521 valid acc 0.08008\n",
      "epoch  87: train loss 0.24151, valid loss 0.24281 train acc 0.09521 valid acc 0.08008\n",
      "epoch  88: train loss 0.24150, valid loss 0.24281 train acc 0.09521 valid acc 0.08008\n",
      "epoch  89: train loss 0.24150, valid loss 0.24280 train acc 0.09521 valid acc 0.08008\n",
      "epoch  90: train loss 0.24149, valid loss 0.24280 train acc 0.09521 valid acc 0.08008\n",
      "epoch  91: train loss 0.24148, valid loss 0.24279 train acc 0.09521 valid acc 0.08008\n",
      "epoch  92: train loss 0.24148, valid loss 0.24278 train acc 0.09521 valid acc 0.08008\n",
      "epoch  93: train loss 0.24147, valid loss 0.24278 train acc 0.09521 valid acc 0.08008\n",
      "epoch  94: train loss 0.24146, valid loss 0.24277 train acc 0.09521 valid acc 0.08008\n",
      "epoch  95: train loss 0.24146, valid loss 0.24276 train acc 0.09521 valid acc 0.08008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  96: train loss 0.24145, valid loss 0.24276 train acc 0.09521 valid acc 0.08008\n",
      "epoch  97: train loss 0.24144, valid loss 0.24275 train acc 0.09521 valid acc 0.08008\n",
      "epoch  98: train loss 0.24144, valid loss 0.24274 train acc 0.09521 valid acc 0.08008\n",
      "epoch  99: train loss 0.24143, valid loss 0.24274 train acc 0.09521 valid acc 0.08008\n",
      "epoch 100: train loss 0.24143, valid loss 0.24273 train acc 0.09521 valid acc 0.08008\n",
      "epoch 101: train loss 0.24142, valid loss 0.24273 train acc 0.09521 valid acc 0.08008\n",
      "epoch 102: train loss 0.24141, valid loss 0.24272 train acc 0.09521 valid acc 0.08008\n",
      "epoch 103: train loss 0.24141, valid loss 0.24271 train acc 0.09521 valid acc 0.08008\n",
      "epoch 104: train loss 0.24140, valid loss 0.24271 train acc 0.09521 valid acc 0.08008\n",
      "epoch 105: train loss 0.24139, valid loss 0.24270 train acc 0.09521 valid acc 0.08008\n",
      "epoch 106: train loss 0.24139, valid loss 0.24270 train acc 0.09521 valid acc 0.08008\n",
      "epoch 107: train loss 0.24138, valid loss 0.24269 train acc 0.09521 valid acc 0.08008\n",
      "epoch 108: train loss 0.24138, valid loss 0.24268 train acc 0.09521 valid acc 0.08008\n",
      "epoch 109: train loss 0.24137, valid loss 0.24268 train acc 0.09521 valid acc 0.08008\n",
      "epoch 110: train loss 0.24136, valid loss 0.24267 train acc 0.09521 valid acc 0.08008\n",
      "epoch 111: train loss 0.24136, valid loss 0.24266 train acc 0.09521 valid acc 0.08008\n",
      "epoch 112: train loss 0.24135, valid loss 0.24266 train acc 0.09521 valid acc 0.08008\n",
      "epoch 113: train loss 0.24134, valid loss 0.24265 train acc 0.09521 valid acc 0.08008\n",
      "epoch 114: train loss 0.24134, valid loss 0.24265 train acc 0.09521 valid acc 0.08008\n",
      "epoch 115: train loss 0.24133, valid loss 0.24264 train acc 0.09521 valid acc 0.08008\n",
      "epoch 116: train loss 0.24133, valid loss 0.24263 train acc 0.09521 valid acc 0.08008\n",
      "epoch 117: train loss 0.24132, valid loss 0.24263 train acc 0.09521 valid acc 0.08008\n",
      "epoch 118: train loss 0.24131, valid loss 0.24262 train acc 0.09521 valid acc 0.08008\n",
      "epoch 119: train loss 0.24131, valid loss 0.24261 train acc 0.09521 valid acc 0.08008\n",
      "epoch 120: train loss 0.24130, valid loss 0.24261 train acc 0.09521 valid acc 0.08008\n",
      "epoch 121: train loss 0.24129, valid loss 0.24260 train acc 0.09521 valid acc 0.08008\n",
      "epoch 122: train loss 0.24129, valid loss 0.24260 train acc 0.09521 valid acc 0.08008\n",
      "epoch 123: train loss 0.24128, valid loss 0.24259 train acc 0.09521 valid acc 0.08008\n",
      "epoch 124: train loss 0.24128, valid loss 0.24258 train acc 0.09521 valid acc 0.08008\n",
      "epoch 125: train loss 0.24127, valid loss 0.24258 train acc 0.09521 valid acc 0.08008\n",
      "epoch 126: train loss 0.24126, valid loss 0.24257 train acc 0.09521 valid acc 0.08008\n",
      "epoch 127: train loss 0.24126, valid loss 0.24257 train acc 0.09521 valid acc 0.08008\n",
      "epoch 128: train loss 0.24125, valid loss 0.24256 train acc 0.09521 valid acc 0.08008\n",
      "epoch 129: train loss 0.24124, valid loss 0.24255 train acc 0.09521 valid acc 0.08008\n",
      "epoch 130: train loss 0.24124, valid loss 0.24255 train acc 0.09521 valid acc 0.08008\n",
      "epoch 131: train loss 0.24123, valid loss 0.24254 train acc 0.09521 valid acc 0.08008\n",
      "epoch 132: train loss 0.24123, valid loss 0.24253 train acc 0.09521 valid acc 0.08008\n",
      "epoch 133: train loss 0.24122, valid loss 0.24253 train acc 0.09521 valid acc 0.08008\n",
      "epoch 134: train loss 0.24121, valid loss 0.24252 train acc 0.09521 valid acc 0.08008\n",
      "epoch 135: train loss 0.24121, valid loss 0.24252 train acc 0.09521 valid acc 0.08008\n",
      "epoch 136: train loss 0.24120, valid loss 0.24251 train acc 0.09521 valid acc 0.08008\n",
      "epoch 137: train loss 0.24119, valid loss 0.24250 train acc 0.09497 valid acc 0.08008\n",
      "epoch 138: train loss 0.24119, valid loss 0.24250 train acc 0.09497 valid acc 0.08008\n",
      "epoch 139: train loss 0.24118, valid loss 0.24249 train acc 0.09497 valid acc 0.08008\n",
      "epoch 140: train loss 0.24118, valid loss 0.24249 train acc 0.09497 valid acc 0.08008\n",
      "epoch 141: train loss 0.24117, valid loss 0.24248 train acc 0.09497 valid acc 0.08008\n",
      "epoch 142: train loss 0.24116, valid loss 0.24247 train acc 0.09497 valid acc 0.08008\n",
      "epoch 143: train loss 0.24116, valid loss 0.24247 train acc 0.09497 valid acc 0.08008\n",
      "epoch 144: train loss 0.24115, valid loss 0.24246 train acc 0.09497 valid acc 0.08008\n",
      "epoch 145: train loss 0.24115, valid loss 0.24246 train acc 0.09497 valid acc 0.08008\n",
      "epoch 146: train loss 0.24114, valid loss 0.24245 train acc 0.09497 valid acc 0.08008\n",
      "epoch 147: train loss 0.24113, valid loss 0.24244 train acc 0.09497 valid acc 0.08008\n",
      "epoch 148: train loss 0.24113, valid loss 0.24244 train acc 0.09497 valid acc 0.08008\n",
      "epoch 149: train loss 0.24112, valid loss 0.24243 train acc 0.09497 valid acc 0.08008\n",
      "epoch 150: train loss 0.24112, valid loss 0.24243 train acc 0.09497 valid acc 0.08008\n",
      "epoch 151: train loss 0.24111, valid loss 0.24242 train acc 0.09497 valid acc 0.08008\n",
      "epoch 152: train loss 0.24110, valid loss 0.24241 train acc 0.09497 valid acc 0.08008\n",
      "epoch 153: train loss 0.24110, valid loss 0.24241 train acc 0.09497 valid acc 0.08008\n",
      "epoch 154: train loss 0.24109, valid loss 0.24240 train acc 0.09497 valid acc 0.08008\n",
      "epoch 155: train loss 0.24108, valid loss 0.24240 train acc 0.09497 valid acc 0.08008\n",
      "epoch 156: train loss 0.24108, valid loss 0.24239 train acc 0.09497 valid acc 0.08008\n",
      "epoch 157: train loss 0.24107, valid loss 0.24238 train acc 0.09497 valid acc 0.08008\n",
      "epoch 158: train loss 0.24107, valid loss 0.24238 train acc 0.09497 valid acc 0.08008\n",
      "epoch 159: train loss 0.24106, valid loss 0.24237 train acc 0.09497 valid acc 0.08008\n",
      "epoch 160: train loss 0.24105, valid loss 0.24237 train acc 0.09497 valid acc 0.08008\n",
      "epoch 161: train loss 0.24105, valid loss 0.24236 train acc 0.09497 valid acc 0.08008\n",
      "epoch 162: train loss 0.24104, valid loss 0.24235 train acc 0.09497 valid acc 0.08008\n",
      "epoch 163: train loss 0.24104, valid loss 0.24235 train acc 0.09497 valid acc 0.08008\n",
      "epoch 164: train loss 0.24103, valid loss 0.24234 train acc 0.09497 valid acc 0.08008\n",
      "epoch 165: train loss 0.24102, valid loss 0.24234 train acc 0.09497 valid acc 0.08008\n",
      "epoch 166: train loss 0.24102, valid loss 0.24233 train acc 0.09497 valid acc 0.08008\n",
      "epoch 167: train loss 0.24101, valid loss 0.24232 train acc 0.09497 valid acc 0.08008\n",
      "epoch 168: train loss 0.24101, valid loss 0.24232 train acc 0.09497 valid acc 0.08008\n",
      "epoch 169: train loss 0.24100, valid loss 0.24231 train acc 0.09497 valid acc 0.08008\n",
      "epoch 170: train loss 0.24099, valid loss 0.24231 train acc 0.09497 valid acc 0.08008\n",
      "epoch 171: train loss 0.24099, valid loss 0.24230 train acc 0.09497 valid acc 0.08008\n",
      "epoch 172: train loss 0.24098, valid loss 0.24230 train acc 0.09497 valid acc 0.08008\n",
      "epoch 173: train loss 0.24098, valid loss 0.24229 train acc 0.09497 valid acc 0.08008\n",
      "epoch 174: train loss 0.24097, valid loss 0.24228 train acc 0.09497 valid acc 0.08008\n",
      "epoch 175: train loss 0.24096, valid loss 0.24228 train acc 0.09497 valid acc 0.08008\n",
      "epoch 176: train loss 0.24096, valid loss 0.24227 train acc 0.09497 valid acc 0.08008\n",
      "epoch 177: train loss 0.24095, valid loss 0.24227 train acc 0.09521 valid acc 0.08008\n",
      "epoch 178: train loss 0.24095, valid loss 0.24226 train acc 0.09521 valid acc 0.08008\n",
      "epoch 179: train loss 0.24094, valid loss 0.24225 train acc 0.09521 valid acc 0.08008\n",
      "epoch 180: train loss 0.24093, valid loss 0.24225 train acc 0.09521 valid acc 0.08008\n",
      "epoch 181: train loss 0.24093, valid loss 0.24224 train acc 0.09521 valid acc 0.08008\n",
      "epoch 182: train loss 0.24092, valid loss 0.24224 train acc 0.09521 valid acc 0.08008\n",
      "epoch 183: train loss 0.24092, valid loss 0.24223 train acc 0.09521 valid acc 0.08008\n",
      "epoch 184: train loss 0.24091, valid loss 0.24223 train acc 0.09497 valid acc 0.08008\n",
      "epoch 185: train loss 0.24091, valid loss 0.24222 train acc 0.09497 valid acc 0.08008\n",
      "epoch 186: train loss 0.24090, valid loss 0.24221 train acc 0.09497 valid acc 0.08008\n",
      "epoch 187: train loss 0.24089, valid loss 0.24221 train acc 0.09497 valid acc 0.08008\n",
      "epoch 188: train loss 0.24089, valid loss 0.24220 train acc 0.09497 valid acc 0.08008\n",
      "epoch 189: train loss 0.24088, valid loss 0.24220 train acc 0.09497 valid acc 0.08008\n",
      "epoch 190: train loss 0.24088, valid loss 0.24219 train acc 0.09497 valid acc 0.08008\n",
      "epoch 191: train loss 0.24087, valid loss 0.24218 train acc 0.09497 valid acc 0.08008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 192: train loss 0.24086, valid loss 0.24218 train acc 0.09497 valid acc 0.08008\n",
      "epoch 193: train loss 0.24086, valid loss 0.24217 train acc 0.09497 valid acc 0.08008\n",
      "epoch 194: train loss 0.24085, valid loss 0.24217 train acc 0.09497 valid acc 0.08008\n",
      "epoch 195: train loss 0.24085, valid loss 0.24216 train acc 0.09497 valid acc 0.08008\n",
      "epoch 196: train loss 0.24084, valid loss 0.24216 train acc 0.09497 valid acc 0.08008\n",
      "epoch 197: train loss 0.24083, valid loss 0.24215 train acc 0.09497 valid acc 0.08008\n",
      "epoch 198: train loss 0.24083, valid loss 0.24214 train acc 0.09497 valid acc 0.08008\n",
      "epoch 199: train loss 0.24082, valid loss 0.24214 train acc 0.09497 valid acc 0.08008\n",
      "epoch 200: train loss 0.24082, valid loss 0.24213 train acc 0.09497 valid acc 0.08008\n",
      "epoch 201: train loss 0.24081, valid loss 0.24213 train acc 0.09497 valid acc 0.08008\n",
      "epoch 202: train loss 0.24081, valid loss 0.24212 train acc 0.09497 valid acc 0.08008\n",
      "epoch 203: train loss 0.24080, valid loss 0.24212 train acc 0.09497 valid acc 0.08008\n",
      "epoch 204: train loss 0.24079, valid loss 0.24211 train acc 0.09497 valid acc 0.08008\n",
      "epoch 205: train loss 0.24079, valid loss 0.24210 train acc 0.09497 valid acc 0.08008\n",
      "epoch 206: train loss 0.24078, valid loss 0.24210 train acc 0.09497 valid acc 0.08008\n",
      "epoch 207: train loss 0.24078, valid loss 0.24209 train acc 0.09497 valid acc 0.08008\n",
      "epoch 208: train loss 0.24077, valid loss 0.24209 train acc 0.09497 valid acc 0.08008\n",
      "epoch 209: train loss 0.24076, valid loss 0.24208 train acc 0.09497 valid acc 0.08008\n",
      "epoch 210: train loss 0.24076, valid loss 0.24208 train acc 0.09497 valid acc 0.08008\n",
      "epoch 211: train loss 0.24075, valid loss 0.24207 train acc 0.09497 valid acc 0.08008\n",
      "epoch 212: train loss 0.24075, valid loss 0.24206 train acc 0.09497 valid acc 0.08008\n",
      "epoch 213: train loss 0.24074, valid loss 0.24206 train acc 0.09497 valid acc 0.08008\n",
      "epoch 214: train loss 0.24074, valid loss 0.24205 train acc 0.09497 valid acc 0.08008\n",
      "epoch 215: train loss 0.24073, valid loss 0.24205 train acc 0.09497 valid acc 0.08008\n",
      "epoch 216: train loss 0.24072, valid loss 0.24204 train acc 0.09497 valid acc 0.08008\n",
      "epoch 217: train loss 0.24072, valid loss 0.24204 train acc 0.09497 valid acc 0.08008\n",
      "epoch 218: train loss 0.24071, valid loss 0.24203 train acc 0.09497 valid acc 0.08008\n",
      "epoch 219: train loss 0.24071, valid loss 0.24202 train acc 0.09497 valid acc 0.08008\n",
      "epoch 220: train loss 0.24070, valid loss 0.24202 train acc 0.09497 valid acc 0.08008\n",
      "epoch 221: train loss 0.24070, valid loss 0.24201 train acc 0.09497 valid acc 0.08008\n",
      "epoch 222: train loss 0.24069, valid loss 0.24201 train acc 0.09497 valid acc 0.08008\n",
      "epoch 223: train loss 0.24068, valid loss 0.24200 train acc 0.09497 valid acc 0.08008\n",
      "epoch 224: train loss 0.24068, valid loss 0.24200 train acc 0.09497 valid acc 0.08008\n",
      "epoch 225: train loss 0.24067, valid loss 0.24199 train acc 0.09497 valid acc 0.08008\n",
      "epoch 226: train loss 0.24067, valid loss 0.24198 train acc 0.09497 valid acc 0.08008\n",
      "epoch 227: train loss 0.24066, valid loss 0.24198 train acc 0.09497 valid acc 0.08008\n",
      "epoch 228: train loss 0.24066, valid loss 0.24197 train acc 0.09497 valid acc 0.08008\n",
      "epoch 229: train loss 0.24065, valid loss 0.24197 train acc 0.09497 valid acc 0.08008\n",
      "epoch 230: train loss 0.24064, valid loss 0.24196 train acc 0.09497 valid acc 0.08008\n",
      "epoch 231: train loss 0.24064, valid loss 0.24196 train acc 0.09497 valid acc 0.08008\n",
      "epoch 232: train loss 0.24063, valid loss 0.24195 train acc 0.09497 valid acc 0.08008\n",
      "epoch 233: train loss 0.24063, valid loss 0.24195 train acc 0.09497 valid acc 0.08008\n",
      "epoch 234: train loss 0.24062, valid loss 0.24194 train acc 0.09497 valid acc 0.08008\n",
      "epoch 235: train loss 0.24062, valid loss 0.24193 train acc 0.09497 valid acc 0.08008\n",
      "epoch 236: train loss 0.24061, valid loss 0.24193 train acc 0.09497 valid acc 0.08008\n",
      "epoch 237: train loss 0.24060, valid loss 0.24192 train acc 0.09473 valid acc 0.08008\n",
      "epoch 238: train loss 0.24060, valid loss 0.24192 train acc 0.09473 valid acc 0.08008\n",
      "epoch 239: train loss 0.24059, valid loss 0.24191 train acc 0.09473 valid acc 0.08008\n",
      "epoch 240: train loss 0.24059, valid loss 0.24191 train acc 0.09473 valid acc 0.08008\n",
      "epoch 241: train loss 0.24058, valid loss 0.24190 train acc 0.09473 valid acc 0.08008\n",
      "epoch 242: train loss 0.24058, valid loss 0.24189 train acc 0.09473 valid acc 0.08008\n",
      "epoch 243: train loss 0.24057, valid loss 0.24189 train acc 0.09473 valid acc 0.08008\n",
      "epoch 244: train loss 0.24056, valid loss 0.24188 train acc 0.09473 valid acc 0.08008\n",
      "epoch 245: train loss 0.24056, valid loss 0.24188 train acc 0.09473 valid acc 0.08008\n",
      "epoch 246: train loss 0.24055, valid loss 0.24187 train acc 0.09473 valid acc 0.08008\n",
      "epoch 247: train loss 0.24055, valid loss 0.24187 train acc 0.09473 valid acc 0.08008\n",
      "epoch 248: train loss 0.24054, valid loss 0.24186 train acc 0.09473 valid acc 0.08008\n",
      "epoch 249: train loss 0.24054, valid loss 0.24186 train acc 0.09473 valid acc 0.08008\n",
      "epoch 250: train loss 0.24053, valid loss 0.24185 train acc 0.09473 valid acc 0.08008\n",
      "epoch 251: train loss 0.24052, valid loss 0.24184 train acc 0.09473 valid acc 0.08008\n",
      "epoch 252: train loss 0.24052, valid loss 0.24184 train acc 0.09473 valid acc 0.08008\n",
      "epoch 253: train loss 0.24051, valid loss 0.24183 train acc 0.09473 valid acc 0.08008\n",
      "epoch 254: train loss 0.24051, valid loss 0.24183 train acc 0.09497 valid acc 0.08008\n",
      "epoch 255: train loss 0.24050, valid loss 0.24182 train acc 0.09497 valid acc 0.08008\n",
      "epoch 256: train loss 0.24050, valid loss 0.24182 train acc 0.09473 valid acc 0.08008\n",
      "epoch 257: train loss 0.24049, valid loss 0.24181 train acc 0.09473 valid acc 0.08008\n",
      "epoch 258: train loss 0.24049, valid loss 0.24181 train acc 0.09473 valid acc 0.08008\n",
      "epoch 259: train loss 0.24048, valid loss 0.24180 train acc 0.09473 valid acc 0.08008\n",
      "epoch 260: train loss 0.24047, valid loss 0.24179 train acc 0.09473 valid acc 0.08008\n",
      "epoch 261: train loss 0.24047, valid loss 0.24179 train acc 0.09473 valid acc 0.08008\n",
      "epoch 262: train loss 0.24046, valid loss 0.24178 train acc 0.09473 valid acc 0.08008\n",
      "epoch 263: train loss 0.24046, valid loss 0.24178 train acc 0.09473 valid acc 0.08008\n",
      "epoch 264: train loss 0.24045, valid loss 0.24177 train acc 0.09473 valid acc 0.08008\n",
      "epoch 265: train loss 0.24045, valid loss 0.24177 train acc 0.09473 valid acc 0.08008\n",
      "epoch 266: train loss 0.24044, valid loss 0.24176 train acc 0.09473 valid acc 0.08008\n",
      "epoch 267: train loss 0.24044, valid loss 0.24176 train acc 0.09473 valid acc 0.08008\n",
      "epoch 268: train loss 0.24043, valid loss 0.24175 train acc 0.09473 valid acc 0.08008\n",
      "epoch 269: train loss 0.24042, valid loss 0.24174 train acc 0.09473 valid acc 0.08008\n",
      "epoch 270: train loss 0.24042, valid loss 0.24174 train acc 0.09473 valid acc 0.08008\n",
      "epoch 271: train loss 0.24041, valid loss 0.24173 train acc 0.09473 valid acc 0.08008\n",
      "epoch 272: train loss 0.24041, valid loss 0.24173 train acc 0.09448 valid acc 0.08008\n",
      "epoch 273: train loss 0.24040, valid loss 0.24172 train acc 0.09448 valid acc 0.08008\n",
      "epoch 274: train loss 0.24040, valid loss 0.24172 train acc 0.09448 valid acc 0.08008\n",
      "epoch 275: train loss 0.24039, valid loss 0.24171 train acc 0.09448 valid acc 0.08008\n",
      "epoch 276: train loss 0.24039, valid loss 0.24171 train acc 0.09448 valid acc 0.08008\n",
      "epoch 277: train loss 0.24038, valid loss 0.24170 train acc 0.09448 valid acc 0.08008\n",
      "epoch 278: train loss 0.24037, valid loss 0.24170 train acc 0.09448 valid acc 0.08008\n",
      "epoch 279: train loss 0.24037, valid loss 0.24169 train acc 0.09448 valid acc 0.08008\n",
      "epoch 280: train loss 0.24036, valid loss 0.24168 train acc 0.09448 valid acc 0.08008\n",
      "epoch 281: train loss 0.24036, valid loss 0.24168 train acc 0.09448 valid acc 0.08008\n",
      "epoch 282: train loss 0.24035, valid loss 0.24167 train acc 0.09448 valid acc 0.08008\n",
      "epoch 283: train loss 0.24035, valid loss 0.24167 train acc 0.09448 valid acc 0.08008\n",
      "epoch 284: train loss 0.24034, valid loss 0.24166 train acc 0.09448 valid acc 0.08008\n",
      "epoch 285: train loss 0.24034, valid loss 0.24166 train acc 0.09448 valid acc 0.08008\n",
      "epoch 286: train loss 0.24033, valid loss 0.24165 train acc 0.09448 valid acc 0.08008\n",
      "epoch 287: train loss 0.24032, valid loss 0.24165 train acc 0.09424 valid acc 0.08008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 288: train loss 0.24032, valid loss 0.24164 train acc 0.09424 valid acc 0.08008\n",
      "epoch 289: train loss 0.24031, valid loss 0.24164 train acc 0.09424 valid acc 0.08008\n",
      "epoch 290: train loss 0.24031, valid loss 0.24163 train acc 0.09424 valid acc 0.08008\n",
      "epoch 291: train loss 0.24030, valid loss 0.24162 train acc 0.09424 valid acc 0.08008\n",
      "epoch 292: train loss 0.24030, valid loss 0.24162 train acc 0.09424 valid acc 0.08008\n",
      "epoch 293: train loss 0.24029, valid loss 0.24161 train acc 0.09424 valid acc 0.08008\n",
      "epoch 294: train loss 0.24029, valid loss 0.24161 train acc 0.09424 valid acc 0.08008\n",
      "epoch 295: train loss 0.24028, valid loss 0.24160 train acc 0.09424 valid acc 0.08008\n",
      "epoch 296: train loss 0.24028, valid loss 0.24160 train acc 0.09424 valid acc 0.08008\n",
      "epoch 297: train loss 0.24027, valid loss 0.24159 train acc 0.09424 valid acc 0.08008\n",
      "epoch 298: train loss 0.24026, valid loss 0.24159 train acc 0.09424 valid acc 0.08008\n",
      "epoch 299: train loss 0.24026, valid loss 0.24158 train acc 0.09424 valid acc 0.08008\n",
      "epoch 300: train loss 0.24025, valid loss 0.24158 train acc 0.09424 valid acc 0.08008\n",
      "epoch 301: train loss 0.24025, valid loss 0.24157 train acc 0.09424 valid acc 0.08008\n",
      "epoch 302: train loss 0.24024, valid loss 0.24156 train acc 0.09424 valid acc 0.08008\n",
      "epoch 303: train loss 0.24024, valid loss 0.24156 train acc 0.09424 valid acc 0.08008\n",
      "epoch 304: train loss 0.24023, valid loss 0.24155 train acc 0.09424 valid acc 0.08008\n",
      "epoch 305: train loss 0.24023, valid loss 0.24155 train acc 0.09424 valid acc 0.08008\n",
      "epoch 306: train loss 0.24022, valid loss 0.24154 train acc 0.09399 valid acc 0.08008\n",
      "epoch 307: train loss 0.24022, valid loss 0.24154 train acc 0.09424 valid acc 0.08008\n",
      "epoch 308: train loss 0.24021, valid loss 0.24153 train acc 0.09424 valid acc 0.08008\n",
      "epoch 309: train loss 0.24021, valid loss 0.24153 train acc 0.09424 valid acc 0.08008\n",
      "epoch 310: train loss 0.24020, valid loss 0.24152 train acc 0.09424 valid acc 0.08008\n",
      "epoch 311: train loss 0.24019, valid loss 0.24152 train acc 0.09424 valid acc 0.08008\n",
      "epoch 312: train loss 0.24019, valid loss 0.24151 train acc 0.09424 valid acc 0.08008\n",
      "epoch 313: train loss 0.24018, valid loss 0.24151 train acc 0.09424 valid acc 0.08008\n",
      "epoch 314: train loss 0.24018, valid loss 0.24150 train acc 0.09424 valid acc 0.08008\n",
      "epoch 315: train loss 0.24017, valid loss 0.24150 train acc 0.09424 valid acc 0.08008\n",
      "epoch 316: train loss 0.24017, valid loss 0.24149 train acc 0.09424 valid acc 0.08008\n",
      "epoch 317: train loss 0.24016, valid loss 0.24148 train acc 0.09424 valid acc 0.08008\n",
      "epoch 318: train loss 0.24016, valid loss 0.24148 train acc 0.09424 valid acc 0.08008\n",
      "epoch 319: train loss 0.24015, valid loss 0.24147 train acc 0.09424 valid acc 0.08008\n",
      "epoch 320: train loss 0.24015, valid loss 0.24147 train acc 0.09424 valid acc 0.08008\n",
      "epoch 321: train loss 0.24014, valid loss 0.24146 train acc 0.09424 valid acc 0.08008\n",
      "epoch 322: train loss 0.24014, valid loss 0.24146 train acc 0.09424 valid acc 0.08008\n",
      "epoch 323: train loss 0.24013, valid loss 0.24145 train acc 0.09424 valid acc 0.08008\n",
      "epoch 324: train loss 0.24012, valid loss 0.24145 train acc 0.09424 valid acc 0.08008\n",
      "epoch 325: train loss 0.24012, valid loss 0.24144 train acc 0.09424 valid acc 0.08008\n",
      "epoch 326: train loss 0.24011, valid loss 0.24144 train acc 0.09424 valid acc 0.08008\n",
      "epoch 327: train loss 0.24011, valid loss 0.24143 train acc 0.09424 valid acc 0.08008\n",
      "epoch 328: train loss 0.24010, valid loss 0.24143 train acc 0.09424 valid acc 0.08008\n",
      "epoch 329: train loss 0.24010, valid loss 0.24142 train acc 0.09424 valid acc 0.08008\n",
      "epoch 330: train loss 0.24009, valid loss 0.24142 train acc 0.09424 valid acc 0.08008\n",
      "epoch 331: train loss 0.24009, valid loss 0.24141 train acc 0.09424 valid acc 0.08008\n",
      "epoch 332: train loss 0.24008, valid loss 0.24140 train acc 0.09424 valid acc 0.08008\n",
      "epoch 333: train loss 0.24008, valid loss 0.24140 train acc 0.09424 valid acc 0.08008\n",
      "epoch 334: train loss 0.24007, valid loss 0.24139 train acc 0.09424 valid acc 0.08008\n",
      "epoch 335: train loss 0.24007, valid loss 0.24139 train acc 0.09424 valid acc 0.08008\n",
      "epoch 336: train loss 0.24006, valid loss 0.24138 train acc 0.09424 valid acc 0.08008\n",
      "epoch 337: train loss 0.24006, valid loss 0.24138 train acc 0.09424 valid acc 0.08008\n",
      "epoch 338: train loss 0.24005, valid loss 0.24137 train acc 0.09424 valid acc 0.08008\n",
      "epoch 339: train loss 0.24005, valid loss 0.24137 train acc 0.09424 valid acc 0.08008\n",
      "epoch 340: train loss 0.24004, valid loss 0.24136 train acc 0.09424 valid acc 0.08008\n",
      "epoch 341: train loss 0.24003, valid loss 0.24136 train acc 0.09424 valid acc 0.08008\n",
      "epoch 342: train loss 0.24003, valid loss 0.24135 train acc 0.09424 valid acc 0.08008\n",
      "epoch 343: train loss 0.24002, valid loss 0.24135 train acc 0.09424 valid acc 0.08008\n",
      "epoch 344: train loss 0.24002, valid loss 0.24134 train acc 0.09424 valid acc 0.08008\n",
      "epoch 345: train loss 0.24001, valid loss 0.24134 train acc 0.09424 valid acc 0.08008\n",
      "epoch 346: train loss 0.24001, valid loss 0.24133 train acc 0.09424 valid acc 0.08008\n",
      "epoch 347: train loss 0.24000, valid loss 0.24133 train acc 0.09424 valid acc 0.08008\n",
      "epoch 348: train loss 0.24000, valid loss 0.24132 train acc 0.09424 valid acc 0.07849\n",
      "epoch 349: train loss 0.23999, valid loss 0.24132 train acc 0.09424 valid acc 0.07812\n",
      "epoch 350: train loss 0.23999, valid loss 0.24131 train acc 0.09399 valid acc 0.07812\n",
      "epoch 351: train loss 0.23998, valid loss 0.24131 train acc 0.09399 valid acc 0.07812\n",
      "epoch 352: train loss 0.23998, valid loss 0.24130 train acc 0.09399 valid acc 0.07812\n",
      "epoch 353: train loss 0.23997, valid loss 0.24129 train acc 0.09399 valid acc 0.07812\n",
      "epoch 354: train loss 0.23997, valid loss 0.24129 train acc 0.09399 valid acc 0.07812\n",
      "epoch 355: train loss 0.23996, valid loss 0.24128 train acc 0.09399 valid acc 0.07812\n",
      "epoch 356: train loss 0.23996, valid loss 0.24128 train acc 0.09399 valid acc 0.07812\n",
      "epoch 357: train loss 0.23995, valid loss 0.24127 train acc 0.09399 valid acc 0.07812\n",
      "epoch 358: train loss 0.23995, valid loss 0.24127 train acc 0.09399 valid acc 0.07812\n",
      "epoch 359: train loss 0.23994, valid loss 0.24126 train acc 0.09399 valid acc 0.07812\n",
      "epoch 360: train loss 0.23994, valid loss 0.24126 train acc 0.09399 valid acc 0.07812\n",
      "epoch 361: train loss 0.23993, valid loss 0.24125 train acc 0.09399 valid acc 0.07812\n",
      "epoch 362: train loss 0.23993, valid loss 0.24125 train acc 0.09399 valid acc 0.07812\n",
      "epoch 363: train loss 0.23992, valid loss 0.24124 train acc 0.09399 valid acc 0.07812\n",
      "epoch 364: train loss 0.23991, valid loss 0.24124 train acc 0.09399 valid acc 0.07812\n",
      "epoch 365: train loss 0.23991, valid loss 0.24123 train acc 0.09399 valid acc 0.07812\n",
      "epoch 366: train loss 0.23990, valid loss 0.24123 train acc 0.09399 valid acc 0.07812\n",
      "epoch 367: train loss 0.23990, valid loss 0.24122 train acc 0.09399 valid acc 0.07812\n",
      "epoch 368: train loss 0.23989, valid loss 0.24122 train acc 0.09399 valid acc 0.08008\n",
      "epoch 369: train loss 0.23989, valid loss 0.24121 train acc 0.09399 valid acc 0.08008\n",
      "epoch 370: train loss 0.23988, valid loss 0.24121 train acc 0.09399 valid acc 0.08008\n",
      "epoch 371: train loss 0.23988, valid loss 0.24120 train acc 0.09399 valid acc 0.08008\n",
      "epoch 372: train loss 0.23987, valid loss 0.24120 train acc 0.09399 valid acc 0.08008\n",
      "epoch 373: train loss 0.23987, valid loss 0.24119 train acc 0.09399 valid acc 0.08008\n",
      "epoch 374: train loss 0.23986, valid loss 0.24119 train acc 0.09399 valid acc 0.08008\n",
      "epoch 375: train loss 0.23986, valid loss 0.24118 train acc 0.09399 valid acc 0.08008\n",
      "epoch 376: train loss 0.23985, valid loss 0.24118 train acc 0.09399 valid acc 0.08008\n",
      "epoch 377: train loss 0.23985, valid loss 0.24117 train acc 0.09399 valid acc 0.08008\n",
      "epoch 378: train loss 0.23984, valid loss 0.24117 train acc 0.09399 valid acc 0.08008\n",
      "epoch 379: train loss 0.23984, valid loss 0.24116 train acc 0.09399 valid acc 0.08008\n",
      "epoch 380: train loss 0.23983, valid loss 0.24116 train acc 0.09399 valid acc 0.08008\n",
      "epoch 381: train loss 0.23983, valid loss 0.24115 train acc 0.09399 valid acc 0.08008\n",
      "epoch 382: train loss 0.23982, valid loss 0.24115 train acc 0.09399 valid acc 0.08008\n",
      "epoch 383: train loss 0.23982, valid loss 0.24114 train acc 0.09399 valid acc 0.08008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 384: train loss 0.23981, valid loss 0.24114 train acc 0.09399 valid acc 0.08008\n",
      "epoch 385: train loss 0.23981, valid loss 0.24113 train acc 0.09399 valid acc 0.08008\n",
      "epoch 386: train loss 0.23980, valid loss 0.24113 train acc 0.09399 valid acc 0.08008\n",
      "epoch 387: train loss 0.23980, valid loss 0.24112 train acc 0.09399 valid acc 0.08008\n",
      "epoch 388: train loss 0.23979, valid loss 0.24112 train acc 0.09399 valid acc 0.08008\n",
      "epoch 389: train loss 0.23979, valid loss 0.24111 train acc 0.09399 valid acc 0.08008\n",
      "epoch 390: train loss 0.23978, valid loss 0.24111 train acc 0.09399 valid acc 0.08008\n",
      "epoch 391: train loss 0.23978, valid loss 0.24110 train acc 0.09399 valid acc 0.08008\n",
      "epoch 392: train loss 0.23977, valid loss 0.24110 train acc 0.09399 valid acc 0.08008\n",
      "epoch 393: train loss 0.23977, valid loss 0.24109 train acc 0.09399 valid acc 0.08008\n",
      "epoch 394: train loss 0.23976, valid loss 0.24109 train acc 0.09375 valid acc 0.08008\n",
      "epoch 395: train loss 0.23976, valid loss 0.24108 train acc 0.09375 valid acc 0.08008\n",
      "epoch 396: train loss 0.23975, valid loss 0.24107 train acc 0.09375 valid acc 0.08008\n",
      "epoch 397: train loss 0.23975, valid loss 0.24107 train acc 0.09375 valid acc 0.08008\n",
      "epoch 398: train loss 0.23974, valid loss 0.24106 train acc 0.09375 valid acc 0.08008\n",
      "epoch 399: train loss 0.23974, valid loss 0.24106 train acc 0.09375 valid acc 0.08008\n",
      "epoch 400: train loss 0.23973, valid loss 0.24105 train acc 0.09375 valid acc 0.08008\n",
      "epoch 401: train loss 0.23973, valid loss 0.24105 train acc 0.09375 valid acc 0.08008\n",
      "epoch 402: train loss 0.23972, valid loss 0.24104 train acc 0.09375 valid acc 0.08008\n",
      "epoch 403: train loss 0.23972, valid loss 0.24104 train acc 0.09375 valid acc 0.08008\n",
      "epoch 404: train loss 0.23971, valid loss 0.24103 train acc 0.09375 valid acc 0.08008\n",
      "epoch 405: train loss 0.23971, valid loss 0.24103 train acc 0.09375 valid acc 0.08008\n",
      "epoch 406: train loss 0.23970, valid loss 0.24102 train acc 0.09375 valid acc 0.08008\n",
      "epoch 407: train loss 0.23970, valid loss 0.24102 train acc 0.09351 valid acc 0.08008\n",
      "epoch 408: train loss 0.23969, valid loss 0.24101 train acc 0.09351 valid acc 0.08008\n",
      "epoch 409: train loss 0.23969, valid loss 0.24101 train acc 0.09351 valid acc 0.08008\n",
      "epoch 410: train loss 0.23968, valid loss 0.24100 train acc 0.09351 valid acc 0.08008\n",
      "epoch 411: train loss 0.23968, valid loss 0.24100 train acc 0.09351 valid acc 0.08008\n",
      "epoch 412: train loss 0.23967, valid loss 0.24099 train acc 0.09351 valid acc 0.08008\n",
      "epoch 413: train loss 0.23967, valid loss 0.24099 train acc 0.09351 valid acc 0.08008\n",
      "epoch 414: train loss 0.23966, valid loss 0.24098 train acc 0.09351 valid acc 0.08008\n",
      "epoch 415: train loss 0.23966, valid loss 0.24098 train acc 0.09351 valid acc 0.08008\n",
      "epoch 416: train loss 0.23965, valid loss 0.24097 train acc 0.09351 valid acc 0.08008\n",
      "epoch 417: train loss 0.23965, valid loss 0.24097 train acc 0.09351 valid acc 0.08008\n",
      "epoch 418: train loss 0.23964, valid loss 0.24096 train acc 0.09351 valid acc 0.08008\n",
      "epoch 419: train loss 0.23964, valid loss 0.24096 train acc 0.09351 valid acc 0.08008\n",
      "epoch 420: train loss 0.23963, valid loss 0.24095 train acc 0.09351 valid acc 0.08008\n",
      "epoch 421: train loss 0.23963, valid loss 0.24095 train acc 0.09351 valid acc 0.08008\n",
      "epoch 422: train loss 0.23962, valid loss 0.24094 train acc 0.09351 valid acc 0.08008\n",
      "epoch 423: train loss 0.23962, valid loss 0.24094 train acc 0.09326 valid acc 0.08008\n",
      "epoch 424: train loss 0.23961, valid loss 0.24093 train acc 0.09326 valid acc 0.08008\n",
      "epoch 425: train loss 0.23961, valid loss 0.24093 train acc 0.09326 valid acc 0.08008\n",
      "epoch 426: train loss 0.23960, valid loss 0.24093 train acc 0.09326 valid acc 0.08008\n",
      "epoch 427: train loss 0.23960, valid loss 0.24092 train acc 0.09326 valid acc 0.08008\n",
      "epoch 428: train loss 0.23959, valid loss 0.24092 train acc 0.09326 valid acc 0.08008\n",
      "epoch 429: train loss 0.23959, valid loss 0.24091 train acc 0.09326 valid acc 0.08008\n",
      "epoch 430: train loss 0.23958, valid loss 0.24091 train acc 0.09326 valid acc 0.08008\n",
      "epoch 431: train loss 0.23958, valid loss 0.24090 train acc 0.09326 valid acc 0.08008\n",
      "epoch 432: train loss 0.23957, valid loss 0.24090 train acc 0.09326 valid acc 0.08008\n",
      "epoch 433: train loss 0.23957, valid loss 0.24089 train acc 0.09326 valid acc 0.08008\n",
      "epoch 434: train loss 0.23956, valid loss 0.24089 train acc 0.09326 valid acc 0.08008\n",
      "epoch 435: train loss 0.23956, valid loss 0.24088 train acc 0.09351 valid acc 0.08008\n",
      "epoch 436: train loss 0.23955, valid loss 0.24088 train acc 0.09326 valid acc 0.08008\n",
      "epoch 437: train loss 0.23955, valid loss 0.24087 train acc 0.09326 valid acc 0.08008\n",
      "epoch 438: train loss 0.23954, valid loss 0.24087 train acc 0.09326 valid acc 0.08008\n",
      "epoch 439: train loss 0.23954, valid loss 0.24086 train acc 0.09326 valid acc 0.08008\n",
      "epoch 440: train loss 0.23953, valid loss 0.24086 train acc 0.09326 valid acc 0.08008\n",
      "epoch 441: train loss 0.23953, valid loss 0.24085 train acc 0.09326 valid acc 0.08008\n",
      "epoch 442: train loss 0.23952, valid loss 0.24085 train acc 0.09326 valid acc 0.08008\n",
      "epoch 443: train loss 0.23952, valid loss 0.24084 train acc 0.09326 valid acc 0.08008\n",
      "epoch 444: train loss 0.23951, valid loss 0.24084 train acc 0.09326 valid acc 0.08008\n",
      "epoch 445: train loss 0.23951, valid loss 0.24083 train acc 0.09326 valid acc 0.08008\n",
      "epoch 446: train loss 0.23950, valid loss 0.24083 train acc 0.09326 valid acc 0.08008\n",
      "epoch 447: train loss 0.23950, valid loss 0.24082 train acc 0.09326 valid acc 0.08008\n",
      "epoch 448: train loss 0.23949, valid loss 0.24082 train acc 0.09326 valid acc 0.08008\n",
      "epoch 449: train loss 0.23949, valid loss 0.24081 train acc 0.09326 valid acc 0.08008\n",
      "epoch 450: train loss 0.23948, valid loss 0.24081 train acc 0.09326 valid acc 0.08008\n",
      "epoch 451: train loss 0.23948, valid loss 0.24080 train acc 0.09326 valid acc 0.08008\n",
      "epoch 452: train loss 0.23947, valid loss 0.24080 train acc 0.09302 valid acc 0.08008\n",
      "epoch 453: train loss 0.23947, valid loss 0.24079 train acc 0.09326 valid acc 0.08008\n",
      "epoch 454: train loss 0.23946, valid loss 0.24079 train acc 0.09326 valid acc 0.08008\n",
      "epoch 455: train loss 0.23946, valid loss 0.24078 train acc 0.09326 valid acc 0.08008\n",
      "epoch 456: train loss 0.23945, valid loss 0.24078 train acc 0.09326 valid acc 0.08008\n",
      "epoch 457: train loss 0.23945, valid loss 0.24077 train acc 0.09326 valid acc 0.08008\n",
      "epoch 458: train loss 0.23944, valid loss 0.24077 train acc 0.09326 valid acc 0.08008\n",
      "epoch 459: train loss 0.23944, valid loss 0.24076 train acc 0.09326 valid acc 0.08008\n",
      "epoch 460: train loss 0.23943, valid loss 0.24076 train acc 0.09326 valid acc 0.08008\n",
      "epoch 461: train loss 0.23943, valid loss 0.24075 train acc 0.09326 valid acc 0.08008\n",
      "epoch 462: train loss 0.23942, valid loss 0.24075 train acc 0.09326 valid acc 0.08008\n",
      "epoch 463: train loss 0.23942, valid loss 0.24074 train acc 0.09326 valid acc 0.08008\n",
      "epoch 464: train loss 0.23941, valid loss 0.24074 train acc 0.09326 valid acc 0.08008\n",
      "epoch 465: train loss 0.23941, valid loss 0.24073 train acc 0.09326 valid acc 0.08008\n",
      "epoch 466: train loss 0.23941, valid loss 0.24073 train acc 0.09326 valid acc 0.08008\n",
      "epoch 467: train loss 0.23940, valid loss 0.24072 train acc 0.09326 valid acc 0.08008\n",
      "epoch 468: train loss 0.23940, valid loss 0.24072 train acc 0.09326 valid acc 0.08008\n",
      "epoch 469: train loss 0.23939, valid loss 0.24072 train acc 0.09326 valid acc 0.08008\n",
      "epoch 470: train loss 0.23939, valid loss 0.24071 train acc 0.09326 valid acc 0.08008\n",
      "epoch 471: train loss 0.23938, valid loss 0.24071 train acc 0.09326 valid acc 0.08008\n",
      "epoch 472: train loss 0.23938, valid loss 0.24070 train acc 0.09326 valid acc 0.08008\n",
      "epoch 473: train loss 0.23937, valid loss 0.24070 train acc 0.09326 valid acc 0.08008\n",
      "epoch 474: train loss 0.23937, valid loss 0.24069 train acc 0.09326 valid acc 0.08008\n",
      "epoch 475: train loss 0.23936, valid loss 0.24069 train acc 0.09326 valid acc 0.08008\n",
      "epoch 476: train loss 0.23936, valid loss 0.24068 train acc 0.09326 valid acc 0.08008\n",
      "epoch 477: train loss 0.23935, valid loss 0.24068 train acc 0.09302 valid acc 0.08008\n",
      "epoch 478: train loss 0.23935, valid loss 0.24067 train acc 0.09302 valid acc 0.08008\n",
      "epoch 479: train loss 0.23934, valid loss 0.24067 train acc 0.09302 valid acc 0.08008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 480: train loss 0.23934, valid loss 0.24066 train acc 0.09302 valid acc 0.08008\n",
      "epoch 481: train loss 0.23933, valid loss 0.24066 train acc 0.09302 valid acc 0.08008\n",
      "epoch 482: train loss 0.23933, valid loss 0.24065 train acc 0.09302 valid acc 0.08008\n",
      "epoch 483: train loss 0.23932, valid loss 0.24065 train acc 0.09302 valid acc 0.08008\n",
      "epoch 484: train loss 0.23932, valid loss 0.24064 train acc 0.09302 valid acc 0.08008\n",
      "epoch 485: train loss 0.23931, valid loss 0.24064 train acc 0.09326 valid acc 0.08008\n",
      "epoch 486: train loss 0.23931, valid loss 0.24063 train acc 0.09326 valid acc 0.08008\n",
      "epoch 487: train loss 0.23930, valid loss 0.24063 train acc 0.09326 valid acc 0.08008\n",
      "epoch 488: train loss 0.23930, valid loss 0.24062 train acc 0.09326 valid acc 0.08008\n",
      "epoch 489: train loss 0.23929, valid loss 0.24062 train acc 0.09326 valid acc 0.08008\n",
      "epoch 490: train loss 0.23929, valid loss 0.24061 train acc 0.09326 valid acc 0.08008\n",
      "epoch 491: train loss 0.23929, valid loss 0.24061 train acc 0.09326 valid acc 0.08008\n",
      "epoch 492: train loss 0.23928, valid loss 0.24061 train acc 0.09326 valid acc 0.08008\n",
      "epoch 493: train loss 0.23928, valid loss 0.24060 train acc 0.09326 valid acc 0.08008\n",
      "epoch 494: train loss 0.23927, valid loss 0.24060 train acc 0.09302 valid acc 0.08008\n",
      "epoch 495: train loss 0.23927, valid loss 0.24059 train acc 0.09302 valid acc 0.08008\n",
      "epoch 496: train loss 0.23926, valid loss 0.24059 train acc 0.09302 valid acc 0.08008\n",
      "epoch 497: train loss 0.23926, valid loss 0.24058 train acc 0.09302 valid acc 0.08008\n",
      "epoch 498: train loss 0.23925, valid loss 0.24058 train acc 0.09302 valid acc 0.08008\n",
      "epoch 499: train loss 0.23925, valid loss 0.24057 train acc 0.09302 valid acc 0.08008\n",
      "epoch 500: train loss 0.23924, valid loss 0.24057 train acc 0.09302 valid acc 0.08008\n",
      "epoch 501: train loss 0.23924, valid loss 0.24056 train acc 0.09302 valid acc 0.08008\n",
      "epoch 502: train loss 0.23923, valid loss 0.24056 train acc 0.09302 valid acc 0.08008\n",
      "epoch 503: train loss 0.23923, valid loss 0.24055 train acc 0.09302 valid acc 0.08008\n",
      "epoch 504: train loss 0.23922, valid loss 0.24055 train acc 0.09302 valid acc 0.08008\n",
      "epoch 505: train loss 0.23922, valid loss 0.24054 train acc 0.09302 valid acc 0.08008\n",
      "epoch 506: train loss 0.23921, valid loss 0.24054 train acc 0.09302 valid acc 0.08008\n",
      "epoch 507: train loss 0.23921, valid loss 0.24053 train acc 0.09302 valid acc 0.08008\n",
      "epoch 508: train loss 0.23920, valid loss 0.24053 train acc 0.09302 valid acc 0.08008\n",
      "epoch 509: train loss 0.23920, valid loss 0.24053 train acc 0.09302 valid acc 0.08008\n",
      "epoch 510: train loss 0.23920, valid loss 0.24052 train acc 0.09302 valid acc 0.08008\n",
      "epoch 511: train loss 0.23919, valid loss 0.24052 train acc 0.09302 valid acc 0.08008\n",
      "epoch 512: train loss 0.23919, valid loss 0.24051 train acc 0.09302 valid acc 0.08008\n",
      "epoch 513: train loss 0.23918, valid loss 0.24051 train acc 0.09302 valid acc 0.08008\n",
      "epoch 514: train loss 0.23918, valid loss 0.24050 train acc 0.09302 valid acc 0.08008\n",
      "epoch 515: train loss 0.23917, valid loss 0.24050 train acc 0.09302 valid acc 0.08008\n",
      "epoch 516: train loss 0.23917, valid loss 0.24049 train acc 0.09302 valid acc 0.08008\n",
      "epoch 517: train loss 0.23916, valid loss 0.24049 train acc 0.09302 valid acc 0.08008\n",
      "epoch 518: train loss 0.23916, valid loss 0.24048 train acc 0.09302 valid acc 0.08008\n",
      "epoch 519: train loss 0.23915, valid loss 0.24048 train acc 0.09302 valid acc 0.08008\n",
      "epoch 520: train loss 0.23915, valid loss 0.24047 train acc 0.09302 valid acc 0.08008\n",
      "epoch 521: train loss 0.23914, valid loss 0.24047 train acc 0.09302 valid acc 0.08008\n",
      "epoch 522: train loss 0.23914, valid loss 0.24046 train acc 0.09302 valid acc 0.08008\n",
      "epoch 523: train loss 0.23913, valid loss 0.24046 train acc 0.09302 valid acc 0.08008\n",
      "epoch 524: train loss 0.23913, valid loss 0.24046 train acc 0.09302 valid acc 0.08008\n",
      "epoch 525: train loss 0.23913, valid loss 0.24045 train acc 0.09302 valid acc 0.08008\n",
      "epoch 526: train loss 0.23912, valid loss 0.24045 train acc 0.09302 valid acc 0.08008\n",
      "epoch 527: train loss 0.23912, valid loss 0.24044 train acc 0.09302 valid acc 0.08008\n",
      "epoch 528: train loss 0.23911, valid loss 0.24044 train acc 0.09302 valid acc 0.08008\n",
      "epoch 529: train loss 0.23911, valid loss 0.24043 train acc 0.09302 valid acc 0.08008\n",
      "epoch 530: train loss 0.23910, valid loss 0.24043 train acc 0.09302 valid acc 0.08008\n",
      "epoch 531: train loss 0.23910, valid loss 0.24042 train acc 0.09302 valid acc 0.08008\n",
      "epoch 532: train loss 0.23909, valid loss 0.24042 train acc 0.09302 valid acc 0.08008\n",
      "epoch 533: train loss 0.23909, valid loss 0.24041 train acc 0.09302 valid acc 0.08008\n",
      "epoch 534: train loss 0.23908, valid loss 0.24041 train acc 0.09302 valid acc 0.08008\n",
      "epoch 535: train loss 0.23908, valid loss 0.24040 train acc 0.09302 valid acc 0.08008\n",
      "epoch 536: train loss 0.23907, valid loss 0.24040 train acc 0.09326 valid acc 0.08008\n",
      "epoch 537: train loss 0.23907, valid loss 0.24040 train acc 0.09326 valid acc 0.08008\n",
      "epoch 538: train loss 0.23907, valid loss 0.24039 train acc 0.09326 valid acc 0.08008\n",
      "epoch 539: train loss 0.23906, valid loss 0.24039 train acc 0.09326 valid acc 0.08008\n",
      "epoch 540: train loss 0.23906, valid loss 0.24038 train acc 0.09302 valid acc 0.08008\n",
      "epoch 541: train loss 0.23905, valid loss 0.24038 train acc 0.09302 valid acc 0.08008\n",
      "epoch 542: train loss 0.23905, valid loss 0.24037 train acc 0.09302 valid acc 0.08008\n",
      "epoch 543: train loss 0.23904, valid loss 0.24037 train acc 0.09302 valid acc 0.08008\n",
      "epoch 544: train loss 0.23904, valid loss 0.24036 train acc 0.09302 valid acc 0.08008\n",
      "epoch 545: train loss 0.23903, valid loss 0.24036 train acc 0.09302 valid acc 0.08008\n",
      "epoch 546: train loss 0.23903, valid loss 0.24035 train acc 0.09302 valid acc 0.08008\n",
      "epoch 547: train loss 0.23902, valid loss 0.24035 train acc 0.09302 valid acc 0.08008\n",
      "epoch 548: train loss 0.23902, valid loss 0.24035 train acc 0.09302 valid acc 0.08008\n",
      "epoch 549: train loss 0.23902, valid loss 0.24034 train acc 0.09302 valid acc 0.08008\n",
      "epoch 550: train loss 0.23901, valid loss 0.24034 train acc 0.09302 valid acc 0.08008\n",
      "epoch 551: train loss 0.23901, valid loss 0.24033 train acc 0.09302 valid acc 0.08008\n",
      "epoch 552: train loss 0.23900, valid loss 0.24033 train acc 0.09302 valid acc 0.08008\n",
      "epoch 553: train loss 0.23900, valid loss 0.24032 train acc 0.09302 valid acc 0.08008\n",
      "epoch 554: train loss 0.23899, valid loss 0.24032 train acc 0.09302 valid acc 0.08008\n",
      "epoch 555: train loss 0.23899, valid loss 0.24031 train acc 0.09302 valid acc 0.08008\n",
      "epoch 556: train loss 0.23898, valid loss 0.24031 train acc 0.09302 valid acc 0.08008\n",
      "epoch 557: train loss 0.23898, valid loss 0.24030 train acc 0.09302 valid acc 0.08008\n",
      "epoch 558: train loss 0.23897, valid loss 0.24030 train acc 0.09302 valid acc 0.08008\n",
      "epoch 559: train loss 0.23897, valid loss 0.24030 train acc 0.09302 valid acc 0.08008\n",
      "epoch 560: train loss 0.23897, valid loss 0.24029 train acc 0.09302 valid acc 0.08008\n",
      "epoch 561: train loss 0.23896, valid loss 0.24029 train acc 0.09302 valid acc 0.08008\n",
      "epoch 562: train loss 0.23896, valid loss 0.24028 train acc 0.09302 valid acc 0.08008\n",
      "epoch 563: train loss 0.23895, valid loss 0.24028 train acc 0.09302 valid acc 0.08008\n",
      "epoch 564: train loss 0.23895, valid loss 0.24027 train acc 0.09302 valid acc 0.08008\n",
      "epoch 565: train loss 0.23894, valid loss 0.24027 train acc 0.09302 valid acc 0.08008\n",
      "epoch 566: train loss 0.23894, valid loss 0.24026 train acc 0.09302 valid acc 0.08008\n",
      "epoch 567: train loss 0.23893, valid loss 0.24026 train acc 0.09302 valid acc 0.08008\n",
      "epoch 568: train loss 0.23893, valid loss 0.24026 train acc 0.09302 valid acc 0.08008\n",
      "epoch 569: train loss 0.23892, valid loss 0.24025 train acc 0.09302 valid acc 0.08008\n",
      "epoch 570: train loss 0.23892, valid loss 0.24025 train acc 0.09302 valid acc 0.08008\n",
      "epoch 571: train loss 0.23892, valid loss 0.24024 train acc 0.09302 valid acc 0.08008\n",
      "epoch 572: train loss 0.23891, valid loss 0.24024 train acc 0.09277 valid acc 0.08008\n",
      "epoch 573: train loss 0.23891, valid loss 0.24023 train acc 0.09277 valid acc 0.08008\n",
      "epoch 574: train loss 0.23890, valid loss 0.24023 train acc 0.09277 valid acc 0.08008\n",
      "epoch 575: train loss 0.23890, valid loss 0.24022 train acc 0.09277 valid acc 0.08008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 576: train loss 0.23889, valid loss 0.24022 train acc 0.09302 valid acc 0.08008\n",
      "epoch 577: train loss 0.23889, valid loss 0.24022 train acc 0.09302 valid acc 0.08008\n",
      "epoch 578: train loss 0.23888, valid loss 0.24021 train acc 0.09302 valid acc 0.08008\n",
      "epoch 579: train loss 0.23888, valid loss 0.24021 train acc 0.09302 valid acc 0.08008\n",
      "epoch 580: train loss 0.23888, valid loss 0.24020 train acc 0.09302 valid acc 0.08008\n",
      "epoch 581: train loss 0.23887, valid loss 0.24020 train acc 0.09302 valid acc 0.08008\n",
      "epoch 582: train loss 0.23887, valid loss 0.24019 train acc 0.09302 valid acc 0.08008\n",
      "epoch 583: train loss 0.23886, valid loss 0.24019 train acc 0.09302 valid acc 0.08008\n",
      "epoch 584: train loss 0.23886, valid loss 0.24018 train acc 0.09302 valid acc 0.08008\n",
      "epoch 585: train loss 0.23885, valid loss 0.24018 train acc 0.09302 valid acc 0.08008\n",
      "epoch 586: train loss 0.23885, valid loss 0.24018 train acc 0.09302 valid acc 0.08008\n",
      "epoch 587: train loss 0.23884, valid loss 0.24017 train acc 0.09302 valid acc 0.08008\n",
      "epoch 588: train loss 0.23884, valid loss 0.24017 train acc 0.09302 valid acc 0.08008\n",
      "epoch 589: train loss 0.23884, valid loss 0.24016 train acc 0.09302 valid acc 0.08008\n",
      "epoch 590: train loss 0.23883, valid loss 0.24016 train acc 0.09302 valid acc 0.08008\n",
      "epoch 591: train loss 0.23883, valid loss 0.24015 train acc 0.09302 valid acc 0.08008\n",
      "epoch 592: train loss 0.23882, valid loss 0.24015 train acc 0.09302 valid acc 0.08008\n",
      "epoch 593: train loss 0.23882, valid loss 0.24014 train acc 0.09302 valid acc 0.08008\n",
      "epoch 594: train loss 0.23881, valid loss 0.24014 train acc 0.09302 valid acc 0.08008\n",
      "epoch 595: train loss 0.23881, valid loss 0.24014 train acc 0.09302 valid acc 0.08008\n",
      "epoch 596: train loss 0.23880, valid loss 0.24013 train acc 0.09302 valid acc 0.08008\n",
      "epoch 597: train loss 0.23880, valid loss 0.24013 train acc 0.09302 valid acc 0.08008\n",
      "epoch 598: train loss 0.23880, valid loss 0.24012 train acc 0.09302 valid acc 0.08008\n",
      "epoch 599: train loss 0.23879, valid loss 0.24012 train acc 0.09302 valid acc 0.08008\n",
      "epoch 600: train loss 0.23879, valid loss 0.24011 train acc 0.09302 valid acc 0.08008\n",
      "epoch 601: train loss 0.23878, valid loss 0.24011 train acc 0.09302 valid acc 0.08008\n",
      "epoch 602: train loss 0.23878, valid loss 0.24011 train acc 0.09302 valid acc 0.08008\n",
      "epoch 603: train loss 0.23877, valid loss 0.24010 train acc 0.09302 valid acc 0.08008\n",
      "epoch 604: train loss 0.23877, valid loss 0.24010 train acc 0.09302 valid acc 0.08008\n",
      "epoch 605: train loss 0.23876, valid loss 0.24009 train acc 0.09302 valid acc 0.08008\n",
      "epoch 606: train loss 0.23876, valid loss 0.24009 train acc 0.09302 valid acc 0.08008\n",
      "epoch 607: train loss 0.23876, valid loss 0.24008 train acc 0.09302 valid acc 0.08008\n",
      "epoch 608: train loss 0.23875, valid loss 0.24008 train acc 0.09302 valid acc 0.08008\n",
      "epoch 609: train loss 0.23875, valid loss 0.24007 train acc 0.09302 valid acc 0.08008\n",
      "epoch 610: train loss 0.23874, valid loss 0.24007 train acc 0.09302 valid acc 0.08008\n",
      "epoch 611: train loss 0.23874, valid loss 0.24007 train acc 0.09302 valid acc 0.08008\n",
      "epoch 612: train loss 0.23873, valid loss 0.24006 train acc 0.09302 valid acc 0.08008\n",
      "epoch 613: train loss 0.23873, valid loss 0.24006 train acc 0.09302 valid acc 0.08008\n",
      "epoch 614: train loss 0.23873, valid loss 0.24005 train acc 0.09302 valid acc 0.08008\n",
      "epoch 615: train loss 0.23872, valid loss 0.24005 train acc 0.09302 valid acc 0.08008\n",
      "epoch 616: train loss 0.23872, valid loss 0.24004 train acc 0.09302 valid acc 0.08008\n",
      "epoch 617: train loss 0.23871, valid loss 0.24004 train acc 0.09302 valid acc 0.08008\n",
      "epoch 618: train loss 0.23871, valid loss 0.24004 train acc 0.09302 valid acc 0.08008\n",
      "epoch 619: train loss 0.23870, valid loss 0.24003 train acc 0.09302 valid acc 0.08008\n",
      "epoch 620: train loss 0.23870, valid loss 0.24003 train acc 0.09302 valid acc 0.08008\n",
      "epoch 621: train loss 0.23869, valid loss 0.24002 train acc 0.09302 valid acc 0.08008\n",
      "epoch 622: train loss 0.23869, valid loss 0.24002 train acc 0.09302 valid acc 0.08008\n",
      "epoch 623: train loss 0.23869, valid loss 0.24001 train acc 0.09302 valid acc 0.08008\n",
      "epoch 624: train loss 0.23868, valid loss 0.24001 train acc 0.09302 valid acc 0.08008\n",
      "epoch 625: train loss 0.23868, valid loss 0.24001 train acc 0.09302 valid acc 0.08008\n",
      "epoch 626: train loss 0.23867, valid loss 0.24000 train acc 0.09302 valid acc 0.08008\n",
      "epoch 627: train loss 0.23867, valid loss 0.24000 train acc 0.09302 valid acc 0.08008\n",
      "epoch 628: train loss 0.23866, valid loss 0.23999 train acc 0.09302 valid acc 0.08008\n",
      "epoch 629: train loss 0.23866, valid loss 0.23999 train acc 0.09302 valid acc 0.08008\n",
      "epoch 630: train loss 0.23866, valid loss 0.23998 train acc 0.09302 valid acc 0.08008\n",
      "epoch 631: train loss 0.23865, valid loss 0.23998 train acc 0.09302 valid acc 0.08008\n",
      "epoch 632: train loss 0.23865, valid loss 0.23998 train acc 0.09302 valid acc 0.08008\n",
      "epoch 633: train loss 0.23864, valid loss 0.23997 train acc 0.09302 valid acc 0.08008\n",
      "epoch 634: train loss 0.23864, valid loss 0.23997 train acc 0.09302 valid acc 0.08008\n",
      "epoch 635: train loss 0.23863, valid loss 0.23996 train acc 0.09302 valid acc 0.08008\n",
      "epoch 636: train loss 0.23863, valid loss 0.23996 train acc 0.09302 valid acc 0.08008\n",
      "epoch 637: train loss 0.23863, valid loss 0.23995 train acc 0.09302 valid acc 0.08008\n",
      "epoch 638: train loss 0.23862, valid loss 0.23995 train acc 0.09302 valid acc 0.08008\n",
      "epoch 639: train loss 0.23862, valid loss 0.23995 train acc 0.09302 valid acc 0.08008\n",
      "epoch 640: train loss 0.23861, valid loss 0.23994 train acc 0.09302 valid acc 0.08008\n",
      "epoch 641: train loss 0.23861, valid loss 0.23994 train acc 0.09302 valid acc 0.08008\n",
      "epoch 642: train loss 0.23860, valid loss 0.23993 train acc 0.09302 valid acc 0.08008\n",
      "epoch 643: train loss 0.23860, valid loss 0.23993 train acc 0.09302 valid acc 0.08008\n",
      "epoch 644: train loss 0.23860, valid loss 0.23992 train acc 0.09302 valid acc 0.08008\n",
      "epoch 645: train loss 0.23859, valid loss 0.23992 train acc 0.09302 valid acc 0.08008\n",
      "epoch 646: train loss 0.23859, valid loss 0.23992 train acc 0.09302 valid acc 0.08008\n",
      "epoch 647: train loss 0.23858, valid loss 0.23991 train acc 0.09302 valid acc 0.08008\n",
      "epoch 648: train loss 0.23858, valid loss 0.23991 train acc 0.09302 valid acc 0.08008\n",
      "epoch 649: train loss 0.23857, valid loss 0.23990 train acc 0.09302 valid acc 0.08008\n",
      "epoch 650: train loss 0.23857, valid loss 0.23990 train acc 0.09302 valid acc 0.08008\n",
      "epoch 651: train loss 0.23857, valid loss 0.23989 train acc 0.09302 valid acc 0.08008\n",
      "epoch 652: train loss 0.23856, valid loss 0.23989 train acc 0.09302 valid acc 0.08008\n",
      "epoch 653: train loss 0.23856, valid loss 0.23989 train acc 0.09302 valid acc 0.08008\n",
      "epoch 654: train loss 0.23855, valid loss 0.23988 train acc 0.09302 valid acc 0.08008\n",
      "epoch 655: train loss 0.23855, valid loss 0.23988 train acc 0.09326 valid acc 0.08008\n",
      "epoch 656: train loss 0.23854, valid loss 0.23987 train acc 0.09326 valid acc 0.08008\n",
      "epoch 657: train loss 0.23854, valid loss 0.23987 train acc 0.09326 valid acc 0.08008\n",
      "epoch 658: train loss 0.23854, valid loss 0.23987 train acc 0.09326 valid acc 0.08008\n",
      "epoch 659: train loss 0.23853, valid loss 0.23986 train acc 0.09326 valid acc 0.08008\n",
      "epoch 660: train loss 0.23853, valid loss 0.23986 train acc 0.09326 valid acc 0.08008\n",
      "epoch 661: train loss 0.23852, valid loss 0.23985 train acc 0.09326 valid acc 0.08008\n",
      "epoch 662: train loss 0.23852, valid loss 0.23985 train acc 0.09326 valid acc 0.08008\n",
      "epoch 663: train loss 0.23851, valid loss 0.23984 train acc 0.09326 valid acc 0.08008\n",
      "epoch 664: train loss 0.23851, valid loss 0.23984 train acc 0.09326 valid acc 0.08008\n",
      "epoch 665: train loss 0.23851, valid loss 0.23984 train acc 0.09326 valid acc 0.08008\n",
      "epoch 666: train loss 0.23850, valid loss 0.23983 train acc 0.09326 valid acc 0.08008\n",
      "epoch 667: train loss 0.23850, valid loss 0.23983 train acc 0.09326 valid acc 0.08008\n",
      "epoch 668: train loss 0.23849, valid loss 0.23982 train acc 0.09326 valid acc 0.08008\n",
      "epoch 669: train loss 0.23849, valid loss 0.23982 train acc 0.09326 valid acc 0.08008\n",
      "epoch 670: train loss 0.23849, valid loss 0.23981 train acc 0.09326 valid acc 0.08008\n",
      "epoch 671: train loss 0.23848, valid loss 0.23981 train acc 0.09302 valid acc 0.08008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 672: train loss 0.23848, valid loss 0.23981 train acc 0.09302 valid acc 0.08008\n",
      "epoch 673: train loss 0.23847, valid loss 0.23980 train acc 0.09302 valid acc 0.08008\n",
      "epoch 674: train loss 0.23847, valid loss 0.23980 train acc 0.09302 valid acc 0.08008\n",
      "epoch 675: train loss 0.23846, valid loss 0.23979 train acc 0.09302 valid acc 0.08008\n",
      "epoch 676: train loss 0.23846, valid loss 0.23979 train acc 0.09302 valid acc 0.08008\n",
      "epoch 677: train loss 0.23846, valid loss 0.23979 train acc 0.09302 valid acc 0.08008\n",
      "epoch 678: train loss 0.23845, valid loss 0.23978 train acc 0.09302 valid acc 0.08008\n",
      "epoch 679: train loss 0.23845, valid loss 0.23978 train acc 0.09302 valid acc 0.08008\n",
      "epoch 680: train loss 0.23844, valid loss 0.23977 train acc 0.09302 valid acc 0.08008\n",
      "epoch 681: train loss 0.23844, valid loss 0.23977 train acc 0.09302 valid acc 0.08008\n",
      "epoch 682: train loss 0.23843, valid loss 0.23976 train acc 0.09326 valid acc 0.08008\n",
      "epoch 683: train loss 0.23843, valid loss 0.23976 train acc 0.09326 valid acc 0.08008\n",
      "epoch 684: train loss 0.23843, valid loss 0.23976 train acc 0.09326 valid acc 0.08008\n",
      "epoch 685: train loss 0.23842, valid loss 0.23975 train acc 0.09326 valid acc 0.08008\n",
      "epoch 686: train loss 0.23842, valid loss 0.23975 train acc 0.09326 valid acc 0.08008\n",
      "epoch 687: train loss 0.23841, valid loss 0.23974 train acc 0.09326 valid acc 0.08008\n",
      "epoch 688: train loss 0.23841, valid loss 0.23974 train acc 0.09326 valid acc 0.08008\n",
      "epoch 689: train loss 0.23841, valid loss 0.23974 train acc 0.09326 valid acc 0.08008\n",
      "epoch 690: train loss 0.23840, valid loss 0.23973 train acc 0.09326 valid acc 0.08008\n",
      "epoch 691: train loss 0.23840, valid loss 0.23973 train acc 0.09326 valid acc 0.08008\n",
      "epoch 692: train loss 0.23839, valid loss 0.23972 train acc 0.09326 valid acc 0.08008\n",
      "epoch 693: train loss 0.23839, valid loss 0.23972 train acc 0.09326 valid acc 0.08008\n",
      "epoch 694: train loss 0.23838, valid loss 0.23972 train acc 0.09326 valid acc 0.08008\n",
      "epoch 695: train loss 0.23838, valid loss 0.23971 train acc 0.09326 valid acc 0.08008\n",
      "epoch 696: train loss 0.23838, valid loss 0.23971 train acc 0.09326 valid acc 0.08008\n",
      "epoch 697: train loss 0.23837, valid loss 0.23970 train acc 0.09326 valid acc 0.08008\n",
      "epoch 698: train loss 0.23837, valid loss 0.23970 train acc 0.09326 valid acc 0.08008\n",
      "epoch 699: train loss 0.23836, valid loss 0.23969 train acc 0.09326 valid acc 0.08008\n",
      "epoch 700: train loss 0.23836, valid loss 0.23969 train acc 0.09326 valid acc 0.08008\n",
      "epoch 701: train loss 0.23836, valid loss 0.23969 train acc 0.09326 valid acc 0.08008\n",
      "epoch 702: train loss 0.23835, valid loss 0.23968 train acc 0.09326 valid acc 0.08008\n",
      "epoch 703: train loss 0.23835, valid loss 0.23968 train acc 0.09326 valid acc 0.08008\n",
      "epoch 704: train loss 0.23834, valid loss 0.23967 train acc 0.09326 valid acc 0.08008\n",
      "epoch 705: train loss 0.23834, valid loss 0.23967 train acc 0.09326 valid acc 0.08008\n",
      "epoch 706: train loss 0.23834, valid loss 0.23967 train acc 0.09326 valid acc 0.08008\n",
      "epoch 707: train loss 0.23833, valid loss 0.23966 train acc 0.09326 valid acc 0.08008\n",
      "epoch 708: train loss 0.23833, valid loss 0.23966 train acc 0.09326 valid acc 0.08008\n",
      "epoch 709: train loss 0.23832, valid loss 0.23965 train acc 0.09326 valid acc 0.08008\n",
      "epoch 710: train loss 0.23832, valid loss 0.23965 train acc 0.09326 valid acc 0.08008\n",
      "epoch 711: train loss 0.23831, valid loss 0.23965 train acc 0.09326 valid acc 0.08008\n",
      "epoch 712: train loss 0.23831, valid loss 0.23964 train acc 0.09326 valid acc 0.08008\n",
      "epoch 713: train loss 0.23831, valid loss 0.23964 train acc 0.09326 valid acc 0.08008\n",
      "epoch 714: train loss 0.23830, valid loss 0.23963 train acc 0.09326 valid acc 0.08008\n",
      "epoch 715: train loss 0.23830, valid loss 0.23963 train acc 0.09326 valid acc 0.08008\n",
      "epoch 716: train loss 0.23829, valid loss 0.23963 train acc 0.09326 valid acc 0.08008\n",
      "epoch 717: train loss 0.23829, valid loss 0.23962 train acc 0.09326 valid acc 0.08008\n",
      "epoch 718: train loss 0.23829, valid loss 0.23962 train acc 0.09326 valid acc 0.08008\n",
      "epoch 719: train loss 0.23828, valid loss 0.23961 train acc 0.09326 valid acc 0.08008\n",
      "epoch 720: train loss 0.23828, valid loss 0.23961 train acc 0.09326 valid acc 0.08008\n",
      "epoch 721: train loss 0.23827, valid loss 0.23961 train acc 0.09326 valid acc 0.08008\n",
      "epoch 722: train loss 0.23827, valid loss 0.23960 train acc 0.09326 valid acc 0.08008\n",
      "epoch 723: train loss 0.23827, valid loss 0.23960 train acc 0.09326 valid acc 0.08008\n",
      "epoch 724: train loss 0.23826, valid loss 0.23959 train acc 0.09326 valid acc 0.08008\n",
      "epoch 725: train loss 0.23826, valid loss 0.23959 train acc 0.09326 valid acc 0.08008\n",
      "epoch 726: train loss 0.23825, valid loss 0.23959 train acc 0.09326 valid acc 0.08008\n",
      "epoch 727: train loss 0.23825, valid loss 0.23958 train acc 0.09326 valid acc 0.08008\n",
      "epoch 728: train loss 0.23825, valid loss 0.23958 train acc 0.09326 valid acc 0.08008\n",
      "epoch 729: train loss 0.23824, valid loss 0.23957 train acc 0.09326 valid acc 0.08008\n",
      "epoch 730: train loss 0.23824, valid loss 0.23957 train acc 0.09326 valid acc 0.08008\n",
      "epoch 731: train loss 0.23823, valid loss 0.23957 train acc 0.09326 valid acc 0.08008\n",
      "epoch 732: train loss 0.23823, valid loss 0.23956 train acc 0.09326 valid acc 0.08008\n",
      "epoch 733: train loss 0.23823, valid loss 0.23956 train acc 0.09326 valid acc 0.08008\n",
      "epoch 734: train loss 0.23822, valid loss 0.23955 train acc 0.09326 valid acc 0.08008\n",
      "epoch 735: train loss 0.23822, valid loss 0.23955 train acc 0.09326 valid acc 0.08008\n",
      "epoch 736: train loss 0.23821, valid loss 0.23955 train acc 0.09326 valid acc 0.08008\n",
      "epoch 737: train loss 0.23821, valid loss 0.23954 train acc 0.09326 valid acc 0.08008\n",
      "epoch 738: train loss 0.23820, valid loss 0.23954 train acc 0.09326 valid acc 0.08008\n",
      "epoch 739: train loss 0.23820, valid loss 0.23953 train acc 0.09326 valid acc 0.08008\n",
      "epoch 740: train loss 0.23820, valid loss 0.23953 train acc 0.09326 valid acc 0.08008\n",
      "epoch 741: train loss 0.23819, valid loss 0.23953 train acc 0.09326 valid acc 0.08008\n",
      "epoch 742: train loss 0.23819, valid loss 0.23952 train acc 0.09326 valid acc 0.08008\n",
      "epoch 743: train loss 0.23818, valid loss 0.23952 train acc 0.09326 valid acc 0.08008\n",
      "epoch 744: train loss 0.23818, valid loss 0.23951 train acc 0.09326 valid acc 0.08008\n",
      "epoch 745: train loss 0.23818, valid loss 0.23951 train acc 0.09326 valid acc 0.08008\n",
      "epoch 746: train loss 0.23817, valid loss 0.23951 train acc 0.09302 valid acc 0.08008\n",
      "epoch 747: train loss 0.23817, valid loss 0.23950 train acc 0.09302 valid acc 0.08008\n",
      "epoch 748: train loss 0.23816, valid loss 0.23950 train acc 0.09302 valid acc 0.08008\n",
      "epoch 749: train loss 0.23816, valid loss 0.23949 train acc 0.09302 valid acc 0.08008\n",
      "epoch 750: train loss 0.23816, valid loss 0.23949 train acc 0.09302 valid acc 0.08008\n",
      "epoch 751: train loss 0.23815, valid loss 0.23949 train acc 0.09302 valid acc 0.08008\n",
      "epoch 752: train loss 0.23815, valid loss 0.23948 train acc 0.09302 valid acc 0.08008\n",
      "epoch 753: train loss 0.23814, valid loss 0.23948 train acc 0.09302 valid acc 0.08008\n",
      "epoch 754: train loss 0.23814, valid loss 0.23947 train acc 0.09302 valid acc 0.08008\n",
      "epoch 755: train loss 0.23814, valid loss 0.23947 train acc 0.09302 valid acc 0.08008\n",
      "epoch 756: train loss 0.23813, valid loss 0.23947 train acc 0.09302 valid acc 0.08008\n",
      "epoch 757: train loss 0.23813, valid loss 0.23946 train acc 0.09302 valid acc 0.08008\n",
      "epoch 758: train loss 0.23812, valid loss 0.23946 train acc 0.09326 valid acc 0.08008\n",
      "epoch 759: train loss 0.23812, valid loss 0.23945 train acc 0.09326 valid acc 0.08008\n",
      "epoch 760: train loss 0.23812, valid loss 0.23945 train acc 0.09326 valid acc 0.08008\n",
      "epoch 761: train loss 0.23811, valid loss 0.23945 train acc 0.09326 valid acc 0.08008\n",
      "epoch 762: train loss 0.23811, valid loss 0.23944 train acc 0.09326 valid acc 0.08008\n",
      "epoch 763: train loss 0.23811, valid loss 0.23944 train acc 0.09326 valid acc 0.08008\n",
      "epoch 764: train loss 0.23810, valid loss 0.23943 train acc 0.09326 valid acc 0.08008\n",
      "epoch 765: train loss 0.23810, valid loss 0.23943 train acc 0.09326 valid acc 0.08008\n",
      "epoch 766: train loss 0.23809, valid loss 0.23943 train acc 0.09326 valid acc 0.08008\n",
      "epoch 767: train loss 0.23809, valid loss 0.23942 train acc 0.09326 valid acc 0.08008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 768: train loss 0.23809, valid loss 0.23942 train acc 0.09326 valid acc 0.08008\n",
      "epoch 769: train loss 0.23808, valid loss 0.23941 train acc 0.09326 valid acc 0.08008\n",
      "epoch 770: train loss 0.23808, valid loss 0.23941 train acc 0.09326 valid acc 0.08008\n",
      "epoch 771: train loss 0.23807, valid loss 0.23941 train acc 0.09326 valid acc 0.08008\n",
      "epoch 772: train loss 0.23807, valid loss 0.23940 train acc 0.09326 valid acc 0.08008\n",
      "epoch 773: train loss 0.23807, valid loss 0.23940 train acc 0.09326 valid acc 0.08008\n",
      "epoch 774: train loss 0.23806, valid loss 0.23940 train acc 0.09326 valid acc 0.08008\n",
      "epoch 775: train loss 0.23806, valid loss 0.23939 train acc 0.09326 valid acc 0.08008\n",
      "epoch 776: train loss 0.23805, valid loss 0.23939 train acc 0.09326 valid acc 0.08008\n",
      "epoch 777: train loss 0.23805, valid loss 0.23938 train acc 0.09326 valid acc 0.08008\n",
      "epoch 778: train loss 0.23805, valid loss 0.23938 train acc 0.09326 valid acc 0.08008\n",
      "epoch 779: train loss 0.23804, valid loss 0.23938 train acc 0.09326 valid acc 0.08008\n",
      "epoch 780: train loss 0.23804, valid loss 0.23937 train acc 0.09326 valid acc 0.08008\n",
      "epoch 781: train loss 0.23803, valid loss 0.23937 train acc 0.09326 valid acc 0.08008\n",
      "epoch 782: train loss 0.23803, valid loss 0.23936 train acc 0.09326 valid acc 0.08008\n",
      "epoch 783: train loss 0.23803, valid loss 0.23936 train acc 0.09326 valid acc 0.08008\n",
      "epoch 784: train loss 0.23802, valid loss 0.23936 train acc 0.09326 valid acc 0.08008\n",
      "epoch 785: train loss 0.23802, valid loss 0.23935 train acc 0.09326 valid acc 0.08008\n",
      "epoch 786: train loss 0.23801, valid loss 0.23935 train acc 0.09326 valid acc 0.08008\n",
      "epoch 787: train loss 0.23801, valid loss 0.23934 train acc 0.09326 valid acc 0.08008\n",
      "epoch 788: train loss 0.23801, valid loss 0.23934 train acc 0.09326 valid acc 0.08008\n",
      "epoch 789: train loss 0.23800, valid loss 0.23934 train acc 0.09326 valid acc 0.08008\n",
      "epoch 790: train loss 0.23800, valid loss 0.23933 train acc 0.09326 valid acc 0.08008\n",
      "epoch 791: train loss 0.23799, valid loss 0.23933 train acc 0.09351 valid acc 0.08008\n",
      "epoch 792: train loss 0.23799, valid loss 0.23933 train acc 0.09351 valid acc 0.08008\n",
      "epoch 793: train loss 0.23799, valid loss 0.23932 train acc 0.09351 valid acc 0.08008\n",
      "epoch 794: train loss 0.23798, valid loss 0.23932 train acc 0.09351 valid acc 0.08008\n",
      "epoch 795: train loss 0.23798, valid loss 0.23931 train acc 0.09351 valid acc 0.08008\n",
      "epoch 796: train loss 0.23798, valid loss 0.23931 train acc 0.09351 valid acc 0.08008\n",
      "epoch 797: train loss 0.23797, valid loss 0.23931 train acc 0.09351 valid acc 0.08008\n",
      "epoch 798: train loss 0.23797, valid loss 0.23930 train acc 0.09351 valid acc 0.08008\n",
      "epoch 799: train loss 0.23796, valid loss 0.23930 train acc 0.09351 valid acc 0.08008\n",
      "epoch 800: train loss 0.23796, valid loss 0.23929 train acc 0.09375 valid acc 0.08008\n",
      "epoch 801: train loss 0.23796, valid loss 0.23929 train acc 0.09375 valid acc 0.08008\n",
      "epoch 802: train loss 0.23795, valid loss 0.23929 train acc 0.09375 valid acc 0.08008\n",
      "epoch 803: train loss 0.23795, valid loss 0.23928 train acc 0.09375 valid acc 0.08008\n",
      "epoch 804: train loss 0.23794, valid loss 0.23928 train acc 0.09375 valid acc 0.08008\n",
      "epoch 805: train loss 0.23794, valid loss 0.23928 train acc 0.09375 valid acc 0.08008\n",
      "epoch 806: train loss 0.23794, valid loss 0.23927 train acc 0.09375 valid acc 0.08008\n",
      "epoch 807: train loss 0.23793, valid loss 0.23927 train acc 0.09375 valid acc 0.08008\n",
      "epoch 808: train loss 0.23793, valid loss 0.23926 train acc 0.09375 valid acc 0.08008\n",
      "epoch 809: train loss 0.23793, valid loss 0.23926 train acc 0.09351 valid acc 0.08008\n",
      "epoch 810: train loss 0.23792, valid loss 0.23926 train acc 0.09351 valid acc 0.08008\n",
      "epoch 811: train loss 0.23792, valid loss 0.23925 train acc 0.09351 valid acc 0.08008\n",
      "epoch 812: train loss 0.23791, valid loss 0.23925 train acc 0.09351 valid acc 0.08008\n",
      "epoch 813: train loss 0.23791, valid loss 0.23924 train acc 0.09351 valid acc 0.08008\n",
      "epoch 814: train loss 0.23791, valid loss 0.23924 train acc 0.09351 valid acc 0.08008\n",
      "epoch 815: train loss 0.23790, valid loss 0.23924 train acc 0.09351 valid acc 0.08008\n",
      "epoch 816: train loss 0.23790, valid loss 0.23923 train acc 0.09351 valid acc 0.08008\n",
      "epoch 817: train loss 0.23789, valid loss 0.23923 train acc 0.09351 valid acc 0.08008\n",
      "epoch 818: train loss 0.23789, valid loss 0.23923 train acc 0.09351 valid acc 0.08008\n",
      "epoch 819: train loss 0.23789, valid loss 0.23922 train acc 0.09351 valid acc 0.08008\n",
      "epoch 820: train loss 0.23788, valid loss 0.23922 train acc 0.09351 valid acc 0.08008\n",
      "epoch 821: train loss 0.23788, valid loss 0.23921 train acc 0.09351 valid acc 0.08008\n",
      "epoch 822: train loss 0.23788, valid loss 0.23921 train acc 0.09351 valid acc 0.08008\n",
      "epoch 823: train loss 0.23787, valid loss 0.23921 train acc 0.09351 valid acc 0.08008\n",
      "epoch 824: train loss 0.23787, valid loss 0.23920 train acc 0.09326 valid acc 0.08008\n",
      "epoch 825: train loss 0.23786, valid loss 0.23920 train acc 0.09326 valid acc 0.08008\n",
      "epoch 826: train loss 0.23786, valid loss 0.23920 train acc 0.09302 valid acc 0.08008\n",
      "epoch 827: train loss 0.23786, valid loss 0.23919 train acc 0.09302 valid acc 0.08008\n",
      "epoch 828: train loss 0.23785, valid loss 0.23919 train acc 0.09302 valid acc 0.08008\n",
      "epoch 829: train loss 0.23785, valid loss 0.23918 train acc 0.09302 valid acc 0.08008\n",
      "epoch 830: train loss 0.23785, valid loss 0.23918 train acc 0.09302 valid acc 0.08008\n",
      "epoch 831: train loss 0.23784, valid loss 0.23918 train acc 0.09302 valid acc 0.08008\n",
      "epoch 832: train loss 0.23784, valid loss 0.23917 train acc 0.09302 valid acc 0.08008\n",
      "epoch 833: train loss 0.23783, valid loss 0.23917 train acc 0.09302 valid acc 0.08008\n",
      "epoch 834: train loss 0.23783, valid loss 0.23917 train acc 0.09302 valid acc 0.08008\n",
      "epoch 835: train loss 0.23783, valid loss 0.23916 train acc 0.09302 valid acc 0.08008\n",
      "epoch 836: train loss 0.23782, valid loss 0.23916 train acc 0.09302 valid acc 0.08008\n",
      "epoch 837: train loss 0.23782, valid loss 0.23915 train acc 0.09302 valid acc 0.08008\n",
      "epoch 838: train loss 0.23781, valid loss 0.23915 train acc 0.09302 valid acc 0.08008\n",
      "epoch 839: train loss 0.23781, valid loss 0.23915 train acc 0.09302 valid acc 0.08008\n",
      "epoch 840: train loss 0.23781, valid loss 0.23914 train acc 0.09302 valid acc 0.08008\n",
      "epoch 841: train loss 0.23780, valid loss 0.23914 train acc 0.09302 valid acc 0.08008\n",
      "epoch 842: train loss 0.23780, valid loss 0.23914 train acc 0.09302 valid acc 0.08008\n",
      "epoch 843: train loss 0.23780, valid loss 0.23913 train acc 0.09302 valid acc 0.08008\n",
      "epoch 844: train loss 0.23779, valid loss 0.23913 train acc 0.09302 valid acc 0.08008\n",
      "epoch 845: train loss 0.23779, valid loss 0.23913 train acc 0.09302 valid acc 0.08008\n",
      "epoch 846: train loss 0.23778, valid loss 0.23912 train acc 0.09302 valid acc 0.08008\n",
      "epoch 847: train loss 0.23778, valid loss 0.23912 train acc 0.09302 valid acc 0.08008\n",
      "epoch 848: train loss 0.23778, valid loss 0.23911 train acc 0.09302 valid acc 0.08008\n",
      "epoch 849: train loss 0.23777, valid loss 0.23911 train acc 0.09302 valid acc 0.08008\n",
      "epoch 850: train loss 0.23777, valid loss 0.23911 train acc 0.09302 valid acc 0.08008\n",
      "epoch 851: train loss 0.23777, valid loss 0.23910 train acc 0.09302 valid acc 0.08008\n",
      "epoch 852: train loss 0.23776, valid loss 0.23910 train acc 0.09302 valid acc 0.08008\n",
      "epoch 853: train loss 0.23776, valid loss 0.23910 train acc 0.09302 valid acc 0.08008\n",
      "epoch 854: train loss 0.23775, valid loss 0.23909 train acc 0.09302 valid acc 0.08008\n",
      "epoch 855: train loss 0.23775, valid loss 0.23909 train acc 0.09302 valid acc 0.08008\n",
      "epoch 856: train loss 0.23775, valid loss 0.23908 train acc 0.09302 valid acc 0.08008\n",
      "epoch 857: train loss 0.23774, valid loss 0.23908 train acc 0.09302 valid acc 0.08008\n",
      "epoch 858: train loss 0.23774, valid loss 0.23908 train acc 0.09302 valid acc 0.08008\n",
      "epoch 859: train loss 0.23774, valid loss 0.23907 train acc 0.09277 valid acc 0.08008\n",
      "epoch 860: train loss 0.23773, valid loss 0.23907 train acc 0.09277 valid acc 0.08008\n",
      "epoch 861: train loss 0.23773, valid loss 0.23907 train acc 0.09277 valid acc 0.08008\n",
      "epoch 862: train loss 0.23772, valid loss 0.23906 train acc 0.09277 valid acc 0.08008\n",
      "epoch 863: train loss 0.23772, valid loss 0.23906 train acc 0.09277 valid acc 0.08008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 864: train loss 0.23772, valid loss 0.23906 train acc 0.09253 valid acc 0.08008\n",
      "epoch 865: train loss 0.23771, valid loss 0.23905 train acc 0.09253 valid acc 0.08008\n",
      "epoch 866: train loss 0.23771, valid loss 0.23905 train acc 0.09253 valid acc 0.08008\n",
      "epoch 867: train loss 0.23771, valid loss 0.23904 train acc 0.09253 valid acc 0.08008\n",
      "epoch 868: train loss 0.23770, valid loss 0.23904 train acc 0.09229 valid acc 0.08008\n",
      "epoch 869: train loss 0.23770, valid loss 0.23904 train acc 0.09229 valid acc 0.08008\n",
      "epoch 870: train loss 0.23770, valid loss 0.23903 train acc 0.09229 valid acc 0.08008\n",
      "epoch 871: train loss 0.23769, valid loss 0.23903 train acc 0.09229 valid acc 0.08008\n",
      "epoch 872: train loss 0.23769, valid loss 0.23903 train acc 0.09229 valid acc 0.08008\n",
      "epoch 873: train loss 0.23768, valid loss 0.23902 train acc 0.09229 valid acc 0.08008\n",
      "epoch 874: train loss 0.23768, valid loss 0.23902 train acc 0.09253 valid acc 0.08008\n",
      "epoch 875: train loss 0.23768, valid loss 0.23902 train acc 0.09253 valid acc 0.08008\n",
      "epoch 876: train loss 0.23767, valid loss 0.23901 train acc 0.09253 valid acc 0.08008\n",
      "epoch 877: train loss 0.23767, valid loss 0.23901 train acc 0.09253 valid acc 0.08008\n",
      "epoch 878: train loss 0.23767, valid loss 0.23900 train acc 0.09253 valid acc 0.08008\n",
      "epoch 879: train loss 0.23766, valid loss 0.23900 train acc 0.09253 valid acc 0.08008\n",
      "epoch 880: train loss 0.23766, valid loss 0.23900 train acc 0.09253 valid acc 0.08008\n",
      "epoch 881: train loss 0.23765, valid loss 0.23899 train acc 0.09253 valid acc 0.08008\n",
      "epoch 882: train loss 0.23765, valid loss 0.23899 train acc 0.09253 valid acc 0.08008\n",
      "epoch 883: train loss 0.23765, valid loss 0.23899 train acc 0.09253 valid acc 0.08008\n",
      "epoch 884: train loss 0.23764, valid loss 0.23898 train acc 0.09253 valid acc 0.08008\n",
      "epoch 885: train loss 0.23764, valid loss 0.23898 train acc 0.09253 valid acc 0.08008\n",
      "epoch 886: train loss 0.23764, valid loss 0.23898 train acc 0.09253 valid acc 0.08008\n",
      "epoch 887: train loss 0.23763, valid loss 0.23897 train acc 0.09253 valid acc 0.08008\n",
      "epoch 888: train loss 0.23763, valid loss 0.23897 train acc 0.09253 valid acc 0.08008\n",
      "epoch 889: train loss 0.23763, valid loss 0.23896 train acc 0.09253 valid acc 0.08008\n",
      "epoch 890: train loss 0.23762, valid loss 0.23896 train acc 0.09253 valid acc 0.08008\n",
      "epoch 891: train loss 0.23762, valid loss 0.23896 train acc 0.09253 valid acc 0.08008\n",
      "epoch 892: train loss 0.23761, valid loss 0.23895 train acc 0.09253 valid acc 0.08008\n",
      "epoch 893: train loss 0.23761, valid loss 0.23895 train acc 0.09253 valid acc 0.08008\n",
      "epoch 894: train loss 0.23761, valid loss 0.23895 train acc 0.09253 valid acc 0.08008\n",
      "epoch 895: train loss 0.23760, valid loss 0.23894 train acc 0.09253 valid acc 0.08008\n",
      "epoch 896: train loss 0.23760, valid loss 0.23894 train acc 0.09253 valid acc 0.08008\n",
      "epoch 897: train loss 0.23760, valid loss 0.23894 train acc 0.09253 valid acc 0.08008\n",
      "epoch 898: train loss 0.23759, valid loss 0.23893 train acc 0.09253 valid acc 0.08008\n",
      "epoch 899: train loss 0.23759, valid loss 0.23893 train acc 0.09253 valid acc 0.08008\n",
      "epoch 900: train loss 0.23758, valid loss 0.23892 train acc 0.09253 valid acc 0.08008\n",
      "epoch 901: train loss 0.23758, valid loss 0.23892 train acc 0.09253 valid acc 0.08008\n",
      "epoch 902: train loss 0.23758, valid loss 0.23892 train acc 0.09253 valid acc 0.08008\n",
      "epoch 903: train loss 0.23757, valid loss 0.23891 train acc 0.09253 valid acc 0.08008\n",
      "epoch 904: train loss 0.23757, valid loss 0.23891 train acc 0.09253 valid acc 0.08008\n",
      "epoch 905: train loss 0.23757, valid loss 0.23891 train acc 0.09253 valid acc 0.08008\n",
      "epoch 906: train loss 0.23756, valid loss 0.23890 train acc 0.09253 valid acc 0.08008\n",
      "epoch 907: train loss 0.23756, valid loss 0.23890 train acc 0.09253 valid acc 0.08008\n",
      "epoch 908: train loss 0.23756, valid loss 0.23890 train acc 0.09253 valid acc 0.08008\n",
      "epoch 909: train loss 0.23755, valid loss 0.23889 train acc 0.09253 valid acc 0.08008\n",
      "epoch 910: train loss 0.23755, valid loss 0.23889 train acc 0.09253 valid acc 0.08008\n",
      "epoch 911: train loss 0.23755, valid loss 0.23889 train acc 0.09253 valid acc 0.08008\n",
      "epoch 912: train loss 0.23754, valid loss 0.23888 train acc 0.09253 valid acc 0.08008\n",
      "epoch 913: train loss 0.23754, valid loss 0.23888 train acc 0.09253 valid acc 0.08008\n",
      "epoch 914: train loss 0.23753, valid loss 0.23888 train acc 0.09253 valid acc 0.08142\n",
      "epoch 915: train loss 0.23753, valid loss 0.23887 train acc 0.09253 valid acc 0.08203\n",
      "epoch 916: train loss 0.23753, valid loss 0.23887 train acc 0.09253 valid acc 0.08203\n",
      "epoch 917: train loss 0.23752, valid loss 0.23886 train acc 0.09253 valid acc 0.08203\n",
      "epoch 918: train loss 0.23752, valid loss 0.23886 train acc 0.09253 valid acc 0.08203\n",
      "epoch 919: train loss 0.23752, valid loss 0.23886 train acc 0.09253 valid acc 0.08203\n",
      "epoch 920: train loss 0.23751, valid loss 0.23885 train acc 0.09253 valid acc 0.08203\n",
      "epoch 921: train loss 0.23751, valid loss 0.23885 train acc 0.09253 valid acc 0.08203\n",
      "epoch 922: train loss 0.23751, valid loss 0.23885 train acc 0.09253 valid acc 0.08203\n",
      "epoch 923: train loss 0.23750, valid loss 0.23884 train acc 0.09253 valid acc 0.08203\n",
      "epoch 924: train loss 0.23750, valid loss 0.23884 train acc 0.09253 valid acc 0.08203\n",
      "epoch 925: train loss 0.23749, valid loss 0.23884 train acc 0.09253 valid acc 0.08203\n",
      "epoch 926: train loss 0.23749, valid loss 0.23883 train acc 0.09253 valid acc 0.08203\n",
      "epoch 927: train loss 0.23749, valid loss 0.23883 train acc 0.09253 valid acc 0.08203\n",
      "epoch 928: train loss 0.23748, valid loss 0.23883 train acc 0.09253 valid acc 0.08203\n",
      "epoch 929: train loss 0.23748, valid loss 0.23882 train acc 0.09253 valid acc 0.08203\n",
      "epoch 930: train loss 0.23748, valid loss 0.23882 train acc 0.09253 valid acc 0.08093\n",
      "epoch 931: train loss 0.23747, valid loss 0.23882 train acc 0.09253 valid acc 0.08008\n",
      "epoch 932: train loss 0.23747, valid loss 0.23881 train acc 0.09253 valid acc 0.08008\n",
      "epoch 933: train loss 0.23747, valid loss 0.23881 train acc 0.09253 valid acc 0.08008\n",
      "epoch 934: train loss 0.23746, valid loss 0.23880 train acc 0.09253 valid acc 0.08008\n",
      "epoch 935: train loss 0.23746, valid loss 0.23880 train acc 0.09253 valid acc 0.08008\n",
      "epoch 936: train loss 0.23746, valid loss 0.23880 train acc 0.09253 valid acc 0.08008\n",
      "epoch 937: train loss 0.23745, valid loss 0.23879 train acc 0.09253 valid acc 0.08008\n",
      "epoch 938: train loss 0.23745, valid loss 0.23879 train acc 0.09253 valid acc 0.08008\n",
      "epoch 939: train loss 0.23744, valid loss 0.23879 train acc 0.09253 valid acc 0.08008\n",
      "epoch 940: train loss 0.23744, valid loss 0.23878 train acc 0.09253 valid acc 0.08008\n",
      "epoch 941: train loss 0.23744, valid loss 0.23878 train acc 0.09253 valid acc 0.08008\n",
      "epoch 942: train loss 0.23743, valid loss 0.23878 train acc 0.09253 valid acc 0.08008\n",
      "epoch 943: train loss 0.23743, valid loss 0.23877 train acc 0.09253 valid acc 0.08008\n",
      "epoch 944: train loss 0.23743, valid loss 0.23877 train acc 0.09253 valid acc 0.08008\n",
      "epoch 945: train loss 0.23742, valid loss 0.23877 train acc 0.09253 valid acc 0.08008\n",
      "epoch 946: train loss 0.23742, valid loss 0.23876 train acc 0.09253 valid acc 0.08008\n",
      "epoch 947: train loss 0.23742, valid loss 0.23876 train acc 0.09253 valid acc 0.08008\n",
      "epoch 948: train loss 0.23741, valid loss 0.23876 train acc 0.09253 valid acc 0.08008\n",
      "epoch 949: train loss 0.23741, valid loss 0.23875 train acc 0.09253 valid acc 0.08008\n",
      "epoch 950: train loss 0.23741, valid loss 0.23875 train acc 0.09253 valid acc 0.08008\n",
      "epoch 951: train loss 0.23740, valid loss 0.23875 train acc 0.09253 valid acc 0.08008\n",
      "epoch 952: train loss 0.23740, valid loss 0.23874 train acc 0.09253 valid acc 0.08008\n",
      "epoch 953: train loss 0.23740, valid loss 0.23874 train acc 0.09253 valid acc 0.08008\n",
      "epoch 954: train loss 0.23739, valid loss 0.23873 train acc 0.09253 valid acc 0.08008\n",
      "epoch 955: train loss 0.23739, valid loss 0.23873 train acc 0.09253 valid acc 0.08008\n",
      "epoch 956: train loss 0.23739, valid loss 0.23873 train acc 0.09253 valid acc 0.08008\n",
      "epoch 957: train loss 0.23738, valid loss 0.23872 train acc 0.09204 valid acc 0.08008\n",
      "epoch 958: train loss 0.23738, valid loss 0.23872 train acc 0.09204 valid acc 0.08008\n",
      "epoch 959: train loss 0.23737, valid loss 0.23872 train acc 0.09204 valid acc 0.08008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 960: train loss 0.23737, valid loss 0.23871 train acc 0.09204 valid acc 0.08008\n",
      "epoch 961: train loss 0.23737, valid loss 0.23871 train acc 0.09204 valid acc 0.08008\n",
      "epoch 962: train loss 0.23736, valid loss 0.23871 train acc 0.09229 valid acc 0.08008\n",
      "epoch 963: train loss 0.23736, valid loss 0.23870 train acc 0.09229 valid acc 0.08008\n",
      "epoch 964: train loss 0.23736, valid loss 0.23870 train acc 0.09229 valid acc 0.08008\n",
      "epoch 965: train loss 0.23735, valid loss 0.23870 train acc 0.09229 valid acc 0.08008\n",
      "epoch 966: train loss 0.23735, valid loss 0.23869 train acc 0.09229 valid acc 0.08008\n",
      "epoch 967: train loss 0.23735, valid loss 0.23869 train acc 0.09229 valid acc 0.08008\n",
      "epoch 968: train loss 0.23734, valid loss 0.23869 train acc 0.09229 valid acc 0.08008\n",
      "epoch 969: train loss 0.23734, valid loss 0.23868 train acc 0.09229 valid acc 0.08008\n",
      "epoch 970: train loss 0.23734, valid loss 0.23868 train acc 0.09229 valid acc 0.08008\n",
      "epoch 971: train loss 0.23733, valid loss 0.23868 train acc 0.09229 valid acc 0.08008\n",
      "epoch 972: train loss 0.23733, valid loss 0.23867 train acc 0.09229 valid acc 0.08008\n",
      "epoch 973: train loss 0.23733, valid loss 0.23867 train acc 0.09229 valid acc 0.08008\n",
      "epoch 974: train loss 0.23732, valid loss 0.23867 train acc 0.09229 valid acc 0.08008\n",
      "epoch 975: train loss 0.23732, valid loss 0.23866 train acc 0.09229 valid acc 0.08008\n",
      "epoch 976: train loss 0.23732, valid loss 0.23866 train acc 0.09229 valid acc 0.08008\n",
      "epoch 977: train loss 0.23731, valid loss 0.23866 train acc 0.09229 valid acc 0.08008\n",
      "epoch 978: train loss 0.23731, valid loss 0.23865 train acc 0.09229 valid acc 0.08008\n",
      "epoch 979: train loss 0.23730, valid loss 0.23865 train acc 0.09229 valid acc 0.08008\n",
      "epoch 980: train loss 0.23730, valid loss 0.23865 train acc 0.09229 valid acc 0.08008\n",
      "epoch 981: train loss 0.23730, valid loss 0.23864 train acc 0.09229 valid acc 0.08008\n",
      "epoch 982: train loss 0.23729, valid loss 0.23864 train acc 0.09229 valid acc 0.08008\n",
      "epoch 983: train loss 0.23729, valid loss 0.23863 train acc 0.09229 valid acc 0.08008\n",
      "epoch 984: train loss 0.23729, valid loss 0.23863 train acc 0.09229 valid acc 0.08008\n",
      "epoch 985: train loss 0.23728, valid loss 0.23863 train acc 0.09229 valid acc 0.08008\n",
      "epoch 986: train loss 0.23728, valid loss 0.23862 train acc 0.09229 valid acc 0.08008\n",
      "epoch 987: train loss 0.23728, valid loss 0.23862 train acc 0.09229 valid acc 0.08008\n",
      "epoch 988: train loss 0.23727, valid loss 0.23862 train acc 0.09229 valid acc 0.08008\n",
      "epoch 989: train loss 0.23727, valid loss 0.23861 train acc 0.09229 valid acc 0.08008\n",
      "epoch 990: train loss 0.23727, valid loss 0.23861 train acc 0.09229 valid acc 0.08008\n",
      "epoch 991: train loss 0.23726, valid loss 0.23861 train acc 0.09229 valid acc 0.08008\n",
      "epoch 992: train loss 0.23726, valid loss 0.23860 train acc 0.09229 valid acc 0.08008\n",
      "epoch 993: train loss 0.23726, valid loss 0.23860 train acc 0.09229 valid acc 0.08008\n",
      "epoch 994: train loss 0.23725, valid loss 0.23860 train acc 0.09229 valid acc 0.08008\n",
      "epoch 995: train loss 0.23725, valid loss 0.23859 train acc 0.09229 valid acc 0.08008\n",
      "epoch 996: train loss 0.23725, valid loss 0.23859 train acc 0.09229 valid acc 0.08008\n",
      "epoch 997: train loss 0.23724, valid loss 0.23859 train acc 0.09229 valid acc 0.08008\n",
      "epoch 998: train loss 0.23724, valid loss 0.23858 train acc 0.09253 valid acc 0.08008\n",
      "epoch 999: train loss 0.23724, valid loss 0.23858 train acc 0.09253 valid acc 0.08008\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_accuracy = []\n",
    "\n",
    "valid_losses = []\n",
    "valid_accuracy = []\n",
    "\n",
    "valid = (x_valid, y_valid)\n",
    "\n",
    "for i in range(epochs):\n",
    "    train_epoch_losses = []\n",
    "    train_epoch_accuracy = []\n",
    "    \n",
    "    valid_epoch_losses = []\n",
    "    valid_epoch_accuracy = []\n",
    "    \n",
    "    for batch in range(train_size//batch_size):\n",
    "        \n",
    "        train = (x_train[batch*batch_size:batch*batch_size+batch_size], \n",
    "                 y_train[batch*batch_size:batch*batch_size+batch_size])\n",
    "        \n",
    "        opt_state = opt_update(i*(train_size//batch_size) + batch, grad_loss(opt_state, *train), opt_state)\n",
    "        \n",
    "        train_epoch_losses.append(loss(get_params(opt_state), *train))\n",
    "        valid_epoch_losses.append(loss(get_params(opt_state), *valid))\n",
    "        \n",
    "        train_correctness = onp.argmax(apply_fn(get_params(opt_state), train[0]), 1) == onp.argmax(train[1], 1)\n",
    "        train_epoch_accuracy.append(onp.average(train_correctness))\n",
    "        \n",
    "        valid_correctness = onp.argmax(apply_fn(get_params(opt_state), valid[0]), 1) == onp.argmax(valid[1], 1)\n",
    "        valid_epoch_accuracy.append(onp.average(valid_correctness))\n",
    "    \n",
    "    print(\"epoch %3d: train loss %3.5f, valid loss %3.5f train acc %.5f valid acc %.5f\"%\\\n",
    "          (i, onp.average(train_epoch_losses), \n",
    "           onp.average(valid_epoch_losses), \n",
    "           onp.average(train_epoch_accuracy), \n",
    "           onp.average(valid_epoch_accuracy)))\n",
    "    \n",
    "    train_losses.append(onp.average(train_epoch_losses))\n",
    "    train_accuracy.append(onp.average(train_epoch_accuracy))\n",
    "    \n",
    "    valid_losses.append(onp.average(valid_epoch_losses))\n",
    "    valid_accuracy.append(onp.average(valid_epoch_accuracy))\n",
    "    \n",
    "    x_train, y_train = unison_shuffled_copies(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attack algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, x_train=None, y_train=None, x_test=None, \n",
    "                         y=None, t=None, loss_weighting=None, fx_train_0=0., fx_test_0=0., eps=0.3, \n",
    "                         norm=np.inf, clip_min=None, clip_max=None, targeted=False):\n",
    "    if norm not in [np.inf, 2]:\n",
    "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
    "        \n",
    "    x = x_test\n",
    "        \n",
    "    # test independent\n",
    "    if obj_fn == 'untargeted':\n",
    "        grads = grads_fn(x_train, x_test, y_train, y, kernel_fn, t)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Objective function must be either train(ntk_train_train) or test(predict_fn)\")\n",
    "\n",
    "    axis = list(range(1, len(grads.shape)))\n",
    "    eps_div = 1e-12\n",
    "    \n",
    "    if norm == np.inf:\n",
    "        perturbation = eps * np.sign(grads)\n",
    "    elif norm == 1:\n",
    "        raise NotImplementedError(\"L_1 norm has not been implemented yet.\")\n",
    "    elif norm == 2:\n",
    "        square = np.maximum(eps_div, np.sum(np.square(grads), axis=axis, keepdims=True))\n",
    "        perturbation = grads / np.sqrt(square)\n",
    "    \n",
    "    # TODO\n",
    "    adv_x = x + perturbation\n",
    "    \n",
    "    # If clipping is needed, reset all values outside of [clip_min, clip_max]\n",
    "    if (clip_min is not None) or (clip_max is not None):\n",
    "        # We don't currently support one-sided clipping\n",
    "        assert clip_min is not None and clip_max is not None\n",
    "        adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "    \n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, x_train=None, y_train=None,\n",
    "                               x_test=None, y=None, t=None, loss_weighting=None, fx_train_0=0., fx_test_0=0., \n",
    "                               eps=0.3, eps_iter=0.03, nb_iter=10, norm=np.inf, clip_min=None, clip_max=None, \n",
    "                               targeted=False, rand_init=None, rand_minmax=0.3):\n",
    "\n",
    "    assert eps_iter <= eps, (eps_iter, eps)\n",
    "    if norm == 1:\n",
    "        raise NotImplementedError(\"It's not clear that FGM is a good inner loop\"\n",
    "                                  \" step for PGD when norm=1, because norm=1 FGM \"\n",
    "                                  \" changes only one pixel at a time. We need \"\n",
    "                                  \" to rigorously test a strong norm=1 PGD \"\n",
    "                                  \"before enabling this feature.\")\n",
    "    if norm not in [np.inf, 2]:\n",
    "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
    "        \n",
    "    x = x_test\n",
    "    \n",
    "    # Initialize loop variables\n",
    "    if rand_init:\n",
    "        rand_minmax = eps\n",
    "        eta = random.uniform(new_key, x.shape, minval=-rand_minmax, maxval=rand_minmax)\n",
    "    else:\n",
    "        eta = np.zeros_like(x)\n",
    "\n",
    "    # Clip eta\n",
    "    eta = clip_eta(eta, norm, eps)\n",
    "    adv_x = x + eta\n",
    "    if clip_min is not None or clip_max is not None:\n",
    "        adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "        \n",
    "    for i in range(nb_iter):\n",
    "        adv_x = fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, x_train, y_train, adv_x, \n",
    "                                        y, t, loss_weighting, fx_train_0, fx_test_0, eps_iter, norm, \n",
    "                                        clip_min, clip_max, targeted)\n",
    "\n",
    "        # Clipping perturbation eta to norm norm ball\n",
    "        eta = adv_x - x\n",
    "        eta = clip_eta(eta, norm, eps)\n",
    "        adv_x = x + eta\n",
    "\n",
    "        # Redo the clipping.\n",
    "        # FGM already did it, but subtracting and re-adding eta can add some\n",
    "        # small numerical error.\n",
    "        if clip_min is not None or clip_max is not None:\n",
    "            adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "    \n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'mnist':\n",
    "    eps = 0.3\n",
    "    eps_iter_10 = (eps/10)*1.1\n",
    "    eps_iter_100 = (eps/100)*1.1\n",
    "    eps_iter_1000 = (eps/1000)*1.1\n",
    "    \n",
    "elif DATASET == 'cifar10':\n",
    "    eps = 0.03\n",
    "    eps_iter_10 = (eps/10)*1.1\n",
    "    eps_iter_100 = (eps/100)*1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(x_train, x_test, y_test, model_fn, kernel_fn, t=None, attack_type=None, ntk_train_train=None):\n",
    "    \n",
    "    y_train_predict, y_test_predict = model_fn(kernel_fn, x_train, x_test, \n",
    "                                               t=t, ntk_train_train=ntk_train_train)\n",
    "    \n",
    "    selected_table = correct(y_test_predict, y_test)\n",
    "    print(\"Accuray({:s}): {:.2f}\".format(attack_type, onp.mean(selected_table)))\n",
    "    \n",
    "    return selected_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_robustness(x_train, x_test, y_test, model_fn, kernel_fn, selected_table, t=None, \n",
    "                        attack_type=None, ntk_train_train=None):\n",
    "    \n",
    "    y_train_predict, y_test_predict = model_fn(kernel_fn, x_train, x_test,\n",
    "                                               t=t, ntk_train_train=ntk_train_train)\n",
    "    \n",
    "    y_test_predict = onp.asarray(y_test_predict)\n",
    "    y_test_predict_select = y_test_predict[onp.asarray(selected_table)]\n",
    "    y_test_select = y_test[onp.asarray(selected_table)]\n",
    "    print(\"Robustness({:s}): {:.2f}\".format(attack_type, onp.mean(correct(y_test_predict_select, y_test_select))))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adv_x generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntk-env",
   "language": "python",
   "name": "ntk-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
