{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "import jax.numpy as np\n",
    "\n",
    "from jax import lax, random\n",
    "from jax.api import grad, jit, vmap\n",
    "from jax.config import config\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "config.update('jax_enable_x64', True)\n",
    "\n",
    "from functools import partial\n",
    "from jax import random\n",
    "\n",
    "from neural_tangents import stax\n",
    "\n",
    "# Attacking\n",
    "from jax.experimental.stax import logsoftmax\n",
    "from cleverhans.utils import clip_eta, one_hot\n",
    "\n",
    "# Plotting\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from utils import *\n",
    "sns.set_style(style='white')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "\"\"\"\n",
    "diag_reg:\n",
    "    a scalar representing the strength of the diagonal regularization for\n",
    "    `k_train_train`, i.e. computing `k_train_train + diag_reg * I` during\n",
    "    Cholesky factorization or eigendecomposition.\n",
    "\"\"\"\n",
    "diag_reg = 1e-4\n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_all, y_train_all, x_test_all, y_test_all = tuple(onp.array(x) for x in get_dataset('mnist', None, None, \n",
    "                                                                                  do_flatten_and_normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "x_train_all, y_train_all = shaffle(x_train_all, y_train_all, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 256\n",
    "x_train = x_train_all[:train_size]\n",
    "y_train = y_train_all[:train_size]\n",
    "\n",
    "test_size = 256\n",
    "x_test = x_test_all[:test_size]\n",
    "y_test = y_test_all[:test_size]\n",
    "\n",
    "shape = (x_train.shape[0], 32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "x_test = np.asarray(x_test)\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective - Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(mean, ys):\n",
    "    return np.mean(np.argmax(mean, axis=-1) == np.argmax(ys, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DenseBlock(neurons, W_std, b_std):\n",
    "    return stax.serial(stax.Dense(neurons, W_std, b_std), \n",
    "                       stax.Erf())\n",
    "\n",
    "def DenseGroup(n, neurons, W_std, b_std):\n",
    "    blocks = []\n",
    "    for _ in range(n):\n",
    "        blocks += [DenseBlock(neurons, W_std, b_std)]\n",
    "    return stax.serial(*blocks)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# b = 0.05\n",
    "b = 0.18\n",
    "W = [1., 1.76, 2.5]\n",
    "phase_list = ['Ordered', 'Critical', 'Chaotic']\n",
    "layers = np.arange(19, 100, 20)\n",
    "layer = 5\n",
    "num_classes = 10\n",
    "\n",
    "kernel_list = []\n",
    "fx_train_0_list = []\n",
    "fx_test_0_list = []\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "b_std = np.sqrt(b)\n",
    "for w in W:\n",
    "    W_std = np.sqrt(w)\n",
    "\n",
    "    init_fn, apply_fn, kernel_fn = stax.serial(DenseGroup(layer, 1024, W_std, b_std))\n",
    "    \n",
    "    # Inference with a single infinite width / linearized network\n",
    "    apply_fn = jit(apply_fn)\n",
    "    kernel_fn = jit(kernel_fn, static_argnums=(2,))\n",
    "    kernel_list.append(kernel_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(kernel_fn, obj_fn, x_train=None, x_test=None, fx_train_0=0., fx_test_0=0., t=None):\n",
    "    # Kernel\n",
    "    ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "    \n",
    "    if obj_fn == 'train':\n",
    "        return ntk_train_train\n",
    "    elif obj_fn == 'test':\n",
    "        ntk_test_train = kernel_fn(x_test, x_train, 'ntk')\n",
    "        # Prediction\n",
    "        predict_fn = nt.predict.gradient_descent_mse(ntk_train_train, y_train, diag_reg=diag_reg) # no convariance\n",
    "        return predict_fn(t, fx_train_0, fx_test_0, ntk_test_train) # fx_train_0, fx_test_0 = (0, 0) for infinite width\n",
    "    else:\n",
    "        raise ValueError(\"Objective function must be either train(ntk_train_train) or test(predict_fn)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def l2_loss_v1(logits, labels, weighting=1):\n",
    "    \"\"\"\n",
    "    Tensorflow version of L2 loss (without sqrt)\n",
    "    \"\"\"\n",
    "    return np.sum(((logits - labels)**2) * weighting) / 2\n",
    "    \n",
    "@jit\n",
    "def l2_loss_v2(logits, lables):\n",
    "    \"\"\"\n",
    "    Normal L2 loss\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(logits - labels)\n",
    "\n",
    "@jit\n",
    "def cross_entropy_loss(logits, lables):\n",
    "    return -np.sum(logsoftmax(logits) * lables)\n",
    "    \n",
    "@jit\n",
    "def mse_loss(logits, lables):\n",
    "    return 0.5 * np.mean((logits - lables) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loss_adv(x_train, y, kernel_fn, weighting):\n",
    "    # Compute NTK on training data\n",
    "    ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "    loss = - l2_loss_v1(ntk_train_train, y, weighting) # y = matrix of 1 / diagnal\n",
    "    return loss\n",
    "\n",
    "# train_grads_fn = grad(train_loss_adv)\n",
    "train_grads_fn = jit(grad(train_loss_adv), static_argnums=(2,)) # static arg: expanding {if / else} loops for graph mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss_adv_matrix(x_train, x_test, kernel_fn, c, t=None):\n",
    "    # Kernel -> matrix of constant c\n",
    "    assert type(c) == int\n",
    "    \n",
    "    ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "    ntk_test_train = kernel_fn(x_test, x_train, 'ntk')\n",
    "    \n",
    "    # Θ(test, train)Θ(train, train)^-1\n",
    "    def inv(k):\n",
    "        #inverse with diag_reg\n",
    "        return np.linalg.inv(k + diag_reg * np.eye(k.shape[0]))\n",
    "    \n",
    "    mean_predictor = np.einsum('ij,jk->ik', ntk_test_train, inv(ntk_train_train))\n",
    "    \n",
    "    # Loss\n",
    "    loss = - l2_loss_v1(mean_predictor, np.ones_like(mean_predictor)*c)\n",
    "    return loss\n",
    "\n",
    "test_grads_fn = jit(grad(test_loss_adv_matrix, argnums=0), static_argnums=(2,))\n",
    "test_c_grads_fn = jit(grad(test_loss_adv_matrix, argnums=3), static_argnums=(2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1 0.4]\n",
      " [0.3 0.8]]\n",
      "\n",
      "\n",
      "[[0.1 0.2]\n",
      " [0.6 0.8]]\n"
     ]
    }
   ],
   "source": [
    "a = onp.asarray(a=[\n",
    "    [1,2],\n",
    "    [3,4]\n",
    "], dtype=onp.float32)\n",
    "b = onp.asarray(a=[\n",
    "    [.1,.2]\n",
    "], dtype=onp.float32)\n",
    "\n",
    "print(a*b)\n",
    "print('\\n')\n",
    "print(a*b.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = np.asarray(a=[1,2])\n",
    "\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss_adv_col(x_train, x_test, kernel_fn, c, t=None):\n",
    "    \"\"\"\n",
    "    Kernel -> matrix with constant cols\n",
    "    \n",
    "    c is a vector of constant. c.shape should be (1, ndim)\n",
    "    \n",
    "    \"\"\" \n",
    "    assert c.shape[0] == 1\n",
    "    assert len(c.shape) == 2\n",
    "    \n",
    "    ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "    ntk_test_train = kernel_fn(x_test, x_train, 'ntk')\n",
    "    \n",
    "    # Θ(test, train)Θ(train, train)^-1\n",
    "    #inverse with diag_reg\n",
    "    def inv(k):\n",
    "        return np.linalg.inv(k + diag_reg * np.eye(k.shape[0]))\n",
    "    \n",
    "    mean_predictor = np.einsum('ij,jk->ik', ntk_test_train, inv(ntk_train_train))\n",
    "    \n",
    "    # Loss\n",
    "    loss = - l2_loss_v1(mean_predictor, np.ones_like(mean_predictor)*c)\n",
    "    return loss\n",
    "\n",
    "test_col_wise_grads_fn = jit(grad(test_loss_adv_col, argnums=0), static_argnums=(2,))\n",
    "test_col_wise_c_grads_fn = jit(grad(test_loss_adv_col, argnums=3), static_argnums=(2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss_adv_row(x_train, x_test, kernel_fn, c, t=None):\n",
    "    \"\"\"\n",
    "    Kernel -> matrix with constant rows\n",
    "    \n",
    "    c is a vector of constant. c.shape should be (1, ndim)\n",
    "    \n",
    "    \"\"\" \n",
    "    assert c.shape[0] == 1\n",
    "    assert len(c.shape) == 2\n",
    "    \n",
    "    ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "    ntk_test_train = kernel_fn(x_test, x_train, 'ntk')\n",
    "    \n",
    "    # Θ(test, train)Θ(train, train)^-1\n",
    "    # inverse with diag_reg\n",
    "    def inv(k):\n",
    "        return np.linalg.inv(k + diag_reg * np.eye(k.shape[0]))\n",
    "    \n",
    "    mean_predictor = np.einsum('ij,jk->ik', ntk_test_train, inv(ntk_train_train))\n",
    "    \n",
    "    # Loss\n",
    "    loss = - l2_loss_v1(mean_predictor, np.ones_like(mean_predictor)*c.T)\n",
    "    return loss\n",
    "\n",
    "test_row_wise_grads_fn = jit(grad(test_loss_adv_row, argnums=0), static_argnums=(2,))\n",
    "test_row_wise_c_grads_fn = jit(grad(test_loss_adv_row, argnums=3), static_argnums=(2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pred_loss_adv(x_train, x_test, y_train, y, kernel_fn, loss='mse', t=None):\n",
    "    \"\"\" update {Kernel_M,N x Kernel_N,N x (dynamic of t)} \"\"\"\n",
    "    \n",
    "    ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "    ntk_test_train = kernel_fn(x_test, x_train, 'ntk')\n",
    "    \n",
    "    # Prediction\n",
    "    predict_fn = nt.predict.gradient_descent_mse(ntk_train_train, y_train, diag_reg=diag_reg) # diag_reg: add to easier inverse\n",
    "    fx = predict_fn(t, 0., 0., ntk_test_train)[1]\n",
    "    \n",
    "    # Loss\n",
    "    if loss == 'cross-entropy':\n",
    "        loss = cross_entropy_loss(fx, y)\n",
    "    elif loss == 'mse':\n",
    "        loss = mse_loss(fx, y)\n",
    "    return loss\n",
    "\n",
    "test_pred_grads_fn = jit(grad(test_pred_loss_adv, argnums=0), static_argnums=(4, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, grads_c_fn=None, x_train=None, y_train=None, x_test=None, \n",
    "                         y=None, t=None, c=None, update_c=False, loss='cross-entropy', loss_weighting=None, phase=None, \n",
    "                         fx_train_0=0., fx_test_0=0., eps=0.3, norm=np.inf, clip_min=None, clip_max=None, targeted=False):\n",
    "    \"\"\"\n",
    "    JAX implementation of the Fast Gradient Method.\n",
    "    :param model_fn: a callable that takes an input tensor and returns the model logits.\n",
    "    :param x: input tensor.\n",
    "    :param eps: epsilon (input variation parameter); see https://arxiv.org/abs/1412.6572.\n",
    "    :param norm: Order of the norm (mimics NumPy). Possible values: np.inf or 2.\n",
    "    :param clip_min: (optional) float. Minimum float value for adversarial example components.\n",
    "    :param clip_max: (optional) float. Maximum float value for adversarial example components.\n",
    "    :param y: (optional) Tensor with one-hot true labels. If targeted is true, then provide the\n",
    "            target one-hot label. Otherwise, only provide this parameter if you'd like to use true\n",
    "            labels when crafting adversarial samples. Otherwise, model predictions are used\n",
    "            as labels to avoid the \"label leaking\" effect (explained in this paper:\n",
    "            https://arxiv.org/abs/1611.01236). Default is None. This argument does not have\n",
    "            to be a binary one-hot label (e.g., [0, 1, 0, 0]), it can be floating points values\n",
    "            that sum up to 1 (e.g., [0.05, 0.85, 0.05, 0.05]).\n",
    "    :param targeted: (optional) bool. Is the attack targeted or untargeted?\n",
    "            Untargeted, the default, will try to make the label incorrect.\n",
    "            Targeted will instead try to move in the direction of being more like y.\n",
    "    :return: a tensor for the adversarial example\n",
    "    \"\"\"\n",
    "    if norm not in [np.inf, 2]:\n",
    "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
    "        \n",
    "    x = x_train\n",
    "    # Obtain y\n",
    "    if obj_fn == 'test':\n",
    "        if y is None:\n",
    "            # Using model predictions as ground truth to avoid label leaking\n",
    "            x_labels = np.argmax(model_fn(kernel_fn, 'test', x_train, x_test, fx_train_0, fx_test_0)[1], 1)\n",
    "            y = one_hot(x_labels, 10)\n",
    "            \n",
    "    elif obj_fn == 'train':\n",
    "        if y is None:\n",
    "            # Compute NTK on training data\n",
    "            ntk_train_train = model_fn(kernel_fn=kernel_fn, obj_fn='train', x_train=x_train)\n",
    "            \n",
    "            # Construct diagonal\n",
    "            if phase == 'ordered':\n",
    "                y = np.ones(ntk_train_train.shape)*100\n",
    "            elif phase == 'chaotic':\n",
    "                y = np.eye(ntk_train_train.shape[0])*100\n",
    "            else:\n",
    "                raise ValueError(\"Phase must be either 'ordered' or 'critical'\")\n",
    "    \n",
    "    # Obtain gradient\n",
    "    # Obj - Θ(train, train)\n",
    "    if obj_fn == 'train':\n",
    "        grads = grads_fn(x_train, y, kernel_fn, loss_weighting)\n",
    "        \n",
    "    # Obj - Θ(test, train)Θ(train, train)^-1\n",
    "    elif obj_fn == 'test_c':\n",
    "        grads = 0\n",
    "        grads_c = 0\n",
    "        for i in range(int(len(x_test)/batch_size)):\n",
    "            grads += grads_fn(x_train, x_test[batch_size*i:batch_size*(i+1)], kernel_fn, c, t)\n",
    "            if update_c is True:\n",
    "                grads_c += grads_c_fn(x_train, x_test[batch_size*i:batch_size*(i+1)], kernel_fn, c, t)\n",
    "                \n",
    "        grads_c = 3e-6 * np.sign(grads_c) # grads_c = 5e-2 * np.sign(grads_c)\n",
    "        \n",
    "        \n",
    "    # Obj - Θ(test, train)Θ(train, train)^-1 y_train\n",
    "    elif obj_fn == 'test':\n",
    "        grads = 0\n",
    "        for i in range(int(len(x_test)/batch_size)):\n",
    "            batch_grads = grads_fn(x_train, \n",
    "                                   x_test[batch_size*i:batch_size*(i+1)], \n",
    "                                   y_train, \n",
    "                                   y[batch_size*i:batch_size*(i+1)], \n",
    "                                   kernel_fn, \n",
    "                                   loss,\n",
    "                                   t)\n",
    "            grads += batch_grads\n",
    "    else:\n",
    "        raise ValueError(\"Objective function must be either train(ntk_train_train) or test(predict_fn)\")\n",
    "\n",
    "    axis = list(range(1, len(grads.shape)))\n",
    "    eps_div = 1e-12\n",
    "    \n",
    "    if norm == np.inf:\n",
    "        perturbation = eps * np.sign(grads)\n",
    "    elif norm == 1:\n",
    "        raise NotImplementedError(\"L_1 norm has not been implemented yet.\")\n",
    "    elif norm == 2:\n",
    "        square = np.maximum(eps_div, np.sum(np.square(grads), axis=axis, keepdims=True))\n",
    "        perturbation = grads / np.sqrt(square)\n",
    "    \n",
    "    adv_x = x + perturbation\n",
    "    \n",
    "    # If clipping is needed, reset all values outside of [clip_min, clip_max]\n",
    "    if (clip_min is not None) or (clip_max is not None):\n",
    "        # We don't currently support one-sided clipping\n",
    "        assert clip_min is not None and clip_max is not None\n",
    "        adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "    \n",
    "    if obj_fn == 'test_c':\n",
    "        c += grads_c\n",
    "        \n",
    "        return adv_x, c\n",
    "    \n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_gradient_descent(model_fn, kernel_fn, obj_fn, grads_fn, grads_c_fn=None, x_train=None, y_train=None,\n",
    "                               x_test=None, y=None, t=None, c=None, update_c=None, loss='cross-entropy', loss_weighting=None, \n",
    "                               phase=None, fx_train_0=0., fx_test_0=0., eps=0.3, eps_iter=0.03, nb_iter=10, norm=np.inf, \n",
    "                               clip_min=None, clip_max=None, targeted=False, rand_init=None, rand_minmax=0.3):\n",
    "    \"\"\"\n",
    "    This class implements either the Basic Iterative Method\n",
    "    (Kurakin et al. 2016) when rand_init is set to 0. or the\n",
    "    Madry et al. (2017) method when rand_minmax is larger than 0.\n",
    "    Paper link (Kurakin et al. 2016): https://arxiv.org/pdf/1607.02533.pdf\n",
    "    Paper link (Madry et al. 2017): https://arxiv.org/pdf/1706.06083.pdf\n",
    "    :param model_fn: a callable that takes an input tensor and returns the model logits.\n",
    "    :param x: input tensor.\n",
    "    :param eps: epsilon (input variation parameter); see https://arxiv.org/abs/1412.6572.\n",
    "    :param eps_iter: step size for each attack iteration\n",
    "    :param nb_iter: Number of attack iterations.\n",
    "    :param norm: Order of the norm (mimics NumPy). Possible values: np.inf or 2.\n",
    "    :param clip_min: (optional) float. Minimum float value for adversarial example components.\n",
    "    :param clip_max: (optional) float. Maximum float value for adversarial example components.\n",
    "    :param y: (optional) Tensor with true labels. If targeted is true, then provide the\n",
    "            target label. Otherwise, only provide this parameter if you'd like to use true\n",
    "            labels when crafting adversarial samples. Otherwise, model predictions are used\n",
    "            as labels to avoid the \"label leaking\" effect (explained in this paper:\n",
    "            https://arxiv.org/abs/1611.01236). Default is None.\n",
    "    :param targeted: (optional) bool. Is the attack targeted or untargeted?\n",
    "            Untargeted, the default, will try to make the label incorrect.\n",
    "            Targeted will instead try to move in the direction of being more like y.\n",
    "    :return: a tensor for the adversarial example\n",
    "    \"\"\"\n",
    "\n",
    "    assert eps_iter <= eps, (eps_iter, eps)\n",
    "    if norm == 1:\n",
    "        raise NotImplementedError(\"It's not clear that FGM is a good inner loop\"\n",
    "                                  \" step for PGD when norm=1, because norm=1 FGM \"\n",
    "                                  \" changes only one pixel at a time. We need \"\n",
    "                                  \" to rigorously test a strong norm=1 PGD \"\n",
    "                                  \"before enabling this feature.\")\n",
    "    if norm not in [np.inf, 2]:\n",
    "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
    "        \n",
    "    x = x_train\n",
    "    # Obtain y\n",
    "    if obj_fn == 'test':\n",
    "        if y is None:\n",
    "            # Using model predictions as ground truth to avoid label leaking\n",
    "            x_labels = np.argmax(model_fn(kernel_fn, 'test', x_train, x_test, fx_train_0, fx_test_0)[1], 1)\n",
    "            y = one_hot(x_labels, 10)\n",
    "            \n",
    "    elif obj_fn == 'train':\n",
    "        if y is None:\n",
    "            # Compute NTK on training data\n",
    "            ntk_train_train = model_fn(kernel_fn=kernel_fn, obj_fn='train', x_train=x_train)\n",
    "            \n",
    "            # Construct diagonal\n",
    "            if phase == 'ordered':\n",
    "                y = np.ones(ntk_train_train.shape)*100\n",
    "            elif phase == 'chaotic':\n",
    "                y = np.eye(ntk_train_train.shape[0])*100\n",
    "            else:\n",
    "                raise ValueError(\"Phase must be either 'ordered' or 'critical'\")\n",
    "        \n",
    "    # Initialize loop variables\n",
    "    if rand_init:\n",
    "        rand_minmax = eps\n",
    "        eta = random.uniform(new_key, x.shape, minval=-rand_minmax, maxval=rand_minmax)\n",
    "    else:\n",
    "        eta = np.zeros_like(x)\n",
    "\n",
    "    # Clip eta\n",
    "    eta = clip_eta(eta, norm, eps)\n",
    "    adv_x = x + eta\n",
    "    if clip_min is not None or clip_max is not None:\n",
    "        adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "        \n",
    "    for i in range(nb_iter):\n",
    "        if update_c is not None and (i+1) % update_c == 0:\n",
    "            adv_x = fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, grads_c_fn, adv_x, y_train, x_test, \n",
    "                                         y, t, c, True, loss, loss_weighting, phase, fx_train_0, fx_test_0, eps_iter, norm, \n",
    "                                         clip_min, clip_max, targeted)\n",
    "        else:\n",
    "            adv_x = fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, grads_c_fn, adv_x, y_train, x_test, \n",
    "                                         y, t, c, False, loss, loss_weighting, phase, fx_train_0, fx_test_0, eps_iter, norm, \n",
    "                                         clip_min, clip_max, targeted)\n",
    "        \n",
    "        if obj_fn == 'test_c':\n",
    "            adv_x, c = adv_x\n",
    "\n",
    "        # Clipping perturbation eta to norm norm ball\n",
    "        eta = adv_x - x\n",
    "        eta = clip_eta(eta, norm, eps)\n",
    "        adv_x = x + eta\n",
    "\n",
    "        # Redo the clipping.\n",
    "        # FGM already did it, but subtracting and re-adding eta can add some\n",
    "        # small numerical error.\n",
    "        if clip_min is not None or clip_max is not None:\n",
    "            adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "\n",
    "    if obj_fn == 'test_c':\n",
    "        return adv_x, c\n",
    "    \n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = 0.05\n",
    "b = 0.18\n",
    "W = [1., 1.76, 2.5]\n",
    "phase_list = ['Ordered', 'Critical', 'Chaotic']\n",
    "layers = np.arange(19, 100, 20)\n",
    "layer = 50\n",
    "num_classes = 10\n",
    "\n",
    "kernel_list = []\n",
    "fx_train_0_list = []\n",
    "fx_test_0_list = []\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "b_std = np.sqrt(b)\n",
    "for w in W:\n",
    "    W_std = np.sqrt(w)\n",
    "\n",
    "    init_fn, apply_fn, kernel_fn = stax.serial(DenseGroup(layer, 1024, W_std, b_std))\n",
    "    \n",
    "    # Inference with a single infinite width / linearized network\n",
    "    apply_fn = jit(apply_fn)\n",
    "    kernel_fn = jit(kernel_fn, static_argnums=(2,))\n",
    "    kernel_list.append(kernel_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordered\n",
      "[[1.37379235 1.37379195 1.37379177 1.37379194]\n",
      " [1.37379195 1.37379236 1.3737918  1.37379179]\n",
      " [1.37379177 1.3737918  1.37379235 1.37379195]\n",
      " [1.37379194 1.37379179 1.37379195 1.37379234]]\n",
      "\n",
      "Critical\n",
      "[[30.82217759 13.98496226 12.30888758 13.81286374]\n",
      " [13.98496226 30.91200152 12.64773843 12.66519516]\n",
      " [12.30888758 12.64773843 30.75443008 13.79566112]\n",
      " [13.81286374 12.66519516 13.79566112 30.68698567]]\n",
      "\n",
      "Chaotic\n",
      "[[2.07880683e+04 2.98662443e+00 2.89718017e+00 2.97092652e+00]\n",
      " [2.98662443e+00 2.12011832e+04 2.91790614e+00 2.92074674e+00]\n",
      " [2.89718017e+00 2.91790614e+00 2.04736293e+04 2.96281474e+00]\n",
      " [2.97092652e+00 2.92074674e+00 2.96281474e+00 2.01557097e+04]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, kernel_fn in enumerate(kernel_list):\n",
    "    print(phase_list[idx])\n",
    "    print(kernel_fn(x_train, x_train, 'ntk')[:4, :4])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_type = [\"Clean\", \"FGSM\", \"PGD-10\", \"PGD-100\"]\n",
    "\n",
    "####### MNIST #######\n",
    "eps = 0.3\n",
    "eps_iter_10 = 0.04\n",
    "eps_iter_100 = 0.004\n",
    "####### MNIST #######\n",
    "\n",
    "####### CIFAR #######\n",
    "# eps = 16/255\n",
    "# eps_iter_10 = (eps/10)*1.1\n",
    "# eps_iter_100 = (eps/100)*1.1\n",
    "####### CIFAR #######\n",
    "\n",
    "val_size = 1200\n",
    "\n",
    "# x_train_all is on host device\n",
    "x_val = x_train_all[train_size:train_size+val_size]\n",
    "y_val = y_train_all[train_size:train_size+val_size]\n",
    "\n",
    "# to gpu\n",
    "x_val = np.asarray(x_val)\n",
    "y_val = np.asarray(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_predictor(x_train, x_test, kernel_fn, c=None):\n",
    "    \"\"\"\n",
    "    return Θ(test, train)Θ(train, train)^-1 and \n",
    "    || # Θ(test, train)Θ(train, train)^-1 - target ||\n",
    "    \n",
    "    \"\"\"\n",
    "    # Kernel\n",
    "    ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "    ntk_test_train = kernel_fn(x_test, x_train, 'ntk')\n",
    "    \n",
    "    # Θ(test, train)Θ(train, train)^-1\n",
    "    def inv(k):\n",
    "        return np.linalg.inv(k + diag_reg * np.eye(k.shape[0]))\n",
    "    mean_predictor = np.einsum('ij,jk->ik', ntk_test_train, inv(ntk_train_train))\n",
    "    \n",
    "    if c is None:\n",
    "        c = np.mean(mean_predictor)\n",
    "    \n",
    "    # Loss\n",
    "    loss = l2_loss_v1(mean_predictor, np.ones_like(mean_predictor)*c)\n",
    "    return loss, mean_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_predictor_row_col_wise(x_train, x_test, kernel_fn, c=None, row=False, col=False):\n",
    "    \"\"\"\n",
    "    return Θ(test, train)Θ(train, train)^-1 and \n",
    "    || # Θ(test, train)Θ(train, train)^-1 - target ||\n",
    "    \n",
    "    \"\"\"\n",
    "    if not row and not col:\n",
    "        raise ValueError(\"at least one of row or col should be true\")\n",
    "    # Kernel\n",
    "    ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "    ntk_test_train = kernel_fn(x_test, x_train, 'ntk')\n",
    "    \n",
    "    # Θ(test, train)Θ(train, train)^-1\n",
    "    def inv(k):\n",
    "        return np.linalg.inv(k + diag_reg * np.eye(k.shape[0]))\n",
    "    mean_predictor = np.einsum('ij,jk->ik', ntk_test_train, inv(ntk_train_train))\n",
    "    loss = 0.0\n",
    "    if c is None:\n",
    "        if row:\n",
    "            c = np.mean(mean_predictor, axis=1)\n",
    "            c = np.reshape(c, (1, -1))\n",
    "            loss = l2_loss_v1(mean_predictor, np.ones_like(mean_predictor)*c.T)\n",
    "        else:\n",
    "            c = np.mean(mean_predictor, axis=0)\n",
    "            loss = l2_loss_v1(mean_predictor, np.ones_like(mean_predictor)*c)\n",
    "    \n",
    "    return loss, mean_predictor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a = np.asarray([[1.,2.],\n",
    "                [3.,4.]])\n",
    "\n",
    "c = np.mean(a, axis=1)\n",
    "c = np.reshape(c, (1, -1))\n",
    "print(l2_loss_v1(a, np.ones_like(a)*c.T))\n",
    "a = np.asarray([[1.,2.],\n",
    "                [3.,4.]])\n",
    "\n",
    "c = np.mean(a, axis=0)\n",
    "print(l2_loss_v1(a, np.ones_like(a)*c))\n",
    "print(l2_loss_v1(a, np.ones_like(a)*np.mean(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordered\n",
      "Mean: 0.00390625, Loss: 0.22936214\n",
      "[[0.00278381 0.00341212 0.0043551  0.00373197 0.00363911]\n",
      " [0.00453375 0.00385129 0.00287366 0.00392162 0.00422013]\n",
      " [0.00415563 0.00306797 0.00385278 0.00317157 0.00361015]\n",
      " [0.00512126 0.00674182 0.00466665 0.00381732 0.00401951]\n",
      " [0.00400983 0.00532461 0.00412389 0.00444167 0.00449761]]\n",
      "\n",
      "Critical\n",
      "Mean: 0.00387526, Loss: 43.94466153\n",
      "[[-0.01028681 -0.00859151  0.00626522  0.00538237 -0.0004104 ]\n",
      " [ 0.02590303  0.00804861 -0.00252358 -0.00456444  0.01541304]\n",
      " [ 0.02432264 -0.01303433  0.00359057 -0.00767022 -0.00457796]\n",
      " [ 0.03438266  0.0779383   0.00298547 -0.00184229 -0.00637986]\n",
      " [ 0.00085543  0.02853235 -0.00523987  0.01379236  0.03353651]]\n",
      "\n",
      "Chaotic\n",
      "Mean: 0.00013465, Loss: 0.00000387\n",
      "[[0.00013403 0.00013083 0.00013597 0.00013877 0.00013605]\n",
      " [0.0001446  0.00013372 0.00013553 0.00014188 0.00014561]\n",
      " [0.00013837 0.00013142 0.0001366  0.00013958 0.00013829]\n",
      " [0.00014457 0.00014643 0.00013819 0.00014086 0.00014079]\n",
      " [0.0001388  0.00013673 0.00013699 0.00014232 0.00014336]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, kernel_fn in enumerate(kernel_list):\n",
    "    l, m = mean_predictor_row_col_wise(x_train, x_val, kernel_fn, row=True)\n",
    "    print(phase_list[idx])\n",
    "    print(\"Mean: {:.8f}, Loss: {:.8f}\".format(np.mean(m), l))\n",
    "    print(m[:5, :5])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordered\n",
      "Mean: 0.00390625, Loss: 0.22936214\n",
      "[[0.00278381 0.00341212 0.0043551  0.00373197 0.00363911]\n",
      " [0.00453375 0.00385129 0.00287366 0.00392162 0.00422013]\n",
      " [0.00415563 0.00306797 0.00385278 0.00317157 0.00361015]\n",
      " [0.00512126 0.00674182 0.00466665 0.00381732 0.00401951]\n",
      " [0.00400983 0.00532461 0.00412389 0.00444167 0.00449761]]\n",
      "\n",
      "Critical\n",
      "Mean: 0.00387526, Loss: 43.94523938\n",
      "[[-0.01028681 -0.00859151  0.00626522  0.00538237 -0.0004104 ]\n",
      " [ 0.02590303  0.00804861 -0.00252358 -0.00456444  0.01541304]\n",
      " [ 0.02432264 -0.01303433  0.00359057 -0.00767022 -0.00457796]\n",
      " [ 0.03438266  0.0779383   0.00298547 -0.00184229 -0.00637986]\n",
      " [ 0.00085543  0.02853235 -0.00523987  0.01379236  0.03353651]]\n",
      "\n",
      "Chaotic\n",
      "Mean: 0.00013465, Loss: 0.00000471\n",
      "[[0.00013403 0.00013083 0.00013597 0.00013877 0.00013605]\n",
      " [0.0001446  0.00013372 0.00013553 0.00014188 0.00014561]\n",
      " [0.00013837 0.00013142 0.0001366  0.00013958 0.00013829]\n",
      " [0.00014457 0.00014643 0.00013819 0.00014086 0.00014079]\n",
      " [0.0001388  0.00013673 0.00013699 0.00014232 0.00014336]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, kernel_fn in enumerate(kernel_list):\n",
    "    l, m = mean_predictor(x_train, x_val, kernel_fn)\n",
    "    print(phase_list[idx])\n",
    "    print(\"Mean: {:.8f}, Loss: {:.8f}\".format(np.mean(m), l))\n",
    "    print(m[:5, :5])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "b = 0.05\n",
    "b = 0.18\n",
    "W = [1., 1.76, 2.5]\n",
    "phase_list = ['Ordered', 'Critical', 'Chaotic']\n",
    "layers = np.arange(19, 100, 20)\n",
    "layer = 100\n",
    "num_classes = 10\n",
    "\n",
    "kernel_list = []\n",
    "fx_train_0_list = []\n",
    "fx_test_0_list = []\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "b_std = np.sqrt(b)\n",
    "for w in W:\n",
    "    W_std = np.sqrt(w)\n",
    "\n",
    "    init_fn, apply_fn, kernel_fn = stax.serial(DenseGroup(layer, 1024, W_std, b_std))\n",
    "    \n",
    "    # Inference with a single infinite width / linearized network\n",
    "    apply_fn = jit(apply_fn)\n",
    "    kernel_fn = jit(kernel_fn, static_argnums=(2,))\n",
    "    kernel_list.append(kernel_fn)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for idx, kernel_fn in enumerate(kernel_list):\n",
    "    l, m = mean_predictor(x_train, x_val, kernel_fn)\n",
    "    print(phase_list[idx])\n",
    "    print(\"Mean: {:.8f}, Loss: {:.8f}\".format(np.mean(m), l))\n",
    "    print(m[:5, :5])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for idx, kernel_fn in enumerate(kernel_list):\n",
    "    l, m = mean_predictor_row_col_wise(x_train, x_val, kernel_fn, row=True)\n",
    "    print(phase_list[idx])\n",
    "    print(\"Mean: {:.8f}, Loss: {:.8f}\".format(np.mean(m), l))\n",
    "    print(m[:5, :5])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(x_train, x_test, model_fn, kernel_fn, t=None, c=0, attack_type=None):\n",
    "\n",
    "    y_train_predict, y_test_predict = model_fn(kernel_fn, 'test', x_train, x_test, t=t)\n",
    "    print(len(y_test_predict))\n",
    "    acc = accuracy(y_test_predict, y_test)\n",
    "    print(\"Robustness({:s}): {:.2f}\".format(attack_type, acc))\n",
    "    \n",
    "    # Mean predictor\n",
    "    l, m = mean_predictor(x_train, x_test, kernel_fn, c)\n",
    "    # print(\"c:{:.8f}, Mean: {:.8f}, Loss: {:.8f}\".format(c_list[idx], np.mean(m), l))\n",
    "    print(m[:5, :5])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train list gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "Robustness(Clean): 0.86\n",
      "[[ 0.00112736  0.01011385  0.00287164  0.001241    0.00544948]\n",
      " [-0.0015968   0.00132874  0.01696907 -0.00243109 -0.00083482]\n",
      " [-0.00199748  0.01193599 -0.00562409 -0.00149281  0.00288802]\n",
      " [-0.00396522  0.012832    0.0027059  -0.00103998  0.00695626]\n",
      " [ 0.04279609 -0.00182015 -0.00166272 -0.00105851 -0.00062796]]\n",
      "\n",
      "256\n",
      "Robustness(FGSM): 0.09\n",
      "[[ 1.65984837e-01 -6.35974114e-03  3.24818026e-03  1.59670319e-02\n",
      "  -4.08938612e-03]\n",
      " [-2.94585783e-03  2.37932304e-01  1.03596277e-03 -6.07121633e-05\n",
      "   1.06485829e-03]\n",
      " [ 4.89280146e-04 -2.03782376e-04  2.72276924e-01 -3.43047163e-03\n",
      "   2.72363970e-03]\n",
      " [ 2.33369591e-02  5.27400725e-04  6.82714139e-04  1.77348016e-01\n",
      "   1.58509089e-03]\n",
      " [-2.24913040e-03  7.87222081e-03  5.61269454e-03 -2.73629669e-03\n",
      "   2.06071546e-01]]\n",
      "\n",
      "256\n",
      "Robustness(PGD-10): 0.09\n",
      "[[ 4.74701985e-01 -2.96471411e-03  3.39336026e-04  2.34685420e-02\n",
      "  -1.23378545e-03]\n",
      " [-1.88852643e-03  3.79469140e-01  3.46437081e-03 -3.10219776e-03\n",
      "   8.70444183e-04]\n",
      " [-1.27694120e-03  1.50582072e-03  2.67389730e-01 -2.64740519e-03\n",
      "   5.28430814e-03]\n",
      " [ 1.89059648e-02  3.81730740e-04 -1.13467486e-03  5.39466856e-01\n",
      "  -1.23971353e-04]\n",
      " [-1.49563043e-03  2.66925382e-03  1.42961865e-02 -3.80183524e-03\n",
      "   3.42701260e-01]]\n",
      "\n",
      "256\n",
      "Robustness(PGD-100): 0.09\n",
      "[[ 3.26440338e-01 -3.68287983e-03  2.97315411e-04  3.02491881e-02\n",
      "  -1.24941502e-03]\n",
      " [-1.00014626e-03  2.55133825e-01  5.38465251e-03 -5.39830785e-03\n",
      "   7.24952425e-04]\n",
      " [-1.44469801e-03  1.50576079e-03  2.39929444e-01 -2.61179822e-03\n",
      "   5.76403315e-03]\n",
      " [ 2.65573470e-02 -5.82765027e-04 -6.40655762e-04  3.57371978e-01\n",
      "   8.16965736e-04]\n",
      " [-1.56293397e-03  2.88813625e-03  1.36570321e-02 -4.00394230e-03\n",
      "   3.24149155e-01]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = 1024\n",
    "c = np.zeros((1, train_size)) + 0.00387526\n",
    "loss_type = ['cross-entropy', 'mse']\n",
    "\n",
    "for _ in range(1):\n",
    "    x_test_list = []\n",
    "    c_list = []\n",
    "    \n",
    "    evaluate(x_train, x_test, model_fn=model_fn, kernel_fn=kernel_list[1], t=t, c=0, attack_type='Clean')\n",
    "    x_test_list.append(x_test)\n",
    "    c_list.append(c)\n",
    "\n",
    "    # FGSM\n",
    "    adv_x, c = fast_gradient_method(model_fn=model_fn, kernel_fn=kernel_list[1], obj_fn='test_c', \n",
    "                                    grads_fn=test_col_wise_grads_fn, grads_c_fn=test_col_wise_c_grads_fn,\n",
    "                                    x_train=x_train, y_train=y_train, x_test=x_test, y=y_test, t=t, c=c,\n",
    "                                    loss=loss_type[0], eps=eps, clip_min=0, clip_max=1)\n",
    "    \n",
    "    evaluate(x_train, adv_x, model_fn=model_fn, kernel_fn=kernel_list[1], t=t, c=c, attack_type='FGSM')\n",
    "    x_test_list.append(adv_x)\n",
    "    c_list.append(c)\n",
    "    \n",
    "    # PGD 10\n",
    "    key, new_key = random.split(key)\n",
    "    c = np.zeros((1, train_size)) + 0.00387526\n",
    "    adv_x, c = projected_gradient_descent(model_fn=model_fn, kernel_fn=kernel_list[1], obj_fn='test_c', \n",
    "                                          grads_fn=test_col_wise_grads_fn, grads_c_fn=test_col_wise_c_grads_fn,\n",
    "                                          x_train=x_train, y_train=y_train, x_test=x_test, y=y_test, t=t, c=c, update_c=3, \n",
    "                                          loss=loss_type[0], eps=eps, eps_iter=eps_iter_10, nb_iter=10, \n",
    "                                          clip_min=0, clip_max=1, rand_init=None, rand_minmax=eps)\n",
    "    \n",
    "    evaluate(x_train, adv_x, model_fn=model_fn, kernel_fn=kernel_list[1], t=t, c=c, attack_type='PGD-10')\n",
    "    x_test_list.append(adv_x)\n",
    "    c_list.append(c)\n",
    "    \n",
    "    # PGD 100\n",
    "    key, new_key = random.split(key)\n",
    "    c = np.zeros((1, train_size)) + 0.00387526\n",
    "    adv_x, c = projected_gradient_descent(model_fn=model_fn, kernel_fn=kernel_list[1], obj_fn='test_c', \n",
    "                                          grads_fn=test_col_wise_grads_fn, grads_c_fn=test_col_wise_c_grads_fn,\n",
    "                                          x_train=x_train, y_train=y_train, x_test=x_test, y=y_test, t=t, c=c, update_c=3, \n",
    "                                          loss=loss_type[0], eps=eps, eps_iter=eps_iter_100, nb_iter=100, \n",
    "                                          clip_min=0, clip_max=1, rand_init=None, rand_minmax=eps)\n",
    "    \n",
    "    evaluate(x_train, adv_x, model_fn=model_fn, kernel_fn=kernel_list[1], t=t, c=c, attack_type='PGD-100')\n",
    "    x_test_list.append(adv_x)\n",
    "    c_list.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f72f9fbb0f0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[6].reshape((32 ,32, 3)))\n",
    "# plt.imshow(x_test_list[1][6].reshape((32 ,32, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robustness(Clean): 0.85\n",
      "[[ 0.00687829 -0.0011803  -0.00094008  0.00337229 -0.00221573]\n",
      " [ 0.00264893  0.01368598 -0.00512646  0.00174916  0.00510037]\n",
      " [-0.00156034  0.00735007 -0.00224796  0.00666143 -0.00181644]\n",
      " [ 0.00569939  0.03786994 -0.00443358  0.00011937 -0.00318557]\n",
      " [-0.00451086  0.00255381  0.00585173  0.00114909  0.0137853 ]]\n",
      "\n",
      "Robustness(FGSM): 0.75\n",
      "[[ 0.00211986 -0.00267262 -0.00214587  0.00497471 -0.0037054 ]\n",
      " [-0.00482155  0.01565273 -0.00790824  0.00423193  0.00393043]\n",
      " [-0.0007713   0.02280377 -0.00269094  0.00693968 -0.00094084]\n",
      " [-0.00050436  0.03085381 -0.00565922  0.00815181 -0.00369204]\n",
      " [-0.00985743  0.00281936  0.00359856 -0.00060674  0.01505319]]\n",
      "\n",
      "Robustness(PGD-10): 0.72\n",
      "[[ 0.01135664 -0.00021928 -0.00109051  0.01912503 -0.00077279]\n",
      " [ 0.00154425  0.01739849 -0.00651074  0.00980257  0.00733367]\n",
      " [ 0.00548791  0.01814354 -0.00293031  0.01734137  0.00097655]\n",
      " [ 0.00538715  0.03585048 -0.0044349   0.01199747 -0.0001929 ]\n",
      " [-0.00615319  0.00652969  0.00256713  0.00666634  0.01660516]]\n",
      "\n",
      "Robustness(PGD-100): 0.73\n",
      "[[ 0.00628725 -0.00017944 -0.00059791  0.00235531 -0.00095372]\n",
      " [ 0.00050193  0.01260655 -0.00499117  0.00259841  0.00628949]\n",
      " [ 0.00156173  0.01009288 -0.00160697  0.00493074  0.00053814]\n",
      " [ 0.0023795   0.02665612 -0.00322468  0.00330837  0.00023991]\n",
      " [-0.00534104  0.00552491  0.00339622 -0.00055272  0.01395765]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, _x_train in enumerate(x_train_list):\n",
    "        _, fx_adv_test_t = model_fn(kernel_list[1], 'test', _x_train, x_test, t=t)\n",
    "        acc = accuracy(fx_adv_test_t, y_test)\n",
    "        acc_l.append(acc)\n",
    "        print(\"Robustness({:s}): {:.2f}\".format(attack_type[idx], acc))\n",
    "        \n",
    "        # Mean predictor\n",
    "        l, m = mean_predictor(_x_train, x_val, kernel_list[1], c_list[idx])\n",
    "        # print(\"c:{:.8f}, Mean: {:.8f}, Loss: {:.8f}\".format(c_list[idx], np.mean(m), l))\n",
    "        print(m[:5, :5])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
