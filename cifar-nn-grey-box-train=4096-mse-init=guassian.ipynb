{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.0\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "\n",
    "import numpy as onp\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "\n",
    "from jax import lax, random\n",
    "from jax.api import grad, jit, vmap\n",
    "from jax.config import config\n",
    "from jax.experimental import optimizers\n",
    "from jax.experimental.stax import logsoftmax\n",
    "\n",
    "config.update('jax_enable_x64', True)\n",
    "\n",
    "from neural_tangents import stax\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# Attacking\n",
    "from cleverhans.utils import clip_eta, one_hot\n",
    "\n",
    "# Plotting\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from utils import *\n",
    "\n",
    "sns.set_style(style='white')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "\"\"\"\n",
    "diag_reg:\n",
    "    a scalar representing the strength of the diagonal regularization for\n",
    "    `k_train_train`, i.e. computing `k_train_train + diag_reg * I` during\n",
    "    Cholesky factorization or eigendecomposition.\n",
    "\"\"\"\n",
    "diag_reg = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'cifar10'\n",
    "class_num   = 10\n",
    "image_shape = None\n",
    "\n",
    "train_size = 4096\n",
    "valid_size = 512\n",
    "test_size = 128\n",
    "\n",
    "batch_size = 256\n",
    "eps = 0.03\n",
    "epochs = 1000\n",
    "\n",
    "if DATASET =='mnist':\n",
    "    image_shape = (28, 28, 1)\n",
    "elif DATASET == 'cifar10':\n",
    "    image_shape = (32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_all, y_train_all, x_test_all, y_test_all = tuple(onp.array(x) for x in get_dataset(DATASET, None, None, \n",
    "                                                                                  do_flatten_and_normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "seed = 0\n",
    "x_train_all, y_train_all = shaffle(x_train_all, y_train_all, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# down sample\n",
    "x_train = x_train_all[:train_size]\n",
    "y_train = y_train_all[:train_size]\n",
    "\n",
    "x_valid = x_train_all[train_size:train_size + valid_size]\n",
    "y_valid = y_train_all[train_size:train_size + valid_size]\n",
    "\n",
    "x_test = x_test_all[:test_size]\n",
    "y_test = y_test_all[:test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, x_test = x_train.reshape((-1, *image_shape)), x_valid.reshape((-1, *image_shape)), x_test.reshape((-1, *image_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(mean, ys):\n",
    "    return onp.argmax(mean, axis=-1) == onp.argmax(ys, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvBlock(channels, W_std, b_std, strides=(1,1)):\n",
    "    return stax.serial(stax.Conv(out_chan=channels, filter_shape=(3,3), strides=strides, padding='SAME',\n",
    "                                 W_std=W_std, b_std=b_std), \n",
    "                       stax.Relu(do_backprop=True))\n",
    "\n",
    "def ConvGroup(n, channels, stride, W_std, b_std, last_stride=False):\n",
    "    blocks = []\n",
    "    if last_stride:\n",
    "        for i in range(n-1):\n",
    "            blocks += [ConvBlock(channels, W_std, b_std, stride)]\n",
    "        blocks += [ConvBlock(channels, W_std, b_std, (2, 2))]\n",
    "    \n",
    "    else:\n",
    "        for i in range(n):\n",
    "            blocks += [ConvBlock(channels, W_std, b_std, stride)]\n",
    "        \n",
    "    return stax.serial(*blocks)\n",
    "        \n",
    "def VGG19_stride(class_num=class_num):\n",
    "    \n",
    "    return stax.serial(\n",
    "        ConvGroup(n=2, channels=64 , stride=(1,1), W_std=0.1, b_std=0.18, last_stride=True),\n",
    "        ConvGroup(n=2, channels=128, stride=(1,1), W_std=0.1, b_std=0.18, last_stride=True),\n",
    "        ConvGroup(n=4, channels=256, stride=(1,1), W_std=0.1, b_std=0.18, last_stride=True),\n",
    "        ConvGroup(n=4, channels=512, stride=(1,1), W_std=0.1, b_std=0.18, last_stride=True),\n",
    "        ConvGroup(n=4, channels=512, stride=(1,1), W_std=0.1, b_std=0.18, last_stride=True),\n",
    "        stax.Flatten(),\n",
    "        stax.Dense(4096), stax.Relu(do_backprop=True),\n",
    "        stax.Dense(4096), stax.Relu(do_backprop=True),\n",
    "        stax.Dense(class_num))\n",
    "\n",
    "def simple_net(class_num=class_num):\n",
    "    return stax.serial(\n",
    "        ConvGroup(n=3, channels=64 , stride=(1,1), W_std=onp.sqrt(2), b_std=0.0, last_stride=False),\n",
    "        stax.Flatten(),\n",
    "        stax.Dense(256, W_std=onp.sqrt(2), b_std=0.0), stax.Relu(do_backprop=True),\n",
    "        stax.Dense(class_num, W_std=onp.sqrt(2), b_std=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_fn, apply_fn, kernel_fn = simple_net(class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_fn = jit(apply_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(88888)\n",
    "key, net_key = random.split(key)\n",
    "_, params = init_fn(net_key, (-1, 32, 32, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "# training_steps = 3200\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.sgd(learning_rate)\n",
    "opt_update = jit(opt_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = jit(lambda params, x, y: 0.5 * np.mean((apply_fn(params, x) - y) ** 2))\n",
    "grad_loss = jit(lambda state, x, y: grad(loss)(get_params(state), x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = onp.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_state = opt_init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.9993765, dtype=float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onp.std(params[0][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   0: train loss 0.22954, valid loss 0.21737 train acc 0.08960 valid acc 0.10083\n",
      "epoch   1: train loss 0.22033, valid loss 0.20867 train acc 0.09009 valid acc 0.09961\n",
      "epoch   2: train loss 0.21187, valid loss 0.20075 train acc 0.08984 valid acc 0.09888\n",
      "epoch   3: train loss 0.20408, valid loss 0.19336 train acc 0.09033 valid acc 0.09766\n",
      "epoch   4: train loss 0.19690, valid loss 0.18662 train acc 0.08960 valid acc 0.09656\n",
      "epoch   5: train loss 0.19028, valid loss 0.18042 train acc 0.09009 valid acc 0.09570\n",
      "epoch   6: train loss 0.18417, valid loss 0.17466 train acc 0.09033 valid acc 0.09570\n",
      "epoch   7: train loss 0.17849, valid loss 0.16933 train acc 0.09058 valid acc 0.09509\n",
      "epoch   8: train loss 0.17321, valid loss 0.16436 train acc 0.09033 valid acc 0.09766\n",
      "epoch   9: train loss 0.16827, valid loss 0.15971 train acc 0.09058 valid acc 0.09766\n",
      "epoch  10: train loss 0.16366, valid loss 0.15535 train acc 0.09033 valid acc 0.09766\n",
      "epoch  11: train loss 0.15933, valid loss 0.15129 train acc 0.08960 valid acc 0.09766\n",
      "epoch  12: train loss 0.15527, valid loss 0.14747 train acc 0.08936 valid acc 0.09778\n",
      "epoch  13: train loss 0.15145, valid loss 0.14391 train acc 0.08960 valid acc 0.09766\n",
      "epoch  14: train loss 0.14786, valid loss 0.14054 train acc 0.08960 valid acc 0.09766\n",
      "epoch  15: train loss 0.14447, valid loss 0.13735 train acc 0.08911 valid acc 0.09766\n",
      "epoch  16: train loss 0.14127, valid loss 0.13436 train acc 0.08887 valid acc 0.09839\n",
      "epoch  17: train loss 0.13824, valid loss 0.13151 train acc 0.08813 valid acc 0.10071\n",
      "epoch  18: train loss 0.13537, valid loss 0.12882 train acc 0.08813 valid acc 0.10156\n",
      "epoch  19: train loss 0.13265, valid loss 0.12627 train acc 0.08765 valid acc 0.10144\n",
      "epoch  20: train loss 0.13007, valid loss 0.12385 train acc 0.08643 valid acc 0.09900\n",
      "epoch  21: train loss 0.12762, valid loss 0.12156 train acc 0.08618 valid acc 0.09692\n",
      "epoch  22: train loss 0.12529, valid loss 0.11937 train acc 0.08618 valid acc 0.09570\n",
      "epoch  23: train loss 0.12308, valid loss 0.11727 train acc 0.08691 valid acc 0.09900\n",
      "epoch  24: train loss 0.12097, valid loss 0.11529 train acc 0.08716 valid acc 0.09961\n",
      "epoch  25: train loss 0.11895, valid loss 0.11341 train acc 0.08716 valid acc 0.09961\n",
      "epoch  26: train loss 0.11704, valid loss 0.11160 train acc 0.08643 valid acc 0.10132\n",
      "epoch  27: train loss 0.11521, valid loss 0.10988 train acc 0.08643 valid acc 0.10376\n",
      "epoch  28: train loss 0.11346, valid loss 0.10822 train acc 0.08618 valid acc 0.10620\n",
      "epoch  29: train loss 0.11180, valid loss 0.10664 train acc 0.08618 valid acc 0.10669\n",
      "epoch  30: train loss 0.11020, valid loss 0.10514 train acc 0.08545 valid acc 0.10547\n",
      "epoch  31: train loss 0.10868, valid loss 0.10370 train acc 0.08594 valid acc 0.10547\n",
      "epoch  32: train loss 0.10722, valid loss 0.10232 train acc 0.08594 valid acc 0.10706\n",
      "epoch  33: train loss 0.10581, valid loss 0.10099 train acc 0.08521 valid acc 0.10742\n",
      "epoch  34: train loss 0.10447, valid loss 0.09972 train acc 0.08496 valid acc 0.10742\n",
      "epoch  35: train loss 0.10318, valid loss 0.09851 train acc 0.08521 valid acc 0.10742\n",
      "epoch  36: train loss 0.10195, valid loss 0.09735 train acc 0.08423 valid acc 0.10742\n",
      "epoch  37: train loss 0.10076, valid loss 0.09623 train acc 0.08398 valid acc 0.10889\n",
      "epoch  38: train loss 0.09962, valid loss 0.09515 train acc 0.08374 valid acc 0.11133\n",
      "epoch  39: train loss 0.09852, valid loss 0.09411 train acc 0.08374 valid acc 0.11133\n",
      "epoch  40: train loss 0.09746, valid loss 0.09312 train acc 0.08374 valid acc 0.11133\n",
      "epoch  41: train loss 0.09644, valid loss 0.09215 train acc 0.08301 valid acc 0.11133\n",
      "epoch  42: train loss 0.09545, valid loss 0.09123 train acc 0.08350 valid acc 0.11133\n",
      "epoch  43: train loss 0.09450, valid loss 0.09033 train acc 0.08276 valid acc 0.11133\n",
      "epoch  44: train loss 0.09358, valid loss 0.08947 train acc 0.08252 valid acc 0.11072\n",
      "epoch  45: train loss 0.09269, valid loss 0.08863 train acc 0.08179 valid acc 0.10962\n",
      "epoch  46: train loss 0.09183, valid loss 0.08782 train acc 0.08228 valid acc 0.10938\n",
      "epoch  47: train loss 0.09099, valid loss 0.08704 train acc 0.08105 valid acc 0.10925\n",
      "epoch  48: train loss 0.09019, valid loss 0.08627 train acc 0.08081 valid acc 0.10840\n",
      "epoch  49: train loss 0.08940, valid loss 0.08555 train acc 0.08081 valid acc 0.10876\n",
      "epoch  50: train loss 0.08864, valid loss 0.08484 train acc 0.08130 valid acc 0.10742\n",
      "epoch  51: train loss 0.08791, valid loss 0.08415 train acc 0.08081 valid acc 0.10864\n",
      "epoch  52: train loss 0.08720, valid loss 0.08348 train acc 0.07983 valid acc 0.10852\n",
      "epoch  53: train loss 0.08650, valid loss 0.08284 train acc 0.07983 valid acc 0.11230\n",
      "epoch  54: train loss 0.08583, valid loss 0.08221 train acc 0.08008 valid acc 0.11133\n",
      "epoch  55: train loss 0.08518, valid loss 0.08160 train acc 0.08008 valid acc 0.11462\n",
      "epoch  56: train loss 0.08454, valid loss 0.08101 train acc 0.08032 valid acc 0.11523\n",
      "epoch  57: train loss 0.08393, valid loss 0.08043 train acc 0.08105 valid acc 0.11523\n",
      "epoch  58: train loss 0.08333, valid loss 0.07988 train acc 0.08179 valid acc 0.11450\n",
      "epoch  59: train loss 0.08275, valid loss 0.07934 train acc 0.08179 valid acc 0.11316\n",
      "epoch  60: train loss 0.08218, valid loss 0.07881 train acc 0.08203 valid acc 0.11133\n",
      "epoch  61: train loss 0.08163, valid loss 0.07830 train acc 0.08130 valid acc 0.11133\n",
      "epoch  62: train loss 0.08110, valid loss 0.07780 train acc 0.08154 valid acc 0.11133\n",
      "epoch  63: train loss 0.08058, valid loss 0.07732 train acc 0.08154 valid acc 0.11060\n",
      "epoch  64: train loss 0.08008, valid loss 0.07686 train acc 0.08130 valid acc 0.10938\n",
      "epoch  65: train loss 0.07959, valid loss 0.07639 train acc 0.08130 valid acc 0.10938\n",
      "epoch  66: train loss 0.07911, valid loss 0.07595 train acc 0.08105 valid acc 0.10803\n",
      "epoch  67: train loss 0.07865, valid loss 0.07552 train acc 0.08032 valid acc 0.10681\n",
      "epoch  68: train loss 0.07820, valid loss 0.07510 train acc 0.08105 valid acc 0.10535\n",
      "epoch  69: train loss 0.07776, valid loss 0.07469 train acc 0.08032 valid acc 0.10547\n",
      "epoch  70: train loss 0.07733, valid loss 0.07430 train acc 0.08032 valid acc 0.10706\n",
      "epoch  71: train loss 0.07692, valid loss 0.07391 train acc 0.08057 valid acc 0.10596\n",
      "epoch  72: train loss 0.07651, valid loss 0.07353 train acc 0.07983 valid acc 0.10510\n",
      "epoch  73: train loss 0.07612, valid loss 0.07317 train acc 0.08032 valid acc 0.10547\n",
      "epoch  74: train loss 0.07573, valid loss 0.07281 train acc 0.08105 valid acc 0.10583\n",
      "epoch  75: train loss 0.07535, valid loss 0.07246 train acc 0.08105 valid acc 0.10938\n",
      "epoch  76: train loss 0.07499, valid loss 0.07212 train acc 0.08057 valid acc 0.10938\n",
      "epoch  77: train loss 0.07463, valid loss 0.07179 train acc 0.08032 valid acc 0.10938\n",
      "epoch  78: train loss 0.07428, valid loss 0.07146 train acc 0.08032 valid acc 0.10742\n",
      "epoch  79: train loss 0.07394, valid loss 0.07115 train acc 0.08130 valid acc 0.10852\n",
      "epoch  80: train loss 0.07361, valid loss 0.07084 train acc 0.08081 valid acc 0.11121\n",
      "epoch  81: train loss 0.07328, valid loss 0.07054 train acc 0.08105 valid acc 0.10938\n",
      "epoch  82: train loss 0.07297, valid loss 0.07024 train acc 0.08105 valid acc 0.10938\n",
      "epoch  83: train loss 0.07266, valid loss 0.06996 train acc 0.08081 valid acc 0.11072\n",
      "epoch  84: train loss 0.07236, valid loss 0.06968 train acc 0.08032 valid acc 0.11401\n",
      "epoch  85: train loss 0.07206, valid loss 0.06941 train acc 0.08105 valid acc 0.11523\n",
      "epoch  86: train loss 0.07178, valid loss 0.06914 train acc 0.08081 valid acc 0.11523\n",
      "epoch  87: train loss 0.07150, valid loss 0.06888 train acc 0.08057 valid acc 0.11523\n",
      "epoch  88: train loss 0.07123, valid loss 0.06863 train acc 0.08105 valid acc 0.11511\n",
      "epoch  89: train loss 0.07096, valid loss 0.06839 train acc 0.08130 valid acc 0.11365\n",
      "epoch  90: train loss 0.07070, valid loss 0.06815 train acc 0.08130 valid acc 0.11304\n",
      "epoch  91: train loss 0.07044, valid loss 0.06791 train acc 0.08130 valid acc 0.11133\n",
      "epoch  92: train loss 0.07019, valid loss 0.06768 train acc 0.08154 valid acc 0.11133\n",
      "epoch  93: train loss 0.06995, valid loss 0.06745 train acc 0.07983 valid acc 0.11133\n",
      "epoch  94: train loss 0.06971, valid loss 0.06724 train acc 0.08008 valid acc 0.11133\n",
      "epoch  95: train loss 0.06947, valid loss 0.06702 train acc 0.07983 valid acc 0.11047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  96: train loss 0.06924, valid loss 0.06681 train acc 0.07959 valid acc 0.10938\n",
      "epoch  97: train loss 0.06902, valid loss 0.06661 train acc 0.07983 valid acc 0.11023\n",
      "epoch  98: train loss 0.06880, valid loss 0.06641 train acc 0.07983 valid acc 0.11279\n",
      "epoch  99: train loss 0.06858, valid loss 0.06621 train acc 0.07959 valid acc 0.11279\n",
      "epoch 100: train loss 0.06837, valid loss 0.06602 train acc 0.07959 valid acc 0.11133\n",
      "epoch 101: train loss 0.06817, valid loss 0.06583 train acc 0.07959 valid acc 0.11133\n",
      "epoch 102: train loss 0.06796, valid loss 0.06564 train acc 0.08032 valid acc 0.11133\n",
      "epoch 103: train loss 0.06776, valid loss 0.06546 train acc 0.07935 valid acc 0.11133\n",
      "epoch 104: train loss 0.06757, valid loss 0.06528 train acc 0.07935 valid acc 0.10950\n",
      "epoch 105: train loss 0.06738, valid loss 0.06511 train acc 0.07959 valid acc 0.10938\n",
      "epoch 106: train loss 0.06719, valid loss 0.06494 train acc 0.08008 valid acc 0.10876\n",
      "epoch 107: train loss 0.06700, valid loss 0.06477 train acc 0.08008 valid acc 0.10620\n",
      "epoch 108: train loss 0.06682, valid loss 0.06460 train acc 0.08008 valid acc 0.10608\n",
      "epoch 109: train loss 0.06664, valid loss 0.06444 train acc 0.08008 valid acc 0.10742\n",
      "epoch 110: train loss 0.06647, valid loss 0.06428 train acc 0.07959 valid acc 0.10742\n",
      "epoch 111: train loss 0.06630, valid loss 0.06412 train acc 0.07959 valid acc 0.10742\n",
      "epoch 112: train loss 0.06613, valid loss 0.06397 train acc 0.08008 valid acc 0.10742\n",
      "epoch 113: train loss 0.06596, valid loss 0.06382 train acc 0.07935 valid acc 0.10559\n",
      "epoch 114: train loss 0.06580, valid loss 0.06367 train acc 0.07861 valid acc 0.10547\n",
      "epoch 115: train loss 0.06564, valid loss 0.06352 train acc 0.07886 valid acc 0.10547\n",
      "epoch 116: train loss 0.06549, valid loss 0.06338 train acc 0.07886 valid acc 0.10547\n",
      "epoch 117: train loss 0.06533, valid loss 0.06324 train acc 0.07861 valid acc 0.10547\n",
      "epoch 118: train loss 0.06518, valid loss 0.06310 train acc 0.07837 valid acc 0.10547\n",
      "epoch 119: train loss 0.06503, valid loss 0.06296 train acc 0.07861 valid acc 0.10425\n",
      "epoch 120: train loss 0.06488, valid loss 0.06283 train acc 0.07861 valid acc 0.10352\n",
      "epoch 121: train loss 0.06474, valid loss 0.06270 train acc 0.07886 valid acc 0.10339\n",
      "epoch 122: train loss 0.06460, valid loss 0.06257 train acc 0.07837 valid acc 0.10156\n",
      "epoch 123: train loss 0.06446, valid loss 0.06244 train acc 0.07861 valid acc 0.10156\n",
      "epoch 124: train loss 0.06432, valid loss 0.06232 train acc 0.07837 valid acc 0.10156\n",
      "epoch 125: train loss 0.06419, valid loss 0.06220 train acc 0.07861 valid acc 0.10156\n",
      "epoch 126: train loss 0.06405, valid loss 0.06207 train acc 0.07910 valid acc 0.10156\n",
      "epoch 127: train loss 0.06392, valid loss 0.06196 train acc 0.07886 valid acc 0.10156\n",
      "epoch 128: train loss 0.06380, valid loss 0.06184 train acc 0.07886 valid acc 0.10156\n",
      "epoch 129: train loss 0.06367, valid loss 0.06172 train acc 0.07935 valid acc 0.10071\n",
      "epoch 130: train loss 0.06354, valid loss 0.06161 train acc 0.07910 valid acc 0.09961\n",
      "epoch 131: train loss 0.06342, valid loss 0.06150 train acc 0.07886 valid acc 0.09961\n",
      "epoch 132: train loss 0.06330, valid loss 0.06139 train acc 0.07886 valid acc 0.09961\n",
      "epoch 133: train loss 0.06318, valid loss 0.06128 train acc 0.07861 valid acc 0.09961\n",
      "epoch 134: train loss 0.06306, valid loss 0.06117 train acc 0.07837 valid acc 0.09961\n",
      "epoch 135: train loss 0.06295, valid loss 0.06107 train acc 0.07837 valid acc 0.09790\n",
      "epoch 136: train loss 0.06283, valid loss 0.06097 train acc 0.07837 valid acc 0.09766\n",
      "epoch 137: train loss 0.06272, valid loss 0.06086 train acc 0.07886 valid acc 0.09717\n",
      "epoch 138: train loss 0.06261, valid loss 0.06076 train acc 0.07886 valid acc 0.09570\n",
      "epoch 139: train loss 0.06250, valid loss 0.06066 train acc 0.07935 valid acc 0.09570\n",
      "epoch 140: train loss 0.06239, valid loss 0.06057 train acc 0.07983 valid acc 0.09570\n",
      "epoch 141: train loss 0.06229, valid loss 0.06047 train acc 0.08008 valid acc 0.09717\n",
      "epoch 142: train loss 0.06218, valid loss 0.06038 train acc 0.08008 valid acc 0.09766\n",
      "epoch 143: train loss 0.06208, valid loss 0.06028 train acc 0.08008 valid acc 0.09912\n",
      "epoch 144: train loss 0.06198, valid loss 0.06019 train acc 0.07983 valid acc 0.09961\n",
      "epoch 145: train loss 0.06188, valid loss 0.06010 train acc 0.08032 valid acc 0.09961\n",
      "epoch 146: train loss 0.06178, valid loss 0.06001 train acc 0.08057 valid acc 0.09961\n",
      "epoch 147: train loss 0.06168, valid loss 0.05992 train acc 0.08057 valid acc 0.09961\n",
      "epoch 148: train loss 0.06159, valid loss 0.05983 train acc 0.08057 valid acc 0.09961\n",
      "epoch 149: train loss 0.06149, valid loss 0.05975 train acc 0.08057 valid acc 0.09961\n",
      "epoch 150: train loss 0.06140, valid loss 0.05966 train acc 0.08105 valid acc 0.09961\n",
      "epoch 151: train loss 0.06130, valid loss 0.05958 train acc 0.08105 valid acc 0.09961\n",
      "epoch 152: train loss 0.06121, valid loss 0.05950 train acc 0.08130 valid acc 0.09961\n",
      "epoch 153: train loss 0.06112, valid loss 0.05942 train acc 0.08203 valid acc 0.09961\n",
      "epoch 154: train loss 0.06103, valid loss 0.05934 train acc 0.08228 valid acc 0.09802\n",
      "epoch 155: train loss 0.06095, valid loss 0.05926 train acc 0.08179 valid acc 0.09766\n",
      "epoch 156: train loss 0.06086, valid loss 0.05918 train acc 0.08154 valid acc 0.09888\n",
      "epoch 157: train loss 0.06077, valid loss 0.05910 train acc 0.08179 valid acc 0.09766\n",
      "epoch 158: train loss 0.06069, valid loss 0.05903 train acc 0.08228 valid acc 0.09851\n",
      "epoch 159: train loss 0.06061, valid loss 0.05895 train acc 0.08252 valid acc 0.09961\n",
      "epoch 160: train loss 0.06052, valid loss 0.05888 train acc 0.08252 valid acc 0.09961\n",
      "epoch 161: train loss 0.06044, valid loss 0.05881 train acc 0.08252 valid acc 0.09961\n",
      "epoch 162: train loss 0.06036, valid loss 0.05874 train acc 0.08179 valid acc 0.10132\n",
      "epoch 163: train loss 0.06028, valid loss 0.05867 train acc 0.08154 valid acc 0.10156\n",
      "epoch 164: train loss 0.06021, valid loss 0.05860 train acc 0.08154 valid acc 0.10156\n",
      "epoch 165: train loss 0.06013, valid loss 0.05853 train acc 0.08203 valid acc 0.10156\n",
      "epoch 166: train loss 0.06005, valid loss 0.05846 train acc 0.08228 valid acc 0.10156\n",
      "epoch 167: train loss 0.05998, valid loss 0.05840 train acc 0.08203 valid acc 0.10156\n",
      "epoch 168: train loss 0.05990, valid loss 0.05833 train acc 0.08203 valid acc 0.10303\n",
      "epoch 169: train loss 0.05983, valid loss 0.05826 train acc 0.08179 valid acc 0.10352\n",
      "epoch 170: train loss 0.05976, valid loss 0.05820 train acc 0.08252 valid acc 0.10352\n",
      "epoch 171: train loss 0.05969, valid loss 0.05814 train acc 0.08228 valid acc 0.10352\n",
      "epoch 172: train loss 0.05962, valid loss 0.05807 train acc 0.08228 valid acc 0.10352\n",
      "epoch 173: train loss 0.05955, valid loss 0.05801 train acc 0.08228 valid acc 0.10352\n",
      "epoch 174: train loss 0.05948, valid loss 0.05795 train acc 0.08301 valid acc 0.10486\n",
      "epoch 175: train loss 0.05941, valid loss 0.05789 train acc 0.08325 valid acc 0.10547\n",
      "epoch 176: train loss 0.05935, valid loss 0.05783 train acc 0.08374 valid acc 0.10547\n",
      "epoch 177: train loss 0.05928, valid loss 0.05777 train acc 0.08374 valid acc 0.10547\n",
      "epoch 178: train loss 0.05921, valid loss 0.05772 train acc 0.08350 valid acc 0.10547\n",
      "epoch 179: train loss 0.05915, valid loss 0.05766 train acc 0.08398 valid acc 0.10547\n",
      "epoch 180: train loss 0.05909, valid loss 0.05760 train acc 0.08423 valid acc 0.10547\n",
      "epoch 181: train loss 0.05902, valid loss 0.05755 train acc 0.08423 valid acc 0.10547\n",
      "epoch 182: train loss 0.05896, valid loss 0.05749 train acc 0.08423 valid acc 0.10547\n",
      "epoch 183: train loss 0.05890, valid loss 0.05744 train acc 0.08447 valid acc 0.10547\n",
      "epoch 184: train loss 0.05884, valid loss 0.05738 train acc 0.08423 valid acc 0.10547\n",
      "epoch 185: train loss 0.05878, valid loss 0.05733 train acc 0.08398 valid acc 0.10547\n",
      "epoch 186: train loss 0.05872, valid loss 0.05728 train acc 0.08374 valid acc 0.10547\n",
      "epoch 187: train loss 0.05866, valid loss 0.05723 train acc 0.08350 valid acc 0.10449\n",
      "epoch 188: train loss 0.05860, valid loss 0.05718 train acc 0.08301 valid acc 0.10352\n",
      "epoch 189: train loss 0.05854, valid loss 0.05713 train acc 0.08350 valid acc 0.10535\n",
      "epoch 190: train loss 0.05849, valid loss 0.05708 train acc 0.08325 valid acc 0.10547\n",
      "epoch 191: train loss 0.05843, valid loss 0.05703 train acc 0.08301 valid acc 0.10547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 192: train loss 0.05837, valid loss 0.05698 train acc 0.08325 valid acc 0.10547\n",
      "epoch 193: train loss 0.05832, valid loss 0.05693 train acc 0.08350 valid acc 0.10547\n",
      "epoch 194: train loss 0.05826, valid loss 0.05688 train acc 0.08325 valid acc 0.10645\n",
      "epoch 195: train loss 0.05821, valid loss 0.05684 train acc 0.08325 valid acc 0.10742\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-fb0cc7b8d1e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                  y_train[batch*batch_size:batch*batch_size+batch_size])\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mopt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtrain_epoch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yurong/ntk-env/lib/python3.6/site-packages/jax/api.py\u001b[0m in \u001b[0;36mf_jitted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcache_miss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# probably won't return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcpp_jitted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m   \u001b[0mf_jitted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cpp_jitted_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcpp_jitted_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yurong/ntk-env/lib/python3.6/site-packages/jax/api.py\u001b[0m in \u001b[0;36mcache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_fun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         donated_invars=donated_invars)\n\u001b[0m\u001b[1;32m    285\u001b[0m     \u001b[0mout_pytree_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_pytree_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yurong/ntk-env/lib/python3.6/site-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, fun, *args, **params)\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yurong/ntk-env/lib/python3.6/site-packages/jax/core.py\u001b[0m in \u001b[0;36mcall_bind\u001b[0;34m(primitive, fun, *args, **params)\u001b[0m\n\u001b[1;32m   1218\u001b[0m   \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mmaybe_new_sublevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_todos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_trace_todo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yurong/ntk-env/lib/python3.6/site-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, trace, fun, tracers, params)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1232\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpost_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yurong/ntk-env/lib/python3.6/site-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_call\u001b[0;34m(self, primitive, f, tracers, params)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m   \u001b[0mprocess_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yurong/ntk-env/lib/python3.6/site-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_xla_call_impl\u001b[0;34m(fun, device, backend, name, donated_invars, *args)\u001b[0m\n\u001b[1;32m    570\u001b[0m                                *unsafe_map(arg_spec, args))\n\u001b[1;32m    571\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mFloatingPointError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjax_debug_nans\u001b[0m  \u001b[0;31m# compiled_fun can only raise in this case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yurong/ntk-env/lib/python3.6/site-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_execute_compiled\u001b[0;34m(compiled, avals, handlers, *args)\u001b[0m\n\u001b[1;32m    828\u001b[0m   \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m   \u001b[0minput_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_put\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m   \u001b[0mout_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjax_debug_nans\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcheck_nans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxla_call_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandlers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_partition_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_accuracy = []\n",
    "\n",
    "valid_losses = []\n",
    "valid_accuracy = []\n",
    "\n",
    "valid = (x_valid, y_valid)\n",
    "\n",
    "for i in range(epochs):\n",
    "    train_epoch_losses = []\n",
    "    train_epoch_accuracy = []\n",
    "    \n",
    "    valid_epoch_losses = []\n",
    "    valid_epoch_accuracy = []\n",
    "    \n",
    "    for batch in range(train_size//batch_size):\n",
    "        \n",
    "        train = (x_train[batch*batch_size:batch*batch_size+batch_size], \n",
    "                 y_train[batch*batch_size:batch*batch_size+batch_size])\n",
    "        \n",
    "        opt_state = opt_update(i*(train_size//batch_size) + batch, grad_loss(opt_state, *train), opt_state)\n",
    "        \n",
    "        train_epoch_losses.append(loss(get_params(opt_state), *train))\n",
    "        valid_epoch_losses.append(loss(get_params(opt_state), *valid))\n",
    "        \n",
    "        train_correctness = onp.argmax(apply_fn(get_params(opt_state), train[0]), 1) == onp.argmax(train[1], 1)\n",
    "        train_epoch_accuracy.append(onp.average(train_correctness))\n",
    "        \n",
    "        valid_correctness = onp.argmax(apply_fn(get_params(opt_state), valid[0]), 1) == onp.argmax(valid[1], 1)\n",
    "        valid_epoch_accuracy.append(onp.average(valid_correctness))\n",
    "    \n",
    "    print(\"epoch %3d: train loss %3.5f, valid loss %3.5f train acc %.5f valid acc %.5f\"%\\\n",
    "          (i, onp.average(train_epoch_losses), \n",
    "           onp.average(valid_epoch_losses), \n",
    "           onp.average(train_epoch_accuracy), \n",
    "           onp.average(valid_epoch_accuracy)))\n",
    "    \n",
    "    train_losses.append(onp.average(train_epoch_losses))\n",
    "    train_accuracy.append(onp.average(train_epoch_accuracy))\n",
    "    \n",
    "    valid_losses.append(onp.average(valid_epoch_losses))\n",
    "    valid_accuracy.append(onp.average(valid_epoch_accuracy))\n",
    "    \n",
    "    x_train, y_train = unison_shuffled_copies(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2721655269759087"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onp.sqrt(2/(3*3*3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8560d70400>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(200), train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness = onp.argmax(apply_fn(get_params(opt_state), x_train), 1) == onp.argmax(y_train, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08203125"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onp.average(correctness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def l2_loss_v1(logits, labels, weighting=1):\n",
    "    \"\"\"\n",
    "    Tensorflow version of L2 loss (without sqrt)\n",
    "    \"\"\"\n",
    "    return np.sum(((logits - labels)**2) * weighting) / 2\n",
    "    \n",
    "@jit\n",
    "def l2_loss_v2(logits, lables):\n",
    "    \"\"\"\n",
    "    Normal L2 loss\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(logits - labels)\n",
    "\n",
    "@jit\n",
    "def cross_entropy_loss(logits, lables):\n",
    "    return -np.sum(logsoftmax(logits) * lables)\n",
    "    \n",
    "@jit\n",
    "def mse_loss(logits, lables):\n",
    "    return 0.5 * np.mean((logits - lables) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attack algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, x_train=None, y_train=None, x_test=None, \n",
    "                         y=None, t=None, loss_weighting=None, fx_train_0=0., fx_test_0=0., eps=0.3, \n",
    "                         norm=np.inf, clip_min=None, clip_max=None, targeted=False):\n",
    "    if norm not in [np.inf, 2]:\n",
    "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
    "        \n",
    "    x = x_test\n",
    "        \n",
    "    # test independent\n",
    "    if obj_fn == 'untargeted':\n",
    "        grads = grads_fn(x_train, x_test, y_train, y, kernel_fn, t)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Objective function must be either train(ntk_train_train) or test(predict_fn)\")\n",
    "\n",
    "    axis = list(range(1, len(grads.shape)))\n",
    "    eps_div = 1e-12\n",
    "    \n",
    "    if norm == np.inf:\n",
    "        perturbation = eps * np.sign(grads)\n",
    "    elif norm == 1:\n",
    "        raise NotImplementedError(\"L_1 norm has not been implemented yet.\")\n",
    "    elif norm == 2:\n",
    "        square = np.maximum(eps_div, np.sum(np.square(grads), axis=axis, keepdims=True))\n",
    "        perturbation = grads / np.sqrt(square)\n",
    "    \n",
    "    # TODO\n",
    "    adv_x = x + perturbation\n",
    "    \n",
    "    # If clipping is needed, reset all values outside of [clip_min, clip_max]\n",
    "    if (clip_min is not None) or (clip_max is not None):\n",
    "        # We don't currently support one-sided clipping\n",
    "        assert clip_min is not None and clip_max is not None\n",
    "        adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "    \n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, x_train=None, y_train=None,\n",
    "                               x_test=None, y=None, t=None, loss_weighting=None, fx_train_0=0., fx_test_0=0., \n",
    "                               eps=0.3, eps_iter=0.03, nb_iter=10, norm=np.inf, clip_min=None, clip_max=None, \n",
    "                               targeted=False, rand_init=None, rand_minmax=0.3):\n",
    "    assert eps_iter <= eps, (eps_iter, eps)\n",
    "    if norm == 1:\n",
    "        raise NotImplementedError(\"It's not clear that FGM is a good inner loop\"\n",
    "                                  \" step for PGD when norm=1, because norm=1 FGM \"\n",
    "                                  \" changes only one pixel at a time. We need \"\n",
    "                                  \" to rigorously test a strong norm=1 PGD \"\n",
    "                                  \"before enabling this feature.\")\n",
    "    if norm not in [np.inf, 2]:\n",
    "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
    "        \n",
    "    x = x_test\n",
    "    \n",
    "    # Initialize loop variables\n",
    "    if rand_init:\n",
    "        rand_minmax = eps\n",
    "        eta = random.uniform(new_key, x.shape, minval=-rand_minmax, maxval=rand_minmax)\n",
    "    else:\n",
    "        eta = np.zeros_like(x)\n",
    "\n",
    "    # Clip eta\n",
    "    eta = clip_eta(eta, norm, eps)\n",
    "    adv_x = x + eta\n",
    "    if clip_min is not None or clip_max is not None:\n",
    "        adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "        \n",
    "    for i in range(nb_iter):\n",
    "        adv_x = fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, x_train, y_train, adv_x, \n",
    "                                        y, t, loss_weighting, fx_train_0, fx_test_0, eps_iter, norm, \n",
    "                                        clip_min, clip_max, targeted)\n",
    "\n",
    "        # Clipping perturbation eta to norm norm ball\n",
    "        eta = adv_x - x\n",
    "        eta = clip_eta(eta, norm, eps)\n",
    "        adv_x = x + eta\n",
    "\n",
    "        # Redo the clipping.\n",
    "        # FGM already did it, but subtracting and re-adding eta can add some\n",
    "        # small numerical error.\n",
    "        if clip_min is not None or clip_max is not None:\n",
    "            adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "    \n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'mnist':\n",
    "    eps = 0.3\n",
    "    eps_iter_10 = (eps/10)*1.1\n",
    "    eps_iter_100 = (eps/100)*1.1\n",
    "    eps_iter_1000 = (eps/1000)*1.1\n",
    "    \n",
    "elif DATASET == 'cifar10':\n",
    "    eps = 0.03\n",
    "    eps_iter_10 = (eps/10)*1.1\n",
    "    eps_iter_100 = (eps/100)*1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(x_train, x_test, y_test, model_fn, kernel_fn, t=None, attack_type=None, ntk_train_train=None):\n",
    "    \n",
    "    y_train_predict, y_test_predict = model_fn(kernel_fn, x_train, x_test, \n",
    "                                               t=t, ntk_train_train=ntk_train_train)\n",
    "    \n",
    "    selected_table = correct(y_test_predict, y_test)\n",
    "    print(\"Accuray({:s}): {:.2f}\".format(attack_type, onp.mean(selected_table)))\n",
    "    \n",
    "    return selected_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_robustness(x_train, x_test, y_test, model_fn, kernel_fn, selected_table, t=None, \n",
    "                        attack_type=None, ntk_train_train=None):\n",
    "    \n",
    "    y_train_predict, y_test_predict = model_fn(kernel_fn, x_train, x_test,\n",
    "                                               t=t, ntk_train_train=ntk_train_train)\n",
    "    \n",
    "    y_test_predict = onp.asarray(y_test_predict)\n",
    "    y_test_predict_select = y_test_predict[onp.asarray(selected_table)]\n",
    "    y_test_select = y_test[onp.asarray(selected_table)]\n",
    "    print(\"Robustness({:s}): {:.2f}\".format(attack_type, onp.mean(correct(y_test_predict_select, y_test_select))))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adv_x generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntk-env",
   "language": "python",
   "name": "ntk-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
