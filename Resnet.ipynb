{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "DATASET = 'cifar10'\n",
    "class_num   = 10\n",
    "test_size   = 2048\n",
    "train_size  = 4096\n",
    "valid_size  = 256\n",
    "image_shape = None\n",
    "\n",
    "if DATASET =='mnist':\n",
    "    image_shape = (28, 28, 1)\n",
    "elif DATASET == 'cifar10':\n",
    "    image_shape = (32, 32, 3)\n",
    "\n",
    "#training\n",
    "run = 10\n",
    "batch_size = 256\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    }
   ],
   "source": [
    "x_train_all, y_train_all, x_test_all, y_test_all = tuple(onp.array(x) for x in get_dataset(DATASET, None, None, \n",
    "                                                                                  do_flatten_and_normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "seed = 0\n",
    "x_train_all, y_train_all = shaffle(x_train_all, y_train_all, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# down sample\n",
    "x_train = x_train_all[:train_size]\n",
    "y_train = y_train_all[:train_size]\n",
    "\n",
    "x_valid = x_train_all[train_size:train_size+valid_size]\n",
    "y_valid = y_train_all[train_size:train_size+valid_size]\n",
    "\n",
    "x_test = x_test_all[:test_size]\n",
    "y_test = y_test_all[:test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, x_test = x_train.reshape((-1, *image_shape)), x_valid.reshape((-1, *image_shape)) ,x_test.reshape((-1, *image_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.applications.ResNet50(include_top=True, weights=None, input_shape=(32, 32, 3), classes=class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 10)           20490       avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 23,608,202\n",
      "Trainable params: 23,555,082\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 2.2509 - accuracy: 0.2102 - val_loss: 2.3593 - val_accuracy: 0.1016\n",
      "Epoch 2/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2355 - accuracy: 0.2253 - val_loss: 2.2853 - val_accuracy: 0.1758\n",
      "Epoch 3/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2382 - accuracy: 0.2227 - val_loss: 2.3281 - val_accuracy: 0.1328\n",
      "Epoch 4/120\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 2.2566 - accuracy: 0.2043 - val_loss: 2.2985 - val_accuracy: 0.1641\n",
      "Epoch 5/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2319 - accuracy: 0.2292 - val_loss: 2.2435 - val_accuracy: 0.2188\n",
      "Epoch 6/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2241 - accuracy: 0.2371 - val_loss: 2.2735 - val_accuracy: 0.1875\n",
      "Epoch 7/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2316 - accuracy: 0.2292 - val_loss: 2.2386 - val_accuracy: 0.2227\n",
      "Epoch 8/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2226 - accuracy: 0.2385 - val_loss: 2.3088 - val_accuracy: 0.1523\n",
      "Epoch 9/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2368 - accuracy: 0.2241 - val_loss: 2.3166 - val_accuracy: 0.1445\n",
      "Epoch 10/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2374 - accuracy: 0.2234 - val_loss: 2.3244 - val_accuracy: 0.1367\n",
      "Epoch 11/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2435 - accuracy: 0.2178 - val_loss: 2.3165 - val_accuracy: 0.1445\n",
      "Epoch 12/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2627 - accuracy: 0.1978 - val_loss: 2.3268 - val_accuracy: 0.1328\n",
      "Epoch 13/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2634 - accuracy: 0.1973 - val_loss: 2.2851 - val_accuracy: 0.1758\n",
      "Epoch 14/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2561 - accuracy: 0.2048 - val_loss: 2.2539 - val_accuracy: 0.2070\n",
      "Epoch 15/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2477 - accuracy: 0.2134 - val_loss: 2.2539 - val_accuracy: 0.2070\n",
      "Epoch 16/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2437 - accuracy: 0.2173 - val_loss: 2.2424 - val_accuracy: 0.2188\n",
      "Epoch 17/120\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 2.2442 - accuracy: 0.2166 - val_loss: 2.2462 - val_accuracy: 0.2148\n",
      "Epoch 18/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2441 - accuracy: 0.2168 - val_loss: 2.2736 - val_accuracy: 0.1875\n",
      "Epoch 19/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2498 - accuracy: 0.2107 - val_loss: 2.2813 - val_accuracy: 0.1797\n",
      "Epoch 20/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2428 - accuracy: 0.2183 - val_loss: 2.2815 - val_accuracy: 0.1797\n",
      "Epoch 21/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2467 - accuracy: 0.2144 - val_loss: 2.3125 - val_accuracy: 0.1484\n",
      "Epoch 22/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2509 - accuracy: 0.2102 - val_loss: 2.2970 - val_accuracy: 0.1641\n",
      "Epoch 23/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2563 - accuracy: 0.2048 - val_loss: 2.3010 - val_accuracy: 0.1602\n",
      "Epoch 24/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2530 - accuracy: 0.2080 - val_loss: 2.2971 - val_accuracy: 0.1641\n",
      "Epoch 25/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2549 - accuracy: 0.2063 - val_loss: 2.3303 - val_accuracy: 0.1250\n",
      "Epoch 26/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2695 - accuracy: 0.1917 - val_loss: 2.3072 - val_accuracy: 0.1523\n",
      "Epoch 27/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2470 - accuracy: 0.2141 - val_loss: 2.3008 - val_accuracy: 0.1602\n",
      "Epoch 28/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2384 - accuracy: 0.2224 - val_loss: 2.2657 - val_accuracy: 0.1953\n",
      "Epoch 29/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2347 - accuracy: 0.2263 - val_loss: 2.2889 - val_accuracy: 0.1719\n",
      "Epoch 30/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2284 - accuracy: 0.2324 - val_loss: 2.2585 - val_accuracy: 0.2031\n",
      "Epoch 31/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2320 - accuracy: 0.2290 - val_loss: 2.2656 - val_accuracy: 0.1953\n",
      "Epoch 32/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2402 - accuracy: 0.2209 - val_loss: 2.2682 - val_accuracy: 0.1953\n",
      "Epoch 33/120\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 2.2466 - accuracy: 0.2144 - val_loss: 2.3285 - val_accuracy: 0.1328\n",
      "Epoch 34/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2816 - accuracy: 0.1794 - val_loss: 2.3281 - val_accuracy: 0.1328\n",
      "Epoch 35/120\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 2.2630 - accuracy: 0.1982 - val_loss: 2.3205 - val_accuracy: 0.1406\n",
      "Epoch 36/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2536 - accuracy: 0.2075 - val_loss: 2.3360 - val_accuracy: 0.1250\n",
      "Epoch 37/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2489 - accuracy: 0.2119 - val_loss: 2.3167 - val_accuracy: 0.1445\n",
      "Epoch 38/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2341 - accuracy: 0.2271 - val_loss: 2.3088 - val_accuracy: 0.1523\n",
      "Epoch 39/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2332 - accuracy: 0.2278 - val_loss: 2.2815 - val_accuracy: 0.1797\n",
      "Epoch 40/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2417 - accuracy: 0.2195 - val_loss: 2.2696 - val_accuracy: 0.1914\n",
      "Epoch 41/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2344 - accuracy: 0.2266 - val_loss: 2.2461 - val_accuracy: 0.2148\n",
      "Epoch 42/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2357 - accuracy: 0.2253 - val_loss: 2.2570 - val_accuracy: 0.2031\n",
      "Epoch 43/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2396 - accuracy: 0.2214 - val_loss: 2.2775 - val_accuracy: 0.1836\n",
      "Epoch 44/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2480 - accuracy: 0.2131 - val_loss: 2.2619 - val_accuracy: 0.1992\n",
      "Epoch 45/120\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 2.2534 - accuracy: 0.2075 - val_loss: 2.2594 - val_accuracy: 0.2031\n",
      "Epoch 46/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2420 - accuracy: 0.2190 - val_loss: 2.2385 - val_accuracy: 0.2227\n",
      "Epoch 47/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2394 - accuracy: 0.2212 - val_loss: 2.2618 - val_accuracy: 0.1992\n",
      "Epoch 48/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2609 - accuracy: 0.2000 - val_loss: 2.2546 - val_accuracy: 0.2070\n",
      "Epoch 49/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2487 - accuracy: 0.2124 - val_loss: 2.2383 - val_accuracy: 0.2227\n",
      "Epoch 50/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2391 - accuracy: 0.2219 - val_loss: 2.3202 - val_accuracy: 0.1406\n",
      "Epoch 51/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2266 - accuracy: 0.2341 - val_loss: 2.2424 - val_accuracy: 0.2188\n",
      "Epoch 52/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2266 - accuracy: 0.2346 - val_loss: 2.2542 - val_accuracy: 0.2070\n",
      "Epoch 53/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2264 - accuracy: 0.2349 - val_loss: 2.2385 - val_accuracy: 0.2227\n",
      "Epoch 54/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2286 - accuracy: 0.2324 - val_loss: 2.3049 - val_accuracy: 0.1562\n",
      "Epoch 55/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2353 - accuracy: 0.2256 - val_loss: 2.3048 - val_accuracy: 0.1562\n",
      "Epoch 56/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2374 - accuracy: 0.2236 - val_loss: 2.3361 - val_accuracy: 0.1250\n",
      "Epoch 57/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2309 - accuracy: 0.2300 - val_loss: 2.2702 - val_accuracy: 0.1914\n",
      "Epoch 58/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2249 - accuracy: 0.2363 - val_loss: 2.2621 - val_accuracy: 0.1992\n",
      "Epoch 59/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2302 - accuracy: 0.2310 - val_loss: 2.2471 - val_accuracy: 0.2148\n",
      "Epoch 60/120\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 2.2327 - accuracy: 0.2280 - val_loss: 2.2780 - val_accuracy: 0.1836\n",
      "Epoch 61/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2297 - accuracy: 0.2314 - val_loss: 2.2735 - val_accuracy: 0.1875\n",
      "Epoch 62/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2299 - accuracy: 0.2312 - val_loss: 2.2897 - val_accuracy: 0.1719\n",
      "Epoch 63/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2423 - accuracy: 0.2185 - val_loss: 2.3088 - val_accuracy: 0.1523\n",
      "Epoch 64/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2346 - accuracy: 0.2266 - val_loss: 2.3007 - val_accuracy: 0.1602\n",
      "Epoch 65/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2275 - accuracy: 0.2334 - val_loss: 2.2619 - val_accuracy: 0.1992\n",
      "Epoch 66/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2302 - accuracy: 0.2307 - val_loss: 2.2514 - val_accuracy: 0.2109\n",
      "Epoch 67/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2256 - accuracy: 0.2354 - val_loss: 2.2971 - val_accuracy: 0.1641\n",
      "Epoch 68/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2296 - accuracy: 0.2314 - val_loss: 2.2658 - val_accuracy: 0.1953\n",
      "Epoch 69/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2191 - accuracy: 0.2422 - val_loss: 2.2541 - val_accuracy: 0.2070\n",
      "Epoch 70/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2173 - accuracy: 0.2439 - val_loss: 2.2832 - val_accuracy: 0.1758\n",
      "Epoch 71/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2312 - accuracy: 0.2297 - val_loss: 2.2658 - val_accuracy: 0.1953\n",
      "Epoch 72/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2238 - accuracy: 0.2373 - val_loss: 2.2462 - val_accuracy: 0.2148\n",
      "Epoch 73/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2168 - accuracy: 0.2441 - val_loss: 2.2502 - val_accuracy: 0.2109\n",
      "Epoch 74/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2270 - accuracy: 0.2341 - val_loss: 2.2502 - val_accuracy: 0.2109\n",
      "Epoch 75/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2197 - accuracy: 0.2417 - val_loss: 2.2502 - val_accuracy: 0.2109\n",
      "Epoch 76/120\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 2.2286 - accuracy: 0.2324 - val_loss: 2.3440 - val_accuracy: 0.1172\n",
      "Epoch 77/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2390 - accuracy: 0.2222 - val_loss: 2.3336 - val_accuracy: 0.1289\n",
      "Epoch 78/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2367 - accuracy: 0.2241 - val_loss: 2.3322 - val_accuracy: 0.1289\n",
      "Epoch 79/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2460 - accuracy: 0.2153 - val_loss: 2.3542 - val_accuracy: 0.1055\n",
      "Epoch 80/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2248 - accuracy: 0.2363 - val_loss: 2.3088 - val_accuracy: 0.1523\n",
      "Epoch 81/120\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 2.2310 - accuracy: 0.2302 - val_loss: 2.2657 - val_accuracy: 0.1953\n",
      "Epoch 82/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2358 - accuracy: 0.2251 - val_loss: 2.2762 - val_accuracy: 0.1836\n",
      "Epoch 83/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2319 - accuracy: 0.2290 - val_loss: 2.2541 - val_accuracy: 0.2070\n",
      "Epoch 84/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2175 - accuracy: 0.2432 - val_loss: 2.2267 - val_accuracy: 0.2344\n",
      "Epoch 85/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2148 - accuracy: 0.2461 - val_loss: 2.2465 - val_accuracy: 0.2148\n",
      "Epoch 86/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2337 - accuracy: 0.2273 - val_loss: 2.2539 - val_accuracy: 0.2070\n",
      "Epoch 87/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2404 - accuracy: 0.2205 - val_loss: 2.3049 - val_accuracy: 0.1562\n",
      "Epoch 88/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2345 - accuracy: 0.2263 - val_loss: 2.3123 - val_accuracy: 0.1484\n",
      "Epoch 89/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2254 - accuracy: 0.2354 - val_loss: 2.3127 - val_accuracy: 0.1484\n",
      "Epoch 90/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2405 - accuracy: 0.2205 - val_loss: 2.2587 - val_accuracy: 0.1992\n",
      "Epoch 91/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2463 - accuracy: 0.2148 - val_loss: 2.3007 - val_accuracy: 0.1602\n",
      "Epoch 92/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2330 - accuracy: 0.2280 - val_loss: 2.2701 - val_accuracy: 0.1914\n",
      "Epoch 93/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2292 - accuracy: 0.2319 - val_loss: 2.2657 - val_accuracy: 0.1953\n",
      "Epoch 94/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2336 - accuracy: 0.2275 - val_loss: 2.2658 - val_accuracy: 0.1953\n",
      "Epoch 95/120\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 2.2294 - accuracy: 0.2314 - val_loss: 2.2736 - val_accuracy: 0.1875\n",
      "Epoch 96/120\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 2.2287 - accuracy: 0.2324 - val_loss: 2.2623 - val_accuracy: 0.1992\n",
      "Epoch 97/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2408 - accuracy: 0.2202 - val_loss: 2.2697 - val_accuracy: 0.1914\n",
      "Epoch 98/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2360 - accuracy: 0.2251 - val_loss: 2.2602 - val_accuracy: 0.1992\n",
      "Epoch 99/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2345 - accuracy: 0.2266 - val_loss: 2.3283 - val_accuracy: 0.1328\n",
      "Epoch 100/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2381 - accuracy: 0.2231 - val_loss: 2.2971 - val_accuracy: 0.1641\n",
      "Epoch 101/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2273 - accuracy: 0.2336 - val_loss: 2.3010 - val_accuracy: 0.1602\n",
      "Epoch 102/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2224 - accuracy: 0.2390 - val_loss: 2.2854 - val_accuracy: 0.1758\n",
      "Epoch 103/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2252 - accuracy: 0.2358 - val_loss: 2.2619 - val_accuracy: 0.1992\n",
      "Epoch 104/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2336 - accuracy: 0.2275 - val_loss: 2.2854 - val_accuracy: 0.1758\n",
      "Epoch 105/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2151 - accuracy: 0.2461 - val_loss: 2.3322 - val_accuracy: 0.1289\n",
      "Epoch 106/120\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 2.2192 - accuracy: 0.2417 - val_loss: 2.3362 - val_accuracy: 0.1250\n",
      "Epoch 107/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2404 - accuracy: 0.2207 - val_loss: 2.3322 - val_accuracy: 0.1289\n",
      "Epoch 108/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2434 - accuracy: 0.2175 - val_loss: 2.3322 - val_accuracy: 0.1289\n",
      "Epoch 109/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2298 - accuracy: 0.2312 - val_loss: 2.3127 - val_accuracy: 0.1484\n",
      "Epoch 110/120\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 2.2255 - accuracy: 0.2356 - val_loss: 2.3086 - val_accuracy: 0.1523\n",
      "Epoch 111/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2190 - accuracy: 0.2422 - val_loss: 2.2541 - val_accuracy: 0.2070\n",
      "Epoch 112/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2168 - accuracy: 0.2441 - val_loss: 2.2764 - val_accuracy: 0.1836\n",
      "Epoch 113/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2170 - accuracy: 0.2439 - val_loss: 2.2541 - val_accuracy: 0.2070\n",
      "Epoch 114/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2178 - accuracy: 0.2432 - val_loss: 2.2262 - val_accuracy: 0.2344\n",
      "Epoch 115/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 59ms/step - loss: 2.2193 - accuracy: 0.2419 - val_loss: 2.2970 - val_accuracy: 0.1641\n",
      "Epoch 116/120\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 2.2231 - accuracy: 0.2380 - val_loss: 2.2814 - val_accuracy: 0.1797\n",
      "Epoch 117/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2235 - accuracy: 0.2375 - val_loss: 2.2736 - val_accuracy: 0.1875\n",
      "Epoch 118/120\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 2.2321 - accuracy: 0.2288 - val_loss: 2.2463 - val_accuracy: 0.2148\n",
      "Epoch 119/120\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 2.2483 - accuracy: 0.2129 - val_loss: 2.2930 - val_accuracy: 0.1680\n",
      "Epoch 120/120\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 2.2542 - accuracy: 0.2068 - val_loss: 2.2658 - val_accuracy: 0.1953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe82c4d3780>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train,\n",
    "          y=y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=120,\n",
    "          callbacks=None,\n",
    "          validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========NTK============\n",
      "4/4 [==============================] - 1s 355ms/step - loss: 2.2885 - accuracy: 0.1719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.2885451316833496, 0.171875]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = onp.load('./npy/cifar-eps-time-any-npy/cifar-fgsm-eps-0.03-time-4096.npy')\n",
    "print(\"==========NTK============\")\n",
    "model.evaluate(tmp, y_test[:128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========CE============\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 2.2887 - accuracy: 0.1719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.2887356281280518, 0.171875]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = onp.load('./npy/cifar-untargeted-cifar-nn-grey-box-train=4096-ce.npy')\n",
    "print(\"==========CE============\")\n",
    "model.evaluate(tmp[:128], y_test[:128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========MSE============\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.2887 - accuracy: 0.1719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.288715362548828, 0.171875]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = onp.load('./npy/cifar-untargeted-cifar-nn-grey-box-train=4096-mse.npy')\n",
    "print(\"==========MSE============\")\n",
    "model.evaluate(tmp[:128], y_test[:128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 8ms/step - loss: 2.2806 - accuracy: 0.1797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.2806341648101807, 0.1796875]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = onp.load('./npy/cifar-fgsm-eps-0.03-time-None-nngp.npy')\n",
    "model.evaluate(tmp, y_test[:128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 7ms/step - loss: 2.2736 - accuracy: 0.1875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.2735934257507324, 0.1875]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test[:128], y_test[:128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
