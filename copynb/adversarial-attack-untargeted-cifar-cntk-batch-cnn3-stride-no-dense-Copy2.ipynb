{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "part = 1\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(part)\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.0\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "\n",
    "import numpy as onp\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "\n",
    "from jax import lax, random\n",
    "from jax.api import grad, jit, vmap\n",
    "from jax.config import config\n",
    "from jax.experimental import optimizers\n",
    "from jax.experimental.stax import logsoftmax\n",
    "\n",
    "config.update('jax_enable_x64', True)\n",
    "\n",
    "from neural_tangents import stax\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# Attacking\n",
    "from cleverhans.utils import clip_eta, one_hot\n",
    "\n",
    "# Plotting\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from utils import *\n",
    "\n",
    "sns.set_style(style='white')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "\"\"\"\n",
    "diag_reg:\n",
    "    a scalar representing the strength of the diagonal regularization for\n",
    "    `k_train_train`, i.e. computing `k_train_train + diag_reg * I` during\n",
    "    Cholesky factorization or eigendecomposition.\n",
    "\"\"\"\n",
    "diag_reg = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = [1e3, 5e3, 1e4, 2e4, 4e4, 8e4, 16e4, None]\n",
    "time = [time[part]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'cifar10'\n",
    "class_num   = 10\n",
    "image_shape = None\n",
    "\n",
    "train_size = None\n",
    "test_size = 512\n",
    "test_batch_size = 1\n",
    "eps = 0.03\n",
    "\n",
    "if DATASET =='mnist':\n",
    "    image_shape = (28, 28, 1)\n",
    "elif DATASET == 'cifar10':\n",
    "    image_shape = (32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_all, y_train_all, x_test_all, y_test_all = tuple(onp.array(x) for x in get_dataset(DATASET, None, None, \n",
    "                                                                                  do_flatten_and_normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "seed = 0\n",
    "x_train_all, y_train_all = shaffle(x_train_all, y_train_all, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# down sample\n",
    "x_train = x_train_all[:train_size]\n",
    "y_train = y_train_all[:train_size]\n",
    "\n",
    "x_test = x_test_all[:test_size]\n",
    "y_test = y_test_all[:test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train.reshape((-1, *image_shape)), x_test.reshape((-1, *image_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(mean, ys):\n",
    "    return onp.argmax(mean, axis=-1) == onp.argmax(ys, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvBlock(channels, W_std, b_std, strides=(1,1)):\n",
    "    return stax.serial(stax.Conv(out_chan=channels, filter_shape=(3,3), strides=strides, padding='SAME',\n",
    "                                 W_std=W_std, b_std=b_std), \n",
    "                       stax.Relu(do_backprop=True))\n",
    "\n",
    "def ConvGroup(n, channels, stride, W_std, b_std, last_stride=False):\n",
    "    blocks = []\n",
    "    if last_stride:\n",
    "        for i in range(n-1):\n",
    "            blocks += [ConvBlock(channels, W_std, b_std, stride)]\n",
    "        blocks += [ConvBlock(channels, W_std, b_std, (2, 2))]\n",
    "    \n",
    "    else:\n",
    "        for i in range(n):\n",
    "            blocks += [ConvBlock(channels, W_std, b_std, stride)]\n",
    "        \n",
    "    return stax.serial(*blocks)\n",
    "        \n",
    "def VGG19(class_num=class_num):\n",
    "    return stax.serial(\n",
    "        ConvGroup(n=2, channels=64 , stride=(1,1), W_std=1.414, b_std=0.18, last_stride=False),\n",
    "        ConvGroup(n=2, channels=128, stride=(1,1), W_std=1.414, b_std=0.18, last_stride=False),\n",
    "        ConvGroup(n=4, channels=256, stride=(1,1), W_std=1.414, b_std=0.18, last_stride=False),\n",
    "        ConvGroup(n=4, channels=512, stride=(1,1), W_std=1.414, b_std=0.18, last_stride=False),\n",
    "        ConvGroup(n=4, channels=512, stride=(1,1), W_std=1.414, b_std=0.18, last_stride=False),\n",
    "        stax.Flatten())\n",
    "\n",
    "def simple_net(class_num=class_num):\n",
    "    return stax.serial(\n",
    "        ConvGroup(n=3, channels=64 , stride=(1 ,1), W_std=1.414, b_std=0.18, last_stride=True),\n",
    "        stax.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_fn, apply_fn, kernel_fn = simple_net(class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_kernel_fn = nt.batch(kernel_fn, batch_size=256, store_on_device=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel_train_m = batch_kernel_fn(x_train[:2048], None, 'ntk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict_fn = nt.predict.gradient_descent_mse(kernel_train_m, y_train[:2048], diag_reg=diag_reg)\n",
    "# kernel_test_train = batch_kernel_fn(x_test[:256], x_train[:2048], 'ntk')\n",
    "# pred = predict_fn(None, 0, 0 , kernel_test_train)\n",
    "# ans = onp.argmax(pred[1], axis=1)==onp.argmax(y_test[:256], axis=1)\n",
    "\n",
    "# print(\"testing accuracy: %.4f\"%(sum(ans)/ans.shape[0]))\n",
    "\n",
    "# plt.figure(dpi=100)\n",
    "# plt.imshow(kernel_train_m[:128, :128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(kernel_fn, x_train=None, x_test=None, fx_train_0=0., fx_test_0=0., t=None, ntk_train_train=None):\n",
    "    # Kernel\n",
    "    if ntk_train_train is None:\n",
    "        ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "    \n",
    "    ntk_test_train = kernel_fn(x_test, x_train, 'ntk')\n",
    "    # Prediction\n",
    "    predict_fn = nt.predict.gradient_descent_mse(ntk_train_train, y_train, diag_reg=diag_reg) # no convariance\n",
    "    \n",
    "    return predict_fn(t, fx_train_0, fx_test_0, ntk_test_train) # fx_train_0, fx_test_0 = (0, 0) for infinite width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def l2_loss_v1(logits, labels, weighting=1):\n",
    "    \"\"\"\n",
    "    Tensorflow version of L2 loss (without sqrt)\n",
    "    \"\"\"\n",
    "    return np.sum(((logits - labels)**2) * weighting) / 2\n",
    "    \n",
    "@jit\n",
    "def l2_loss_v2(logits, lables):\n",
    "    \"\"\"\n",
    "    Normal L2 loss\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(logits - labels)\n",
    "\n",
    "@jit\n",
    "def cross_entropy_loss(logits, lables):\n",
    "    return -np.sum(logsoftmax(logits) * lables)\n",
    "    \n",
    "@jit\n",
    "def mse_loss(logits, lables):\n",
    "    return 0.5 * np.mean((logits - lables) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attack algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, x_train=None, y_train=None, x_test=None, \n",
    "                         y=None, t=None, loss_weighting=None, fx_train_0=0., fx_test_0=0., eps=0.3, \n",
    "                         norm=np.inf, clip_min=None, clip_max=None, targeted=False):\n",
    "    \n",
    "    if norm not in [np.inf, 2]:\n",
    "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
    "        \n",
    "    x = x_test\n",
    "        \n",
    "    # test independent\n",
    "    if obj_fn == 'untargeted':\n",
    "        grads = grads_fn(x_train, x_test, y_train, y, kernel_fn, t)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Objective function must be either train(ntk_train_train) or test(predict_fn)\")\n",
    "\n",
    "    axis = list(range(1, len(grads.shape)))\n",
    "    eps_div = 1e-12\n",
    "    \n",
    "    if norm == np.inf:\n",
    "        perturbation = eps * np.sign(grads)\n",
    "    elif norm == 1:\n",
    "        raise NotImplementedError(\"L_1 norm has not been implemented yet.\")\n",
    "    elif norm == 2:\n",
    "        square = np.maximum(eps_div, np.sum(np.square(grads), axis=axis, keepdims=True))\n",
    "        perturbation = grads / np.sqrt(square)\n",
    "    \n",
    "    adv_x = x + perturbation\n",
    "    \n",
    "    # If clipping is needed, reset all values outside of [clip_min, clip_max]\n",
    "    if (clip_min is not None) or (clip_max is not None):\n",
    "        # We don't currently support one-sided clipping\n",
    "        assert clip_min is not None and clip_max is not None\n",
    "        adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "    \n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_gradient_method_batch(model_fn, kernel_fn, obj_fn, grads_fn, ntk_train_train, x_train=None, y_train=None, \n",
    "                               x_test=None, y=None, t=None, loss_weighting=None, fx_train_0=0., fx_test_0=0., eps=0.3, \n",
    "                               norm=np.inf, clip_min=None, clip_max=None, targeted=False):\n",
    "    \n",
    "    if norm not in [np.inf, 2]:\n",
    "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
    "        \n",
    "    x = x_test\n",
    "        \n",
    "    # test independent\n",
    "    if obj_fn == 'untargeted':\n",
    "        grads = grads_fn(x_train, x_test, y_train, y, kernel_fn, ntk_train_train, t)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Objective function must be either train(ntk_train_train) or test(predict_fn)\")\n",
    "\n",
    "    axis = list(range(1, len(grads.shape)))\n",
    "    eps_div = 1e-12\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, x_train=None, y_train=None,\n",
    "                               x_test=None, y=None, t=None, loss_weighting=None, fx_train_0=0., fx_test_0=0., \n",
    "                               eps=0.3, eps_iter=0.03, nb_iter=10, norm=np.inf, clip_min=None, clip_max=None, \n",
    "                               targeted=False, rand_init=None, rand_minmax=0.3):\n",
    "\n",
    "    assert eps_iter <= eps, (eps_iter, eps)\n",
    "    if norm == 1:\n",
    "        raise NotImplementedError(\"It's not clear that FGM is a good inner loop\"\n",
    "                                  \" step for PGD when norm=1, because norm=1 FGM \"\n",
    "                                  \" changes only one pixel at a time. We need \"\n",
    "                                  \" to rigorously test a strong norm=1 PGD \"\n",
    "                                  \"before enabling this feature.\")\n",
    "    if norm not in [np.inf, 2]:\n",
    "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
    "        \n",
    "    x = x_test\n",
    "    \n",
    "    # Initialize loop variables\n",
    "    if rand_init:\n",
    "        rand_minmax = eps\n",
    "        eta = random.uniform(new_key, x.shape, minval=-rand_minmax, maxval=rand_minmax)\n",
    "    else:\n",
    "        eta = np.zeros_like(x)\n",
    "\n",
    "    # Clip eta\n",
    "    eta = clip_eta(eta, norm, eps)\n",
    "    adv_x = x + eta\n",
    "    if clip_min is not None or clip_max is not None:\n",
    "        adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "        \n",
    "    for i in range(nb_iter):\n",
    "        adv_x = fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, x_train, y_train, adv_x, \n",
    "                                        y, t, loss_weighting, fx_train_0, fx_test_0, eps_iter, norm, \n",
    "                                        clip_min, clip_max, targeted)\n",
    "\n",
    "        # Clipping perturbation eta to norm norm ball\n",
    "        eta = adv_x - x\n",
    "        eta = clip_eta(eta, norm, eps)\n",
    "        adv_x = x + eta\n",
    "\n",
    "        # Redo the clipping.\n",
    "        # FGM already did it, but subtracting and re-adding eta can add some\n",
    "        # small numerical error.\n",
    "        if clip_min is not None or clip_max is not None:\n",
    "            adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "    \n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'mnist':\n",
    "    eps = 0.3\n",
    "    eps_iter_10 = (eps/10)*1.1\n",
    "    eps_iter_100 = (eps/100)*1.1\n",
    "    eps_iter_1000 = (eps/1000)*1.1\n",
    "    \n",
    "elif DATASET == 'cifar10':\n",
    "    eps = 0.03\n",
    "    eps_iter_10 = (eps/10)*1.1\n",
    "    eps_iter_100 = (eps/100)*1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(x_train, x_test, y_test, model_fn, kernel_fn, t=None, attack_type=None, ntk_train_train=None):\n",
    "    \n",
    "    y_train_predict, y_test_predict = model_fn(kernel_fn, x_train, x_test, \n",
    "                                               t=t, ntk_train_train=ntk_train_train)\n",
    "    \n",
    "    selected_table = correct(y_test_predict, y_test)\n",
    "    print(\"Accuray({:s}): {:.2f}\".format(attack_type, onp.mean(selected_table)))\n",
    "    \n",
    "    return selected_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_robustness(x_train, x_test, y_test, model_fn, kernel_fn, selected_table, t=None, \n",
    "                        attack_type=None, ntk_train_train=None):\n",
    "    \n",
    "    y_train_predict, y_test_predict = model_fn(kernel_fn, x_train, x_test,\n",
    "                                               t=t, ntk_train_train=ntk_train_train)\n",
    "    \n",
    "    y_test_predict = onp.asarray(y_test_predict)\n",
    "    y_test_predict_select = y_test_predict[onp.asarray(selected_table)]\n",
    "    y_test_select = y_test[onp.asarray(selected_table)]\n",
    "    print(\"Robustness({:s}): {:.2f}\".format(attack_type, onp.mean(correct(y_test_predict_select, y_test_select))))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adv_x generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv(k):\n",
    "        #inverse with diag_reg\n",
    "        return onp.linalg.inv(k + diag_reg * onp.eye(k.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel_train_inv_m = inv(kernel_train_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss_adv_mse(x_train, x_test, y_train, y, kernel_fn, ntk_train_train, t=None, diag_reg=diag_reg):\n",
    "    \n",
    "    ntk_test_train = kernel_fn(x_test[None], x_train, 'ntk')\n",
    "\n",
    "    predict_fn = nt.predict.gradient_descent_mse(ntk_train_train, y_train, diag_reg=diag_reg)\n",
    "    # predict_fn(t, train_0, test_0, kernel_matrix)\n",
    "    pred = predict_fn(t, 0., 0., ntk_test_train)[1]\n",
    "\n",
    "    # loss = -mse_loss(pred, y)\n",
    "    loss = -np.sum(logsoftmax(pred) * y)\n",
    "    return loss\n",
    "    \n",
    "test_mse_grads_fn = jit(vmap(grad(test_loss_adv_mse, argnums=1), in_axes=(None, 0, None, 0, None, None, None), \n",
    "                             out_axes=0), static_argnums=(4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_adv_x(kernel_fn, x_train, x_test, y_test, t=None, train_batch=256):\n",
    "    \n",
    "    \n",
    "    # for building matrix\n",
    "    kernel_fn = jit(kernel_fn, static_argnums=(2,))\n",
    "    \n",
    "    def inv(k):\n",
    "        #inverse with diag_reg\n",
    "        return onp.linalg.inv(k + diag_reg * onp.eye(k.shape[0]))\n",
    "    \n",
    "    num_iter = x_train.shape[0] // train_batch\n",
    "    \n",
    "    grads = 0\n",
    "    # print('generating FGSM data...')\n",
    "    for idx in range(num_iter):\n",
    "        \n",
    "        x_train_batch = x_train[idx*train_batch: (idx+1)*train_batch]\n",
    "        y_train_batch = y_train[idx*train_batch: (idx+1)*train_batch]\n",
    "        \n",
    "        ntk_train_train     = kernel_fn(x_train_batch, None, 'ntk')\n",
    "        # ntk_train_train_inv = inv(ntk_train_train)\n",
    "    \n",
    "        # FGSM\n",
    "        grads += fast_gradient_method_batch(model_fn=model_fn, kernel_fn=kernel_fn, obj_fn='untargeted', \n",
    "                                           grads_fn=test_mse_grads_fn, x_train=x_train_batch, y_train=y_train_batch, \n",
    "                                           x_test=x_test, y=y_test, t=t, eps=eps, clip_min=0, clip_max=1, ntk_train_train=ntk_train_train)\n",
    "    \n",
    "    perturbation = eps * np.sign(grads)\n",
    "    adv_x_FGSM = x_test + perturbation\n",
    "    adv_x_FGSM = np.clip(adv_x_FGSM, a_min=0, a_max=1)\n",
    "\n",
    "    return adv_x_FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [2:59:45<00:00, 21.06s/it]  \n"
     ]
    }
   ],
   "source": [
    "adv_x_FGSM, adv_x_IFGSM_100 = {}, {}\n",
    "for t in time:\n",
    "    adv_x_FGSM[t] = []\n",
    "    adv_x_IFGSM_100[t] = []\n",
    "    # print(\"generating time:\", t)\n",
    "    \n",
    "    for batch_id in tqdm(range(test_size//test_batch_size)):\n",
    "        fgsm = gen_adv_x(kernel_fn,\n",
    "                         x_train,\n",
    "                         x_test[batch_id*test_batch_size:(batch_id+1)*test_batch_size], \n",
    "                         y_test[batch_id*test_batch_size:(batch_id+1)*test_batch_size],\n",
    "                         t)\n",
    "        \n",
    "        adv_x_FGSM[t].append(fgsm)\n",
    "        # adv_x_IFGSM_100[t].append(ifgsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "onp.save('./batch_NTK_simple_stride_no_dense_time=%d.npy'%(time[0]),\n",
    "         onp.concatenate(adv_x_FGSM[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6871d4d9b0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(adv_x_FGSM[t][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
