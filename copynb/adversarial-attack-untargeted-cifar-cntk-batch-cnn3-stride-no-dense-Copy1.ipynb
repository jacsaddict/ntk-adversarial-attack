{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "part = 0\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(part)\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.0\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "\n",
    "import numpy as onp\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "\n",
    "from jax import lax, random\n",
    "from jax.api import grad, jit, vmap\n",
    "from jax.config import config\n",
    "from jax.experimental import optimizers\n",
    "from jax.experimental.stax import logsoftmax\n",
    "\n",
    "config.update('jax_enable_x64', True)\n",
    "\n",
    "from neural_tangents import stax\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# Attacking\n",
    "from cleverhans.utils import clip_eta, one_hot\n",
    "\n",
    "# Plotting\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from utils import *\n",
    "\n",
    "sns.set_style(style='white')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "\"\"\"\n",
    "diag_reg:\n",
    "    a scalar representing the strength of the diagonal regularization for\n",
    "    `k_train_train`, i.e. computing `k_train_train + diag_reg * I` during\n",
    "    Cholesky factorization or eigendecomposition.\n",
    "\"\"\"\n",
    "diag_reg = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = [1e3, 5e3, 1e4, 2e4, 4e4, 8e4, 16e4, None]\n",
    "time = [time[part]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'cifar10'\n",
    "class_num   = 10\n",
    "image_shape = None\n",
    "\n",
    "train_size = None\n",
    "test_size = 512\n",
    "test_batch_size = 1\n",
    "eps = 0.03\n",
    "\n",
    "if DATASET =='mnist':\n",
    "    image_shape = (28, 28, 1)\n",
    "elif DATASET == 'cifar10':\n",
    "    image_shape = (32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_all, y_train_all, x_test_all, y_test_all = tuple(onp.array(x) for x in get_dataset(DATASET, None, None, \n",
    "                                                                                  do_flatten_and_normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "seed = 0\n",
    "x_train_all, y_train_all = shaffle(x_train_all, y_train_all, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# down sample\n",
    "x_train = x_train_all[:train_size]\n",
    "y_train = y_train_all[:train_size]\n",
    "\n",
    "x_test = x_test_all[:test_size]\n",
    "y_test = y_test_all[:test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train.reshape((-1, *image_shape)), x_test.reshape((-1, *image_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(mean, ys):\n",
    "    return onp.argmax(mean, axis=-1) == onp.argmax(ys, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvBlock(channels, W_std, b_std, strides=(1,1)):\n",
    "    return stax.serial(stax.Conv(out_chan=channels, filter_shape=(3,3), strides=strides, padding='SAME',\n",
    "                                 W_std=W_std, b_std=b_std), \n",
    "                       stax.Relu(do_backprop=True))\n",
    "\n",
    "def ConvGroup(n, channels, stride, W_std, b_std, last_stride=False):\n",
    "    blocks = []\n",
    "    if last_stride:\n",
    "        for i in range(n-1):\n",
    "            blocks += [ConvBlock(channels, W_std, b_std, stride)]\n",
    "        blocks += [ConvBlock(channels, W_std, b_std, (2, 2))]\n",
    "    \n",
    "    else:\n",
    "        for i in range(n):\n",
    "            blocks += [ConvBlock(channels, W_std, b_std, stride)]\n",
    "        \n",
    "    return stax.serial(*blocks)\n",
    "        \n",
    "def VGG19(class_num=class_num):\n",
    "    return stax.serial(\n",
    "        ConvGroup(n=2, channels=64 , stride=(1,1), W_std=1.414, b_std=0.18, last_stride=False),\n",
    "        ConvGroup(n=2, channels=128, stride=(1,1), W_std=1.414, b_std=0.18, last_stride=False),\n",
    "        ConvGroup(n=4, channels=256, stride=(1,1), W_std=1.414, b_std=0.18, last_stride=False),\n",
    "        ConvGroup(n=4, channels=512, stride=(1,1), W_std=1.414, b_std=0.18, last_stride=False),\n",
    "        ConvGroup(n=4, channels=512, stride=(1,1), W_std=1.414, b_std=0.18, last_stride=False),\n",
    "        stax.Flatten())\n",
    "\n",
    "def simple_net(class_num=class_num):\n",
    "    return stax.serial(\n",
    "        ConvGroup(n=3, channels=64 , stride=(1 ,1), W_std=1.414, b_std=0.18, last_stride=True),\n",
    "        stax.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_fn, apply_fn, kernel_fn = simple_net(class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_kernel_fn = nt.batch(kernel_fn, batch_size=256, store_on_device=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel_train_m = batch_kernel_fn(x_train[:2048], None, 'ntk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict_fn = nt.predict.gradient_descent_mse(kernel_train_m, y_train[:2048], diag_reg=diag_reg)\n",
    "# kernel_test_train = batch_kernel_fn(x_test[:256], x_train[:2048], 'ntk')\n",
    "# pred = predict_fn(None, 0, 0 , kernel_test_train)\n",
    "# ans = onp.argmax(pred[1], axis=1)==onp.argmax(y_test[:256], axis=1)\n",
    "\n",
    "# print(\"testing accuracy: %.4f\"%(sum(ans)/ans.shape[0]))\n",
    "\n",
    "# plt.figure(dpi=100)\n",
    "# plt.imshow(kernel_train_m[:128, :128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(kernel_fn, x_train=None, x_test=None, fx_train_0=0., fx_test_0=0., t=None, ntk_train_train=None):\n",
    "    # Kernel\n",
    "    if ntk_train_train is None:\n",
    "        ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "    \n",
    "    ntk_test_train = kernel_fn(x_test, x_train, 'ntk')\n",
    "    # Prediction\n",
    "    predict_fn = nt.predict.gradient_descent_mse(ntk_train_train, y_train, diag_reg=diag_reg) # no convariance\n",
    "    \n",
    "    return predict_fn(t, fx_train_0, fx_test_0, ntk_test_train) # fx_train_0, fx_test_0 = (0, 0) for infinite width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def l2_loss_v1(logits, labels, weighting=1):\n",
    "    \"\"\"\n",
    "    Tensorflow version of L2 loss (without sqrt)\n",
    "    \"\"\"\n",
    "    return np.sum(((logits - labels)**2) * weighting) / 2\n",
    "    \n",
    "@jit\n",
    "def l2_loss_v2(logits, lables):\n",
    "    \"\"\"\n",
    "    Normal L2 loss\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(logits - labels)\n",
    "\n",
    "@jit\n",
    "def cross_entropy_loss(logits, lables):\n",
    "    return -np.sum(logsoftmax(logits) * lables)\n",
    "    \n",
    "@jit\n",
    "def mse_loss(logits, lables):\n",
    "    return 0.5 * np.mean((logits - lables) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attack algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, x_train=None, y_train=None, x_test=None, \n",
    "                         y=None, t=None, loss_weighting=None, fx_train_0=0., fx_test_0=0., eps=0.3, \n",
    "                         norm=np.inf, clip_min=None, clip_max=None, targeted=False):\n",
    "    \n",
    "    if norm not in [np.inf, 2]:\n",
    "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
    "        \n",
    "    x = x_test\n",
    "        \n",
    "    # test independent\n",
    "    if obj_fn == 'untargeted':\n",
    "        grads = grads_fn(x_train, x_test, y_train, y, kernel_fn, t)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Objective function must be either train(ntk_train_train) or test(predict_fn)\")\n",
    "\n",
    "    axis = list(range(1, len(grads.shape)))\n",
    "    eps_div = 1e-12\n",
    "    \n",
    "    if norm == np.inf:\n",
    "        perturbation = eps * np.sign(grads)\n",
    "    elif norm == 1:\n",
    "        raise NotImplementedError(\"L_1 norm has not been implemented yet.\")\n",
    "    elif norm == 2:\n",
    "        square = np.maximum(eps_div, np.sum(np.square(grads), axis=axis, keepdims=True))\n",
    "        perturbation = grads / np.sqrt(square)\n",
    "    \n",
    "    adv_x = x + perturbation\n",
    "    \n",
    "    # If clipping is needed, reset all values outside of [clip_min, clip_max]\n",
    "    if (clip_min is not None) or (clip_max is not None):\n",
    "        # We don't currently support one-sided clipping\n",
    "        assert clip_min is not None and clip_max is not None\n",
    "        adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "    \n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_gradient_method_batch(model_fn, kernel_fn, obj_fn, grads_fn, ntk_train_train, x_train=None, y_train=None, \n",
    "                               x_test=None, y=None, t=None, loss_weighting=None, fx_train_0=0., fx_test_0=0., eps=0.3, \n",
    "                               norm=np.inf, clip_min=None, clip_max=None, targeted=False):\n",
    "    \n",
    "    if norm not in [np.inf, 2]:\n",
    "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
    "        \n",
    "    x = x_test\n",
    "        \n",
    "    # test independent\n",
    "    if obj_fn == 'untargeted':\n",
    "        grads = grads_fn(x_train, x_test, y_train, y, kernel_fn, ntk_train_train, t)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Objective function must be either train(ntk_train_train) or test(predict_fn)\")\n",
    "\n",
    "    axis = list(range(1, len(grads.shape)))\n",
    "    eps_div = 1e-12\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, x_train=None, y_train=None,\n",
    "                               x_test=None, y=None, t=None, loss_weighting=None, fx_train_0=0., fx_test_0=0., \n",
    "                               eps=0.3, eps_iter=0.03, nb_iter=10, norm=np.inf, clip_min=None, clip_max=None, \n",
    "                               targeted=False, rand_init=None, rand_minmax=0.3):\n",
    "\n",
    "    assert eps_iter <= eps, (eps_iter, eps)\n",
    "    if norm == 1:\n",
    "        raise NotImplementedError(\"It's not clear that FGM is a good inner loop\"\n",
    "                                  \" step for PGD when norm=1, because norm=1 FGM \"\n",
    "                                  \" changes only one pixel at a time. We need \"\n",
    "                                  \" to rigorously test a strong norm=1 PGD \"\n",
    "                                  \"before enabling this feature.\")\n",
    "    if norm not in [np.inf, 2]:\n",
    "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
    "        \n",
    "    x = x_test\n",
    "    \n",
    "    # Initialize loop variables\n",
    "    if rand_init:\n",
    "        rand_minmax = eps\n",
    "        eta = random.uniform(new_key, x.shape, minval=-rand_minmax, maxval=rand_minmax)\n",
    "    else:\n",
    "        eta = np.zeros_like(x)\n",
    "\n",
    "    # Clip eta\n",
    "    eta = clip_eta(eta, norm, eps)\n",
    "    adv_x = x + eta\n",
    "    if clip_min is not None or clip_max is not None:\n",
    "        adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "        \n",
    "    for i in range(nb_iter):\n",
    "        adv_x = fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, x_train, y_train, adv_x, \n",
    "                                        y, t, loss_weighting, fx_train_0, fx_test_0, eps_iter, norm, \n",
    "                                        clip_min, clip_max, targeted)\n",
    "\n",
    "        # Clipping perturbation eta to norm norm ball\n",
    "        eta = adv_x - x\n",
    "        eta = clip_eta(eta, norm, eps)\n",
    "        adv_x = x + eta\n",
    "\n",
    "        # Redo the clipping.\n",
    "        # FGM already did it, but subtracting and re-adding eta can add some\n",
    "        # small numerical error.\n",
    "        if clip_min is not None or clip_max is not None:\n",
    "            adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "    \n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'mnist':\n",
    "    eps = 0.3\n",
    "    eps_iter_10 = (eps/10)*1.1\n",
    "    eps_iter_100 = (eps/100)*1.1\n",
    "    eps_iter_1000 = (eps/1000)*1.1\n",
    "    \n",
    "elif DATASET == 'cifar10':\n",
    "    eps = 0.03\n",
    "    eps_iter_10 = (eps/10)*1.1\n",
    "    eps_iter_100 = (eps/100)*1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(x_train, x_test, y_test, model_fn, kernel_fn, t=None, attack_type=None, ntk_train_train=None):\n",
    "    \n",
    "    y_train_predict, y_test_predict = model_fn(kernel_fn, x_train, x_test, \n",
    "                                               t=t, ntk_train_train=ntk_train_train)\n",
    "    \n",
    "    selected_table = correct(y_test_predict, y_test)\n",
    "    print(\"Accuray({:s}): {:.2f}\".format(attack_type, onp.mean(selected_table)))\n",
    "    \n",
    "    return selected_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_robustness(x_train, x_test, y_test, model_fn, kernel_fn, selected_table, t=None, \n",
    "                        attack_type=None, ntk_train_train=None):\n",
    "    \n",
    "    y_train_predict, y_test_predict = model_fn(kernel_fn, x_train, x_test,\n",
    "                                               t=t, ntk_train_train=ntk_train_train)\n",
    "    \n",
    "    y_test_predict = onp.asarray(y_test_predict)\n",
    "    y_test_predict_select = y_test_predict[onp.asarray(selected_table)]\n",
    "    y_test_select = y_test[onp.asarray(selected_table)]\n",
    "    print(\"Robustness({:s}): {:.2f}\".format(attack_type, onp.mean(correct(y_test_predict_select, y_test_select))))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adv_x generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv(k):\n",
    "        #inverse with diag_reg\n",
    "        return onp.linalg.inv(k + diag_reg * onp.eye(k.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel_train_inv_m = inv(kernel_train_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss_adv_mse(x_train, x_test, y_train, y, kernel_fn, ntk_train_train, t=None, diag_reg=diag_reg):\n",
    "    \n",
    "    ntk_test_train = kernel_fn(x_test[None], x_train, 'ntk')\n",
    "\n",
    "    predict_fn = nt.predict.gradient_descent_mse(ntk_train_train, y_train, diag_reg=diag_reg)\n",
    "    # predict_fn(t, train_0, test_0, kernel_matrix)\n",
    "    pred = predict_fn(t, 0., 0., ntk_test_train)[1]\n",
    "\n",
    "    # loss = -mse_loss(pred, y)\n",
    "    loss = -np.sum(logsoftmax(pred) * y)\n",
    "    return loss\n",
    "    \n",
    "test_mse_grads_fn = jit(vmap(grad(test_loss_adv_mse, argnums=1), in_axes=(None, 0, None, 0, None, None, None), \n",
    "                             out_axes=0), static_argnums=(4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_adv_x(kernel_fn, x_train, x_test, y_test, t=None, train_batch=256):\n",
    "    \n",
    "    \n",
    "    # for building matrix\n",
    "    kernel_fn = jit(kernel_fn, static_argnums=(2,))\n",
    "    \n",
    "    def inv(k):\n",
    "        #inverse with diag_reg\n",
    "        return onp.linalg.inv(k + diag_reg * onp.eye(k.shape[0]))\n",
    "    \n",
    "    num_iter = x_train.shape[0] // train_batch\n",
    "    \n",
    "    grads = 0\n",
    "    # print('generating FGSM data...')\n",
    "    for idx in range(num_iter):\n",
    "        \n",
    "        x_train_batch = x_train[idx*train_batch: (idx+1)*train_batch]\n",
    "        y_train_batch = y_train[idx*train_batch: (idx+1)*train_batch]\n",
    "        \n",
    "        ntk_train_train     = kernel_fn(x_train_batch, None, 'ntk')\n",
    "        # ntk_train_train_inv = inv(ntk_train_train)\n",
    "    \n",
    "        # FGSM\n",
    "        grads += fast_gradient_method_batch(model_fn=model_fn, kernel_fn=kernel_fn, obj_fn='untargeted', \n",
    "                                           grads_fn=test_mse_grads_fn, x_train=x_train_batch, y_train=y_train_batch, \n",
    "                                           x_test=x_test, y=y_test, t=t, eps=eps, clip_min=0, clip_max=1, ntk_train_train=ntk_train_train)\n",
    "    \n",
    "    perturbation = eps * np.sign(grads)\n",
    "    adv_x_FGSM = x_test + perturbation\n",
    "    adv_x_FGSM = np.clip(adv_x_FGSM, a_min=0, a_max=1)\n",
    "\n",
    "    return adv_x_FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [2:59:24<00:00, 21.02s/it]  \n"
     ]
    }
   ],
   "source": [
    "adv_x_FGSM, adv_x_IFGSM_100 = {}, {}\n",
    "for t in time:\n",
    "    adv_x_FGSM[t] = []\n",
    "    adv_x_IFGSM_100[t] = []\n",
    "    # print(\"generating time:\", t)\n",
    "    \n",
    "    for batch_id in tqdm(range(test_size//test_batch_size)):\n",
    "        fgsm = gen_adv_x(kernel_fn,\n",
    "                         x_train,\n",
    "                         x_test[batch_id*test_batch_size:(batch_id+1)*test_batch_size], \n",
    "                         y_test[batch_id*test_batch_size:(batch_id+1)*test_batch_size],\n",
    "                         t)\n",
    "        \n",
    "        adv_x_FGSM[t].append(fgsm)\n",
    "        # adv_x_IFGSM_100[t].append(ifgsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "onp.save('./batch_NTK_simple_stride_no_dense_time=%d.npy'%(time[0]),\n",
    "         onp.concatenate(adv_x_FGSM[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7eff88e34908>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(adv_x_FGSM[t][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
