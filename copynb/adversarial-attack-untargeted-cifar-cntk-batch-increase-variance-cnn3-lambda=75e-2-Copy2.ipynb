{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "gpu_id = 2\n",
    "lamb = 0.75\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.0\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "\n",
    "import numpy as onp\n",
    "\n",
    "time = [onp.load('time.npy')[gpu_id]]\n",
    "\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "\n",
    "from jax import lax, random\n",
    "from jax.api import grad, jit, vmap\n",
    "from jax.config import config\n",
    "from jax.experimental import optimizers\n",
    "from jax.experimental.stax import logsoftmax\n",
    "\n",
    "config.update('jax_enable_x64', True)\n",
    "\n",
    "from neural_tangents import stax\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# Attacking\n",
    "from cleverhans.utils import clip_eta, one_hot\n",
    "\n",
    "# Plotting\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from utils import *\n",
    "\n",
    "sns.set_style(style='white')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "\"\"\"\n",
    "diag_reg:\n",
    "    a scalar representing the strength of the diagonal regularization for\n",
    "    `k_train_train`, i.e. computing `k_train_train + diag_reg * I` during\n",
    "    Cholesky factorization or eigendecomposition.\n",
    "\"\"\"\n",
    "diag_reg = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'cifar10'\n",
    "class_num   = 10\n",
    "image_shape = None\n",
    "\n",
    "train_size = None\n",
    "test_size = 512\n",
    "test_batch_size = 1\n",
    "eps = 0.03\n",
    "\n",
    "if DATASET =='mnist':\n",
    "    image_shape = (28, 28, 1)\n",
    "elif DATASET == 'cifar10':\n",
    "    image_shape = (32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_all, y_train_all, x_test_all, y_test_all = tuple(onp.array(x) for x in get_dataset(DATASET, None, None, \n",
    "                                                                                  do_flatten_and_normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "seed = 0\n",
    "x_train_all, y_train_all = shaffle(x_train_all, y_train_all, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# down sample\n",
    "x_train = x_train_all[:train_size]\n",
    "y_train = y_train_all[:train_size]\n",
    "\n",
    "x_test = x_test_all[:test_size]\n",
    "y_test = y_test_all[:test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train.reshape((-1, *image_shape)), x_test.reshape((-1, *image_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(mean, ys):\n",
    "    return onp.argmax(mean, axis=-1) == onp.argmax(ys, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 0.18\n",
    "W = 1.76\n",
    "\n",
    "b_std = np.sqrt(b)\n",
    "W_std = np.sqrt(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvBlock(channels, W_std, b_std, strides=(1,1)):\n",
    "    return stax.serial(stax.Conv(out_chan=channels, filter_shape=(3,3), strides=strides, padding='SAME',\n",
    "                                 W_std=W_std, b_std=b_std), \n",
    "                       stax.Relu(do_backprop=True))\n",
    "\n",
    "def ConvGroup(n, channels, stride, W_std, b_std, last_stride=False):\n",
    "    blocks = []\n",
    "    if last_stride:\n",
    "        for i in range(n-1):\n",
    "            blocks += [ConvBlock(channels, W_std, b_std, stride)]\n",
    "        blocks += [ConvBlock(channels, W_std, b_std, (2, 2))]\n",
    "    \n",
    "    else:\n",
    "        for i in range(n):\n",
    "            blocks += [ConvBlock(channels, W_std, b_std, stride)]\n",
    "        \n",
    "    return stax.serial(*blocks)\n",
    "        \n",
    "def VGG19(class_num=class_num):\n",
    "    return stax.serial(\n",
    "        ConvGroup(n=2, channels=64 , stride=(1,1), W_std=W_std, b_std=b_std, last_stride=False),\n",
    "        ConvGroup(n=2, channels=128, stride=(1,1), W_std=W_std, b_std=b_std, last_stride=False),\n",
    "        ConvGroup(n=4, channels=256, stride=(1,1), W_std=W_std, b_std=b_std, last_stride=False),\n",
    "        ConvGroup(n=4, channels=512, stride=(1,1), W_std=W_std, b_std=b_std, last_stride=False),\n",
    "        ConvGroup(n=4, channels=512, stride=(1,1), W_std=W_std, b_std=b_std, last_stride=False),\n",
    "        stax.Flatten(),\n",
    "        stax.Dense(512), stax.Relu(do_backprop=True),\n",
    "        stax.Dense(class_num))\n",
    "\n",
    "def simple_net(class_num=class_num):\n",
    "    return stax.serial(\n",
    "        ConvGroup(n=3, channels=64 , stride=(1,1), W_std=W_std, b_std=b_std, last_stride=False),\n",
    "        stax.Flatten(),\n",
    "        stax.Dense(512, W_std=W_std), stax.Relu(do_backprop=True),\n",
    "        stax.Dense(class_num, W_std=W_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_fn, apply_fn, kernel_fn = simple_net(class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_kernel_fn = nt.batch(kernel_fn, batch_size=256, store_on_device=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel_train_m = batch_kernel_fn(x_train[:256], None, 'ntk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(kernel_fn, x_train=None, x_test=None, fx_train_0=0., fx_test_0=0., t=None, ntk_train_train=None):\n",
    "    # Kernel\n",
    "    if ntk_train_train is None:\n",
    "        ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "    \n",
    "    ntk_test_train = kernel_fn(x_test, x_train, 'ntk')\n",
    "    # Prediction\n",
    "    predict_fn = nt.predict.gradient_descent_mse(ntk_train_train, y_train, diag_reg=diag_reg) # no convariance\n",
    "    \n",
    "    return predict_fn(t, fx_train_0, fx_test_0, ntk_test_train) # fx_train_0, fx_test_0 = (0, 0) for infinite width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def l2_loss_v1(logits, labels, weighting=1):\n",
    "    \"\"\"\n",
    "    Tensorflow version of L2 loss (without sqrt)\n",
    "    \"\"\"\n",
    "    return np.sum(((logits - labels)**2) * weighting) / 2\n",
    "    \n",
    "@jit\n",
    "def l2_loss_v2(logits, lables):\n",
    "    \"\"\"\n",
    "    Normal L2 loss\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(logits - labels)\n",
    "\n",
    "@jit\n",
    "def cross_entropy_loss(logits, lables):\n",
    "    return -np.sum(logsoftmax(logits) * lables)\n",
    "    \n",
    "@jit\n",
    "def mse_loss(logits, lables):\n",
    "    return 0.5 * np.mean((logits - lables) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attack algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, x_train=None, y_train=None, x_test=None, \n",
    "                         y=None, t=None, loss_weighting=None, fx_train_0=0., fx_test_0=0., eps=0.3, \n",
    "                         norm=np.inf, clip_min=None, clip_max=None, targeted=False):\n",
    "    \n",
    "    if norm not in [np.inf, 2]:\n",
    "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
    "        \n",
    "    x = x_test\n",
    "        \n",
    "    # test independent\n",
    "    if obj_fn == 'untargeted':\n",
    "        grads = grads_fn(x_train, x_test, y_train, y, kernel_fn, t)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Objective function must be either train(ntk_train_train) or test(predict_fn)\")\n",
    "\n",
    "    axis = list(range(1, len(grads.shape)))\n",
    "    eps_div = 1e-12\n",
    "    \n",
    "    if norm == np.inf:\n",
    "        perturbation = eps * np.sign(grads)\n",
    "    elif norm == 1:\n",
    "        raise NotImplementedError(\"L_1 norm has not been implemented yet.\")\n",
    "    elif norm == 2:\n",
    "        square = np.maximum(eps_div, np.sum(np.square(grads), axis=axis, keepdims=True))\n",
    "        perturbation = grads / np.sqrt(square)\n",
    "    \n",
    "    adv_x = x + perturbation\n",
    "    \n",
    "    # If clipping is needed, reset all values outside of [clip_min, clip_max]\n",
    "    if (clip_min is not None) or (clip_max is not None):\n",
    "        # We don't currently support one-sided clipping\n",
    "        assert clip_min is not None and clip_max is not None\n",
    "        adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "    \n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_gradient_method_batch(model_fn, kernel_fn, obj_fn, grads_fn, ntk_train_train, x_train=None, y_train=None, \n",
    "                               x_test=None, y=None, t=None, loss_weighting=None, fx_train_0=0., fx_test_0=0., eps=0.3, \n",
    "                               norm=np.inf, clip_min=None, clip_max=None, targeted=False):\n",
    "    \n",
    "    if norm not in [np.inf, 2]:\n",
    "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
    "        \n",
    "    x = x_test\n",
    "        \n",
    "    # test independent\n",
    "    if obj_fn == 'untargeted':\n",
    "        grads = grads_fn(x_train, x_test, y_train, y, kernel_fn, ntk_train_train, t)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Objective function must be either train(ntk_train_train) or test(predict_fn)\")\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, x_train=None, y_train=None,\n",
    "                               x_test=None, y=None, t=None, loss_weighting=None, fx_train_0=0., fx_test_0=0., \n",
    "                               eps=0.3, eps_iter=0.03, nb_iter=10, norm=np.inf, clip_min=None, clip_max=None, \n",
    "                               targeted=False, rand_init=None, rand_minmax=0.3):\n",
    "\n",
    "    assert eps_iter <= eps, (eps_iter, eps)\n",
    "    if norm == 1:\n",
    "        raise NotImplementedError(\"It's not clear that FGM is a good inner loop\"\n",
    "                                  \" step for PGD when norm=1, because norm=1 FGM \"\n",
    "                                  \" changes only one pixel at a time. We need \"\n",
    "                                  \" to rigorously test a strong norm=1 PGD \"\n",
    "                                  \"before enabling this feature.\")\n",
    "    if norm not in [np.inf, 2]:\n",
    "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
    "        \n",
    "    x = x_test\n",
    "    \n",
    "    # Initialize loop variables\n",
    "    if rand_init:\n",
    "        rand_minmax = eps\n",
    "        eta = random.uniform(new_key, x.shape, minval=-rand_minmax, maxval=rand_minmax)\n",
    "    else:\n",
    "        eta = np.zeros_like(x)\n",
    "\n",
    "    # Clip eta\n",
    "    eta = clip_eta(eta, norm, eps)\n",
    "    adv_x = x + eta\n",
    "    if clip_min is not None or clip_max is not None:\n",
    "        adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "        \n",
    "    for i in range(nb_iter):\n",
    "        adv_x = fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, x_train, y_train, adv_x, \n",
    "                                        y, t, loss_weighting, fx_train_0, fx_test_0, eps_iter, norm, \n",
    "                                        clip_min, clip_max, targeted)\n",
    "\n",
    "        # Clipping perturbation eta to norm norm ball\n",
    "        eta = adv_x - x\n",
    "        eta = clip_eta(eta, norm, eps)\n",
    "        adv_x = x + eta\n",
    "\n",
    "        # Redo the clipping.\n",
    "        # FGM already did it, but subtracting and re-adding eta can add some\n",
    "        # small numerical error.\n",
    "        if clip_min is not None or clip_max is not None:\n",
    "            adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "    \n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'mnist':\n",
    "    eps = 0.3\n",
    "    eps_iter_10 = (eps/10)*1.1\n",
    "    eps_iter_100 = (eps/100)*1.1\n",
    "    eps_iter_1000 = (eps/1000)*1.1\n",
    "    \n",
    "elif DATASET == 'cifar10':\n",
    "    eps = 0.03\n",
    "    eps_iter_10 = (eps/10)*1.1\n",
    "    eps_iter_100 = (eps/100)*1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(x_train, x_test, y_test, model_fn, kernel_fn, t=None, attack_type=None, ntk_train_train=None):\n",
    "    \n",
    "    y_train_predict, y_test_predict = model_fn(kernel_fn, x_train, x_test, \n",
    "                                               t=t, ntk_train_train=ntk_train_train)\n",
    "    \n",
    "    selected_table = correct(y_test_predict, y_test)\n",
    "    print(\"Accuray({:s}): {:.2f}\".format(attack_type, onp.mean(selected_table)))\n",
    "    \n",
    "    return selected_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_robustness(x_train, x_test, y_test, model_fn, kernel_fn, selected_table, t=None, \n",
    "                        attack_type=None, ntk_train_train=None):\n",
    "    \n",
    "    y_train_predict, y_test_predict = model_fn(kernel_fn, x_train, x_test,\n",
    "                                               t=t, ntk_train_train=ntk_train_train)\n",
    "    \n",
    "    y_test_predict = onp.asarray(y_test_predict)\n",
    "    y_test_predict_select = y_test_predict[onp.asarray(selected_table)]\n",
    "    y_test_select = y_test[onp.asarray(selected_table)]\n",
    "    print(\"Robustness({:s}): {:.2f}\".format(attack_type, onp.mean(correct(y_test_predict_select, y_test_select))))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adv_x generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv(k):\n",
    "        #inverse with diag_reg\n",
    "        return onp.linalg.inv(k + diag_reg * onp.eye(k.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss_adv_mse(x_train, x_test, y_train, y, kernel_fn, ntk_train_train=None, t=None, diag_reg=diag_reg):\n",
    "\n",
    "    # ntk_test_train = kernel_fn(x_test[None], x_train, 'ntk')\n",
    "\n",
    "    predict_fn = nt.predict.gradient_descent_mse_ensemble(kernel_fn, x_train, y_train, diag_reg=diag_reg)\n",
    "\n",
    "    pred = predict_fn(t, x_test[None], 'ntk', True)\n",
    "\n",
    "    variance_test = np.diag(pred.covariance)\n",
    "    \n",
    "    loss = -np.sum(logsoftmax(pred.mean) * y) - lamb * np.sum(variance_test)\n",
    "    return loss\n",
    "    \n",
    "test_mse_grads_fn = jit(vmap(grad(test_loss_adv_mse, argnums=1), in_axes=(None, 0, None, 0, None, None, None), \n",
    "                             out_axes=0), static_argnums=(4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_fn = jit(kernel_fn, static_argnums=(2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_adv_x(kernel_fn, x_train, x_test, y_test, t=None, train_batch=256):\n",
    "    \n",
    "    num_iter = x_train.shape[0] // train_batch\n",
    "    grads = 0\n",
    "    for idx in range(num_iter):\n",
    "        x_train_batch = x_train[idx*train_batch: (idx+1)*train_batch]\n",
    "        y_train_batch = y_train[idx*train_batch: (idx+1)*train_batch]\n",
    "        \n",
    "        # ntk_train_train     = kernel_fn(x_train_batch, None, 'ntk')\n",
    "        # ntk_train_train_inv = inv(ntk_train_train)\n",
    "    \n",
    "        # FGSM\n",
    "        grads += fast_gradient_method_batch(model_fn=model_fn, kernel_fn=kernel_fn, obj_fn='untargeted', \n",
    "                                            grads_fn=test_mse_grads_fn, x_train=x_train_batch, y_train=y_train_batch, \n",
    "                                            x_test=x_test, y=y_test, t=t, eps=eps, clip_min=0, clip_max=1, ntk_train_train=None)\n",
    "    \n",
    "    perturbation = eps * np.sign(grads)\n",
    "    adv_x_FGSM = x_test + perturbation\n",
    "    adv_x_FGSM = np.clip(adv_x_FGSM, a_min=0, a_max=1)\n",
    "\n",
    "    return adv_x_FGSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [5:18:25<00:00, 37.32s/it]  \n"
     ]
    }
   ],
   "source": [
    "adv_x_FGSM, adv_x_IFGSM_100 = {}, {}\n",
    "for t in time:\n",
    "    adv_x_FGSM[t] = []\n",
    "    adv_x_IFGSM_100[t] = []\n",
    "    # print(\"generating time:\", t)\n",
    "    \n",
    "    for batch_id in tqdm(range(test_size//test_batch_size)):\n",
    "        fgsm = gen_adv_x(kernel_fn,\n",
    "                         x_train,\n",
    "                         x_test[batch_id*test_batch_size:(batch_id+1)*test_batch_size], \n",
    "                         y_test[batch_id*test_batch_size:(batch_id+1)*test_batch_size],\n",
    "                         t)\n",
    "        \n",
    "        adv_x_FGSM[t].append(fgsm)\n",
    "        # adv_x_IFGSM_100[t].append(ifgsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "onp.save('./batch_NTK_simple_increase_variance_lambda=%.2f_time=%d.npy'%(lamb, time[0]), \n",
    "         onp.concatenate(adv_x_FGSM[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
