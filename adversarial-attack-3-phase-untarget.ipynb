{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as np\n",
    "\n",
    "from jax import lax, random\n",
    "from jax.api import grad, jit, vmap\n",
    "from jax.config import config\n",
    "from jax.experimental import optimizers\n",
    "from jax.experimental.stax import logsoftmax\n",
    "\n",
    "import neural_tangents as nt\n",
    "import neural_tangents.stax as stax\n",
    "\n",
    "from functools import partial\n",
    "from cleverhans.utils import clip_eta, one_hot\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu\n"
     ]
    }
   ],
   "source": [
    "from jax.lib import xla_bridge\n",
    "print(xla_bridge.get_backend().platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## minist down sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, y_train, x_test, y_test = tuple(np.array(x) for x in get_dataset('mnist', 256, 10000))\n",
    "x_train, y_train, x_test, y_test = tuple(np.array(x) for x in get_dataset('mnist', 256, 512, \n",
    "                                                                          do_flatten_and_normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_reg_default = 1e-4 # for inverse solve\n",
    "config.update('jax_enable_x64', True) # for inverse precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(mean, ys):\n",
    "    return np.mean(np.argmax(mean, axis=-1) == np.argmax(ys, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WideDenseBlock(output_dim, W_std=1.0, b_std=0.0):\n",
    "    return stax.serial(stax.Dense(output_dim, W_std=W_std, b_std=b_std),\n",
    "                       stax.Erf())\n",
    "\n",
    "def WideDenseGroup(layer_num, output_dim, W_std, b_std):\n",
    "    blocks = []\n",
    "    blocks += [WideDenseBlock(output_dim, W_std, b_std)]\n",
    "    for _ in range(layer_num - 1):\n",
    "        blocks += [WideDenseBlock(output_dim, W_std, b_std)]\n",
    "        \n",
    "    return stax.serial(*blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_loss = lambda x1, x2, weight: ((x1 - x2)**2*weight)*0.5 # tf version\n",
    "cross_entropy = lambda fx, y_hat: -np.mean(stax.logsoftmax(fx) * y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 = random.normal(k[1], (10 ,1))\n",
    "# x2 = random.normal(k[2], (10 ,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loss_adv(x_train, y, kernel_fn, weight):\n",
    "    # Compute NTK on training data\n",
    "    ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "    loss = - l2_loss(ntk_train_train, y, weight) # y = matrix of 1 / diagnal\n",
    "    return loss\n",
    "\n",
    "# train_grads_fn = grad(train_loss_adv)\n",
    "train_grads_fn = jit(grad(train_loss_adv), static_argnums=(2,)) # static arg: expanding {if / else} loops for graph mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512, 784)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss_adv_mse(x_train, x_test, y_train, y, kernel_fn, diag_reg=diag_reg_default):\n",
    "    \"\"\"\n",
    "    x_train: training set of input\n",
    "    x_test:  testing set of input\n",
    "    y_train: training labels\n",
    "    y:       target kernel\n",
    "    kernel_fn: fn space of kernels\n",
    "    \n",
    "    This function calculate L2 loss between T_{M,N}, T_{N,N} and y (c*I)\n",
    "    \"\"\"\n",
    "    # Kernel\n",
    "    ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "    ntk_test_train = kernel_fn(x_test[None], x_train, 'ntk')\n",
    "    # ntk_test_train = kernel_fn(x_test, x_train, 'ntk')\n",
    "    \n",
    "    # Prediction\n",
    "    predict_fn = nt.predict.gradient_descent_mse(ntk_train_train, y_train, diag_reg=diag_reg) # diag_reg: add to easier inverse\n",
    "    # predict_fn(t, train_0, test_0, kernel_matrix)\n",
    "    # t = training step\n",
    "    # train_y_0 = init mean of y in training set\n",
    "    pred = predict_fn(None, 0., 0., ntk_test_train)[1]\n",
    "    \n",
    "    # cross entropy\n",
    "    loss = -np.sum(logsoftmax(pred) * y)\n",
    "    return loss\n",
    "\n",
    "test_mse_grads_fn = jit(vmap(grad(test_loss_adv_mse, argnums=1), in_axes=(None, 0, None, 0, None), out_axes=0), static_argnums=(4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss_adv_cross_entropy(x_train, x_test, y_train, y, kernel_fn, diag_reg=diag_reg_default):\n",
    "    # Kernel\n",
    "    ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "    ntk_test_train = kernel_fn(x_test[None], x_train, 'ntk')\n",
    "    # ntk_test_train = kernel_fn(x_test, x_train, 'ntk')\n",
    "\n",
    "    \n",
    "    # Prediction\n",
    "    predict_fn = nt.predict.gradient_descent(cross_entropy, ntk_train_train, y_train, diag_reg=diag_reg_default) # diag_reg: add to easier inverse\n",
    "    pred = predict_fn(None, 0., 0., ntk_test_train)[1]\n",
    "    \n",
    "    loss = - np.sum(logsoftmax(pred) * y)\n",
    "    return loss\n",
    "\n",
    "test_cross_entropy_grads_fn = jit(vmap(grad(test_loss_adv_cross_entropy, argnums=1), in_axes=(None, 0, None, 0, None), out_axes=0), static_argnums=(4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_mse_fn(kernel_fn, obj_fn, x_train=None, x_test=None, fx_train_0=0., fx_test_0=0., diag_reg=diag_reg_default):\n",
    "    # Kernel\n",
    "    ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "    \n",
    "    if obj_fn == 'train':\n",
    "        return ntk_train_train\n",
    "    elif obj_fn == 'test':\n",
    "        ntk_test_train = kernel_fn(x_test, x_train, 'ntk')\n",
    "        # Prediction\n",
    "        predict_fn = nt.predict.gradient_descent_mse(ntk_train_train, y_train, diag_reg=diag_reg) # no convariance\n",
    "        return predict_fn(None, fx_train_0, fx_test_0, ntk_test_train) # fx_train_0, diag_reg_default = (0, 0) for infinite width\n",
    "    else:\n",
    "        raise ValueError(\"Objective function must be either train(ntk_train_train) or test(predict_fn)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cross_entropy_fn(kernel_fn, obj_fn, x_train=None, x_test=None, fx_train_0=0., fx_test_0=0., diag_reg=diag_reg_default):\n",
    "    # Kernel\n",
    "    ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "    \n",
    "    if obj_fn == 'train':\n",
    "        return ntk_train_train\n",
    "    elif obj_fn == 'test':\n",
    "        ntk_test_train = kernel_fn(x_test, x_train, 'ntk')\n",
    "        # Prediction\n",
    "        predict_fn = nt.predict.gradient_descent(cross_entropy, ntk_train_train, y_train, diag_reg=diag_reg_default) # no convariance\n",
    "        return predict_fn(None, fx_train_0, fx_test_0, ntk_test_train) # fx_train_0, fx_test_0 = (0, 0) for infinite width\n",
    "    else:\n",
    "        raise ValueError(\"Objective function must be either train(ntk_train_train) or test(predict_fn)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, loss_weighting, phase, x_train, x_test, \n",
    "                         fx_train_0=0.0, fx_test_0=0.0, eps=0.3, norm=np.inf, clip_min=None, clip_max=None, \n",
    "                         y_train=None, y=None, targeted=False):\n",
    "    \"\"\"\n",
    "    JAX implementation of the Fast Gradient Method.\n",
    "    :param model_fn: a callable that takes an input tensor and returns the model logits.\n",
    "    :param x: input tensor.\n",
    "    :param obj_fn: test or train.\n",
    "    :param grads_fn: grads of loss function.\n",
    "    :param phase: direction of the corrupted data. one of chaotic or ordered.\n",
    "    :param eps: epsilon (input variation parameter); see https://arxiv.org/abs/1412.6572.\n",
    "    :param norm: Order of the norm (mimics NumPy). Possible values: np.inf or 2.\n",
    "    :param clip_min: (optional) float. Minimum float value for adversarial example components.\n",
    "    :param clip_max: (optional) float. Maximum float value for adversarial example components.\n",
    "    :param y: (optional) Tensor with one-hot true labels. If targeted is true, then provide the\n",
    "            target one-hot label. Otherwise, only provide this parameter if you'd like to use true\n",
    "            labels when crafting adversarial samples. Otherwise, model predictions are used\n",
    "            as labels to avoid the \"label leaking\" effect (explained in this paper:\n",
    "            https://arxiv.org/abs/1611.01236). Default is None. This argument does not have\n",
    "            to be a binary one-hot label (e.g., [0, 1, 0, 0]), it can be floating points values\n",
    "            that sum up to 1 (e.g., [0.05, 0.85, 0.05, 0.05]).\n",
    "    :param targeted: (optional) bool. Is the attack targeted or untargeted?\n",
    "            Untargeted, the default, will try to make the label incorrect.\n",
    "            Targeted will instead try to move in the direction of being more like y.\n",
    "            \n",
    "    :return: a tensor for the adversarial example\n",
    "    \"\"\"\n",
    "    if norm not in [np.inf, 2]:\n",
    "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
    "        \n",
    "    # define y if y is none\n",
    "    if obj_fn == 'train':\n",
    "        x = x_train\n",
    "        if y is None:\n",
    "            # Compute NTK on training data\n",
    "            ntk_train_train = model_fn(kernel_fn=kernel_fn, obj_fn='train', x_train=x_train)\n",
    "            \n",
    "            # Construct diagonal\n",
    "            if phase == 'ordered':\n",
    "                y = np.ones(ntk_train_train.shape)*100\n",
    "            elif phase == 'chaotic':\n",
    "                y = np.eye(ntk_train_train.shape[0])*100\n",
    "            else:\n",
    "                raise ValueError(\"Phase must be either 'ordered' or 'critical'\")\n",
    "                \n",
    "    elif obj_fn == 'test':\n",
    "        x = x_test\n",
    "        if y is None:\n",
    "            # Using model predictions as ground truth to avoid label leaking\n",
    "            x_labels = np.argmax(model_fn(kernel_fn, 'test', x_train, x_test, fx_train_0, fx_test_0)[1], 1)\n",
    "            y = one_hot(x_labels, 10)\n",
    "            \n",
    "        if y_train is None:\n",
    "            x_labels = np.argmax(model_fn(kernel_fn, 'test', x_train, x_train, fx_train_0, fx_test_0)[1], 1)\n",
    "            y_train = one_hot(x_labels, 10)\n",
    "    \n",
    "    if obj_fn == 'train':\n",
    "        grads = grads_fn(x_train, y, kernel_fn, loss_weighting)\n",
    "    elif obj_fn == 'test':\n",
    "        grads = grads_fn(x_train, x_test, y_train, y, kernel_fn)\n",
    "    else:\n",
    "        raise ValueError(\"Objective function must be either train(ntk_train_train) or test(predict_fn)\")\n",
    "\n",
    "    axis = list(range(1, len(grads.shape)))\n",
    "    avoid_zero_div = 1e-12\n",
    "    if norm == np.inf:\n",
    "        perturbation = eps * np.sign(grads)\n",
    "    elif norm == 1:\n",
    "        raise NotImplementedError(\"L_1 norm has not been implemented yet.\")\n",
    "    elif norm == 2:\n",
    "        square = np.maximum(avoid_zero_div, np.sum(np.square(grads), axis=axis, keepdims=True))\n",
    "        perturbation = grads / np.sqrt(square)\n",
    "\n",
    "    adv_x = x + perturbation\n",
    "\n",
    "    # If clipping is needed, reset all values outside of [clip_min, clip_max]\n",
    "    if (clip_min is not None) or (clip_max is not None):\n",
    "        # We don't currently support one-sided clipping\n",
    "        assert clip_min is not None and clip_max is not None\n",
    "        adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "        \n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_gradient_descent(model_fn, kernel_fn, obj_fn, grads_fn, loss_weighting, phase, x_train, x_test,\n",
    "                               fx_train_0=0., fx_test_0=0., eps=0.3, eps_iter=0.03, nb_iter=10, norm=np.inf, \n",
    "                               clip_min=None, clip_max=None, y_train=None, y=None, targeted=False, \n",
    "                               rand_init=None, rand_minmax=0.3):\n",
    "    \"\"\"\n",
    "    This class implements either the Basic Iterative Method\n",
    "    (Kurakin et al. 2016) when rand_init is set to 0. or the\n",
    "    Madry et al. (2017) method when rand_minmax is larger than 0.\n",
    "    Paper link (Kurakin et al. 2016): https://arxiv.org/pdf/1607.02533.pdf\n",
    "    Paper link (Madry et al. 2017): https://arxiv.org/pdf/1706.06083.pdf\n",
    "    :param model_fn: a callable that takes an input tensor and returns the model logits.\n",
    "    :param x: input tensor.\n",
    "    :param obj_fn: test or train.\n",
    "    :param grads_fn: grads of loss function.\n",
    "    :param phase: direction of the corrupted data. one of chaotic or ordered.\n",
    "    :param eps: epsilon (input variation parameter); see https://arxiv.org/abs/1412.6572.\n",
    "    :param eps_iter: step size for each attack iteration\n",
    "    :param nb_iter: Number of attack iterations.\n",
    "    :param norm: Order of the norm (mimics NumPy). Possible values: np.inf or 2.\n",
    "    :param clip_min: (optional) float. Minimum float value for adversarial example components.\n",
    "    :param clip_max: (optional) float. Maximum float value for adversarial example components.\n",
    "    :param y: (optional) Tensor with true labels. If targeted is true, then provide the\n",
    "            target label. Otherwise, only provide this parameter if you'd like to use true\n",
    "            labels when crafting adversarial samples. Otherwise, model predictions are used\n",
    "            as labels to avoid the \"label leaking\" effect (explained in this paper:\n",
    "            https://arxiv.org/abs/1611.01236). Default is None.\n",
    "    :param targeted: (optional) bool. Is the attack targeted or untargeted?\n",
    "            Untargeted, the default, will try to make the label incorrect.\n",
    "            Targeted will instead try to move in the direction of being more like y.\n",
    "    :return: a tensor for the adversarial example\n",
    "    \"\"\"\n",
    "\n",
    "    assert eps_iter <= eps, (eps_iter, eps)\n",
    "    if norm == 1:\n",
    "        raise NotImplementedError(\"It's not clear that FGM is a good inner loop\"\n",
    "                                  \" step for PGD when norm=1, because norm=1 FGM \"\n",
    "                                  \" changes only one pixel at a time. We need \"\n",
    "                                  \" to rigorously test a strong norm=1 PGD \"\n",
    "                                  \"before enabling this feature.\")\n",
    "    if norm not in [np.inf, 2]:\n",
    "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
    "        \n",
    "    \n",
    "    if obj_fn == 'train':\n",
    "        x = x_train\n",
    "        if y is None:\n",
    "            # Compute NTK on training data\n",
    "            ntk_train_train = model_fn(kernel_fn=kernel_fn, obj_fn='train', x_train=x_train)\n",
    "            \n",
    "            # Construct diagonal\n",
    "            if phase == 'ordered':\n",
    "                y = np.ones(ntk_train_train.shape)*100\n",
    "            elif phase == 'chaotic':\n",
    "                y = np.eye(ntk_train_train.shape[0])*100\n",
    "            else:\n",
    "                raise ValueError(\"Phase must be either 'ordered' or 'critical'\")\n",
    "                \n",
    "    elif obj_fn == 'test':\n",
    "        x = x_test\n",
    "        if y is None:\n",
    "            # Using model predictions as ground truth to avoid label leaking\n",
    "            x_labels = np.argmax(model_fn(kernel_fn, 'test', x_train, x_test, fx_train_0, fx_test_0)[1], 1)\n",
    "            y = one_hot(x_labels, 10)\n",
    "            \n",
    "        if y_train is None:\n",
    "            x_labels = np.argmax(model_fn(kernel_fn, 'test', x_train, x_train, fx_train_0, fx_test_0)[1], 1)\n",
    "            y_train = one_hot(x_labels, 10)\n",
    "            \n",
    "    else:\n",
    "        raise ValueError(\"Objective function must be either train(ntk_train_train) or test(predict_fn)\")\n",
    "        \n",
    "    # Initialize loop variables\n",
    "    if rand_init:\n",
    "        rand_minmax = eps\n",
    "        eta = random.uniform(new_key, x.shape, minval=-rand_minmax, maxval=rand_minmax)\n",
    "    else:\n",
    "        eta = np.zeros_like(x)\n",
    "\n",
    "    # Clip eta\n",
    "    eta = clip_eta(eta, norm, eps)\n",
    "    adv_x = x + eta\n",
    "    if clip_min is not None or clip_max is not None:\n",
    "        adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "        \n",
    "    for _ in range(nb_iter):\n",
    "        if obj_fn == 'test':\n",
    "            adv_x = fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, loss_weighting, phase, x_train, adv_x, \n",
    "                                         fx_train_0=0., fx_test_0=0., eps=eps_iter, norm=np.inf, clip_min=clip_min, \n",
    "                                         clip_max=clip_max, y_train=y_train, y=y, targeted=targeted)\n",
    "        elif obj_fn == 'train':\n",
    "            adv_x = fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, loss_weighting, phase, adv_x, x_test, \n",
    "                                         fx_train_0=0., fx_test_0=0., eps=eps_iter, norm=np.inf, clip_min=clip_min, \n",
    "                                         clip_max=clip_max, y_train=y_train, y=y, targeted=targeted)\n",
    "\n",
    "        # Clipping perturbation eta to norm norm ball\n",
    "        eta = adv_x - x\n",
    "        eta = clip_eta(eta, norm, eps)\n",
    "        adv_x = x + eta\n",
    "\n",
    "        # Redo the clipping.\n",
    "        # FGM already did it, but subtracting and re-adding eta can add some\n",
    "        # small numerical error.\n",
    "        if clip_min is not None or clip_max is not None:\n",
    "            adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "\n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the three phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_b = 0.18\n",
    "variance_w = np.asarray([1., 1.76, 2.5])\n",
    "phase_list = ['Ordered', 'Critical', 'Chaotic']\n",
    "\n",
    "layer_num = 50\n",
    "num_classes = 10\n",
    "\n",
    "kernel_list = []\n",
    "fx_train_0_list = []\n",
    "fx_test_0_list = []\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "b_std = np.sqrt(variance_b)\n",
    "\n",
    "for w_std in np.sqrt(variance_w):\n",
    "    init_fn, apply_fn, kernel_fn = stax.serial(WideDenseGroup(layer_num=layer_num, output_dim=1024, \n",
    "                                                              W_std=w_std, b_std=b_std), \n",
    "                                               stax.Dense(num_classes, W_std=w_std, b_std=b_std))\n",
    "    \n",
    "    # Inference with a single infinite width / linearized network\n",
    "    apply_fn = jit(apply_fn)\n",
    "    kernel_fn = jit(kernel_fn, static_argnums=(2,))\n",
    "    kernel_list.append(kernel_fn)\n",
    "    \n",
    "#     _, params = init_fn(key, (-1, x_train.shape[1]))\n",
    "#     fx_train_0_list.append(apply_fn(params, x_train))\n",
    "#     fx_test_0_list.append(apply_fn(params, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for phase, kernel in zip(phase_list, kernel_list):\n",
    "#     _, fx_test = model_mse_fn(kernel, 'test', x_train, x_test)\n",
    "#     acc = accuracy(fx_test, y_test)\n",
    "#     print(\"Clean Acc({:s}): {:.2f}\".format(phase, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attck gerneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 'chaotic'\n",
    "n = x_train.shape[0]\n",
    "w = n**2 - n\n",
    "w = 1e4\n",
    "d = 1\n",
    "eye = np.eye(n)\n",
    "loss_weighting = np.where(eye==0, 1/w, eye) * d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_type = [\"Clean\", \"FGSM\", \"PGD-10\", \"PGD-100\"]\n",
    "x_test_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diag/off-diag: 1.00000/0.00010\n",
      "Robustness(Clean): 0.85\n",
      "Robustness(FGSM): 0.19\n",
      "Robustness(PGD-10): 0.13\n",
      "Robustness(PGD-100): 0.13\n"
     ]
    }
   ],
   "source": [
    "x_test_list.append(x_test)\n",
    "\n",
    "# FGSM\n",
    "x_test_list.append(fast_gradient_method(model_fn=model_mse_fn, kernel_fn=kernel_list[1], obj_fn='test', \n",
    "                                         grads_fn=test_mse_grads_fn, loss_weighting=loss_weighting, phase=phase, \n",
    "                                         x_train=x_train, x_test=x_test, eps=0.3, clip_min=0, clip_max=1, y=None, \n",
    "                                         targeted=False))\n",
    "\n",
    "# PGD 10\n",
    "key, new_key = random.split(key)\n",
    "x_test_list.append(projected_gradient_descent(model_fn=model_mse_fn, kernel_fn=kernel_list[1], obj_fn='test', \n",
    "                                               grads_fn=test_mse_grads_fn, loss_weighting=loss_weighting, phase=phase,\n",
    "                                               x_train=x_train, x_test=x_test, eps=0.3, eps_iter=0.04, nb_iter=10, \n",
    "                                               clip_min=0, clip_max=1, y=None, targeted=False, rand_init=True, \n",
    "                                               rand_minmax=0.3))\n",
    "\n",
    "# PGD 100\n",
    "key, new_key = random.split(key)\n",
    "x_test_list.append(projected_gradient_descent(model_fn=model_mse_fn, kernel_fn=kernel_list[1], obj_fn='test', \n",
    "                                               grads_fn=test_mse_grads_fn, loss_weighting=loss_weighting, phase=phase,\n",
    "                                               x_train=x_train, x_test=x_test, eps=0.3, eps_iter=0.004, nb_iter=100, \n",
    "                                               clip_min=0, clip_max=1, y=None, targeted=False, rand_init=True, \n",
    "                                               rand_minmax=0.3))\n",
    "\n",
    "# Evaluation\n",
    "print(\"diag/off-diag: {:.5f}/{:.5f}\".format(loss_weighting[0][0], loss_weighting[0][1]))\n",
    "\n",
    "for idx, _x_test in enumerate(x_test_list):\n",
    "    _, fx_adv_test_t = model_mse_fn(kernel_list[1], 'test', x_train, _x_test)\n",
    "    acc = accuracy(fx_adv_test_t, y_test)\n",
    "    print(\"Robustness({:s}): {:.2f}\".format(attack_type[idx], acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4a7123a8d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[0].reshape((28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4a7c2b8e80>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAPPklEQVR4nO3df4xV9ZnH8c9THBBGbcEfdPhRUUp3C92IZoI/qi7G1h80WzTZuGUbYruu2EY3bbbZaN1k62azWWNWTJs2TbGSUks1zYqVZl2VztKgsWUdXVTQbUEWrPwQLbgotgj47B9zMKPO+Z7hnnvuOczzfiWTe+957pnz5DAfzr33e8/5mrsLwMj3gbobANAZhB0IgrADQRB2IAjCDgRxTCc3NtrG+LHq7uQmwzt0Yrn9Pep3+9rUCTrhD9qnt3y/DVUrFXYzu0zSNyWNkvR9d7819fxj1a2z7eIym8QReu0z55Za/0N3/7JNnaAT1npfbq3ll/FmNkrSdyRdLmmmpAVmNrPV3wegWmXes8+RtMndN7v7W5LulTS/PW0BaLcyYZ8s6beDHr+ULXsXM1tkZv1m1n9A+0tsDkAZlX8a7+5L3L3X3Xu7NKbqzQHIUSbs2yRNHfR4SrYMQAOVCfsTkmaY2WlmNlrS5yStbE9bANqt5aE3dz9oZjdIelgDQ29L3X1D2zrDUeG1ha0P7TGs11mlxtnd/UFJD7apFwAV4uuyQBCEHQiCsANBEHYgCMIOBEHYgSCsk1eXPcEmeF2nuBaNB9c55ltmrBr1aOp3BNZ6n/b67iHPZ+fIDgRB2IEgCDsQBGEHgiDsQBCEHQgizNBbWU0eHmvqMFCRJu/Tsur6N2HoDQBhB6Ig7EAQhB0IgrADQRB2IAjCDgQRZpy9yae4onmq/g5AVX9vjLMDIOxAFIQdCIKwA0EQdiAIwg4EQdiBIErN4tppI/n8Z7zfC8vPTNannrI7WV8964Hc2gU3XJdct2gcvOzfYmr9qsbgS4XdzLZIel3SIUkH3b23HU0BaL92HNkvcvdX2/B7AFSI9+xAEGXD7pIeMbMnzWzRUE8ws0Vm1m9m/Qe0v+TmALSq7Mv48919m5mdImmVmf2Pu68Z/AR3XyJpiTRwIkzJ7QFoUakju7tvy253Sbpf0px2NAWg/VoOu5l1m9nxh+9LukTS+nY1BqC9yryMnyjpfjM7/Ht+7O4PtaWrCnC++tAe3r6u1Ppz//raZH37hfl/Yt0f35Nc99Tu3yXrfTNXJuspj377e+knfDtdvnRSut7E74S0HHZ33yzpjDb2AqBCDL0BQRB2IAjCDgRB2IEgCDsQxFF1iiuOXNmhtSK/+P6dLa87/T+/mKx/eMGLyfrlOi9Z/49Njx9xTyMZR3YgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCCLMOPvRPGXzxm+dnaxv/vOC0zUrNOfrX07WP7TxzdzaRx//7+S6b7fUUXv8dN9xNW69GhzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiCIMOPsRZp46d/DFv7pmuInVeTSSbOT9fFq7vcTynj6zY/Utu2qvhPCkR0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgmCc/SiwYnN6stwvjV+bW1v86gXJddfccU564wvT5TqvA3DMtPRY+LlPT8+t/fKM+5LrPn7G6JZ6arLCI7uZLTWzXWa2ftCyCWa2ysw2Zrfjq20TQFnDeRn/A0mXvWfZTZL63H2GpL7sMYAGKwy7u6+RtPs9i+dLWpbdXybpijb3BaDNWn3PPtHdd2T3d0qamPdEM1skaZEkHatxLW4OQFmlP413d5fkifoSd+91994ujSm7OQAtajXsL5tZjyRlt7va1xKAKrQa9pWSrs7uXy3pgfa0A6Aqhe/ZzeweSXMlnWRmL0n6hqRbJf3EzK6RtFXSVVU2Gd24FR9M1q9Y8Xcd6uT96rwe/wtfnJKs9xyzrbJtV3n9g6r2WWHY3X1BTuniNvcCoEJ8XRYIgrADQRB2IAjCDgRB2IEgOMUVRy3/4zeS9dWz8r/+8YlffT65bvfCE1rqqck4sgNBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIyzo7Ee3r6u4BlF9XxdfenThhMXXzpqcWQHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAYZ++APTPT9T+7JH/KZUm6veepNnbTWSv/IX/Kr892v1nptk/72bW5tVP2jrxx9CIc2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMbZh2nflXtza+vPWV7pti/aML+y3526tno73LHl0/m1gnWLejur/y+S9VMeq+/Pu8qpqltVeGQ3s6VmtsvM1g9adouZbTOzddnPvGrbBFDWcF7G/0DSZUMsv8PdZ2c/D7a3LQDtVhh2d18jaXcHegFQoTIf0N1gZs9kL/PH5z3JzBaZWb+Z9R/Q/hKbA1BGq2H/rqTpkmZL2iHp9rwnuvsSd+91994ujWlxcwDKains7v6yux9y97cl3SlpTnvbAtBuLYXdzHoGPbxS0vq85wJohsKBSDO7R9JcSSeZ2UuSviFprpnN1sDFtbdIuq7CHt+RGrt8beG5La8rSTYm/RbDFkzNrRWNg2/fnb5GuRecWj1pWbq3Y35/KLc26hfpc+Ev1exk/f8+f06yPvYLO5L1Ku3ZmZ5D/WMNHOuuU2HY3X3BEIvvqqAXABXi67JAEIQdCIKwA0EQdiAIwg4EMWJOcS0aWvvAuPxLGkvSnn+blKyP7TqQW9v3o/S605Y1dwjo0NyzkvU9n01f7nlsO5s5Qss+dWey/s8Fw4rRcGQHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSBGzDh7kf+9MT3m2tO1LVlPjaWPb/A4+t6/TJ+iOu6vtifrv565MlmfcfeXk/XTb8zfNzv+9rzkupqVLl94bLr+rUdPyq29fsGr6ZVHII7sQBCEHQiCsANBEHYgCMIOBEHYgSAIOxCEedF1jNvoBJvgZ9vFHdveYAd//pFkffOmDyfrH/vSf7WznSNyzOT0+fLP/WN+/fRpu5LrvnmgK1kfe0fuzF6SpK5H+pP1Mh7evq6y333ppJF5rvta79Ne321D1TiyA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQI+Z89qIx2Ys2nJqsT7/3YDvbeZdRHz0tWX/xtvQ17U8+/o1kfZpeya3t7JuSXHfKvzyerNepaCy8+N88fyrt7239cXLdvzn1k8l6kbJTiFeh8MhuZlPNbLWZPWdmG8zsK9nyCWa2ysw2Zrfpb18AqNVwXsYflPQ1d58p6RxJ15vZTEk3Sepz9xmS+rLHABqqMOzuvsPdn8ruvy7peUmTJc2XtCx72jJJV1TVJIDyjug9u5lNk3SmpLWSJrr7jqy0U9LEnHUWSVokSccq/d4UQHWG/Wm8mR0n6T5JX3X3vYNrPnA2zZBn1Lj7EnfvdffeLo0p1SyA1g0r7GbWpYGgL3f3Fdnil82sJ6v3SEqfXgWgVoUv483MJN0l6Xl3XzyotFLS1ZJuzW4fqKTDDtk2N31d4kOX5Q+ldH98T3LdCd3paY/H7X8rWV89K71rU0NUU7Q1uW7REFFZVQ4xvfl2mf3WnVz3heVnJusnPlRwHesGGs579k9KWijpWTM7PLB5swZC/hMzu0bSVklXVdMigHYoDLu7PyZpyJPhJdVzJQoAR4yvywJBEHYgCMIOBEHYgSAIOxDEiDnF9Vd/OJSsF41VX6T80yElaetv8i81Pe5HH0yuO/rfX0zWX73tT5L1Sz/T+mWPqx5Hr3P7F309/buX/tPi3Nqs0WOT6877o/XJ+tqHepP1JuLIDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBhJmyuaxd15+XWzvlO+nLMVd9WeG6x9Kb6vcn552sKfUsLncJ7SZeKlpiymYAIuxAGIQdCIKwA0EQdiAIwg4EQdiBIMKMszMWjXaqaxy9COPsAAg7EAVhB4Ig7EAQhB0IgrADQRB2IIjhzM8+VdIPJU2U5JKWuPs3zewWSddKeiV76s3u/mBVjaIaRePFfD9h5BjOJBEHJX3N3Z8ys+MlPWlmq7LaHe7+r9W1B6BdhjM/+w5JO7L7r5vZ85ImV90YgPY6ovfsZjZN0pmS1maLbjCzZ8xsqZmNz1lnkZn1m1n/Ae0v1SyA1g077GZ2nKT7JH3V3fdK+q6k6ZJma+DIf/tQ67n7EnfvdffeLo1pQ8sAWjGssJtZlwaCvtzdV0iSu7/s7ofc/W1Jd0qaU12bAMoqDLuZmaS7JD3v7osHLe8Z9LQrJaWnvQRQq8JTXM3sfEmPSnpW0tvZ4pslLdDAS3iXtEXSddmHebmO5ktJVzkE1dTTJYejzqG5o3m/VSV1iutwPo1/TNJQKzOmDhxF+AYdEARhB4Ig7EAQhB0IgrADQRB2IIjhnPUG5GKs++jBkR0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgujolM1m9oqkrYMWnSTp1Y41cGSa2ltT+5LorVXt7O1Udz95qEJHw/6+jZv1u3tvbQ0kNLW3pvYl0VurOtUbL+OBIAg7EETdYV9S8/ZTmtpbU/uS6K1VHemt1vfsADqn7iM7gA4h7EAQtYTdzC4zs1+b2SYzu6mOHvKY2RYze9bM1plZf829LDWzXWa2ftCyCWa2ysw2ZrdDzrFXU2+3mNm2bN+tM7N5NfU21cxWm9lzZrbBzL6SLa913yX66sh+6/h7djMbJek3kj4t6SVJT0ha4O7PdbSRHGa2RVKvu9f+BQwzu1DSG5J+6O6fyJbdJmm3u9+a/Uc53t1vbEhvt0h6o+5pvLPZinoGTzMu6QpJX1CN+y7R11XqwH6r48g+R9Imd9/s7m9JulfS/Br6aDx3XyNp93sWz5e0LLu/TAN/LB2X01sjuPsOd38qu/+6pMPTjNe67xJ9dUQdYZ8s6beDHr+kZs337pIeMbMnzWxR3c0MYeKgabZ2SppYZzNDKJzGu5PeM814Y/ZdK9Ofl8UHdO93vrufJelySddnL1cbyQfegzVp7HRY03h3yhDTjL+jzn3X6vTnZdUR9m2Spg56PCVb1gjuvi273SXpfjVvKuqXD8+gm93uqrmfdzRpGu+hphlXA/ZdndOf1xH2JyTNMLPTzGy0pM9JWllDH+9jZt3ZBycys25Jl6h5U1GvlHR1dv9qSQ/U2Mu7NGUa77xpxlXzvqt9+nN37/iPpHka+ET+BUl/X0cPOX2dLunp7GdD3b1JukcDL+sOaOCzjWsknSipT9JGST+XNKFBvd2tgam9n9FAsHpq6u18DbxEf0bSuuxnXt37LtFXR/YbX5cFguADOiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0I4v8BAPu7CHPD+ZQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test_list[1][0].reshape((28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## questions\n",
    "\n",
    "why y_train = None ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
