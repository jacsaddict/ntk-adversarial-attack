{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "\n",
    "from jax import lax, random\n",
    "from jax.api import grad, jit, vmap\n",
    "from jax.config import config\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "config.update('jax_enable_x64', True)\n",
    "\n",
    "from functools import partial\n",
    "from jax import random\n",
    "\n",
    "from neural_tangents import stax\n",
    "\n",
    "# Attacking\n",
    "from jax.experimental.stax import logsoftmax\n",
    "from cleverhans.utils import clip_eta, one_hot\n",
    "\n",
    "# Plotting\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from utils import *\n",
    "sns.set_style(style='white')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "\"\"\"\n",
    "diag_reg:\n",
    "    a scalar representing the strength of the diagonal regularization for\n",
    "    `k_train_train`, i.e. computing `k_train_train + diag_reg * I` during\n",
    "    Cholesky factorization or eigendecomposition.\n",
    "\"\"\"\n",
    "diag_reg = 1e-5\n",
    "batch_size = 30\n",
    "\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_all, y_train_all, x_test_all, y_test_all = tuple(onp.array(x) for x in get_dataset('mnist', None, None, \n",
    "                                                                                  do_flatten_and_normalize=False))\n",
    "class_num = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "seed = 0\n",
    "x_train_all, y_train_all = shaffle(x_train_all, y_train_all, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# down sample\n",
    "\n",
    "train_size = 512\n",
    "x_train = x_train_all[:train_size]\n",
    "y_train = y_train_all[:train_size]\n",
    "\n",
    "test_size = 512\n",
    "x_test = x_test_all[:test_size]\n",
    "y_test = y_test_all[:test_size]\n",
    "\n",
    "# shape = (x_train.shape[0], 32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_id = jax.devices()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to gpu\n",
    "\n",
    "x_train = jax.device_put(x_train, device=device_id)\n",
    "y_train = jax.device_put(y_train, device=device_id)\n",
    "\n",
    "x_test = jax.device_put(x_test, device=device_id)\n",
    "y_test = jax.device_put(y_test, device=device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(mean, ys):\n",
    "    return np.mean(np.argmax(mean, axis=-1) == np.argmax(ys, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DenseBlock(neurons, W_std, b_std):\n",
    "    return stax.serial(stax.Dense(neurons, W_std, b_std), \n",
    "                       stax.Erf())\n",
    "\n",
    "def DenseGroup(n, neurons, W_std, b_std):\n",
    "    blocks = []\n",
    "    for _ in range(n):\n",
    "        blocks += [DenseBlock(neurons, W_std, b_std)]\n",
    "    # final layer    \n",
    "    blocks += [stax.Dense(class_num, W_std, b_std)]\n",
    "    return stax.serial(*blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 0.18\n",
    "W = 1.76\n",
    "\n",
    "phase_list = 'Critical'\n",
    "\n",
    "layer = 3\n",
    "# num_classes = 10\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "b_std = np.sqrt(b)\n",
    "W_std = np.sqrt(W)\n",
    "\n",
    "init_fn, apply_fn, kernel_fn = stax.serial(DenseGroup(layer, 1024, W_std, b_std))\n",
    "    \n",
    "# Inference with a single infinite width / linearized network\n",
    "# kernel_fn(x_train, x_test, 'ntk' or 'nngp')\n",
    "kernel_fn = jit(kernel_fn, static_argnums=(2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(kernel_fn, obj_fn, x_train=None, x_test=None, fx_train_0=0., fx_test_0=0., t=None):\n",
    "    # Kernel\n",
    "    ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "    \n",
    "    if obj_fn == 'train':\n",
    "        return ntk_train_train\n",
    "    elif obj_fn == 'test':\n",
    "        ntk_test_train = kernel_fn(x_test, x_train, 'ntk')\n",
    "        # Prediction\n",
    "        predict_fn = nt.predict.gradient_descent_mse(ntk_train_train, y_train, diag_reg=diag_reg) # no convariance\n",
    "        return predict_fn(t, fx_train_0, fx_test_0, ntk_test_train) # fx_train_0, fx_test_0 = (0, 0) for infinite width\n",
    "    else:\n",
    "        raise ValueError(\"Objective function must be either train(ntk_train_train) or test(predict_fn)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv(k):\n",
    "        #inverse with diag_reg\n",
    "        return np.linalg.inv(k + diag_reg * np.eye(k.shape[0]))\n",
    "\n",
    "ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "ntk_train_train_inv = inv(ntk_train_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def l2_loss_v1(logits, labels, weighting=1):\n",
    "    \"\"\"\n",
    "    Tensorflow version of L2 loss (without sqrt)\n",
    "    \"\"\"\n",
    "    return np.sum(((logits - labels)**2) * weighting) / 2\n",
    "    \n",
    "@jit\n",
    "def l2_loss_v2(logits, lables):\n",
    "    \"\"\"\n",
    "    Normal L2 loss\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(logits - labels)\n",
    "\n",
    "@jit\n",
    "def cross_entropy_loss(logits, lables):\n",
    "    return -np.sum(logsoftmax(logits) * lables)\n",
    "    \n",
    "@jit\n",
    "def mse_loss(logits, lables):\n",
    "    return 0.5 * np.mean((logits - lables) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss_adv_matrix(x_train, x_test, kernel_fn, c, t=None, \n",
    "                         ntk_train_train_inv=ntk_train_train_inv):\n",
    "    # Kernel -> matrix of constant c\n",
    "    assert type(c) == int\n",
    "    \n",
    "    ntk_test_train = kernel_fn(x_test, x_train, 'ntk')\n",
    "    # Θ(test, train)Θ(train, train)^-1\n",
    "    # def inv(k):\n",
    "        #inverse with diag_reg\n",
    "    #    return np.linalg.inv(k + diag_reg * np.eye(k.shape[0]))\n",
    "    \n",
    "    mean_predictor = np.einsum('ij,jk->ik', ntk_test_train, ntk_train_train_inv)\n",
    "    \n",
    "    # Loss\n",
    "    loss = - l2_loss_v1(mean_predictor, np.ones_like(mean_predictor)*c)\n",
    "    return loss\n",
    "\n",
    "# grad for x_test\n",
    "test_grads_fn = jit(grad(test_loss_adv_matrix, argnums=1), static_argnums=(0, 2))\n",
    "test_c_grads_fn = jit(grad(test_loss_adv_matrix, argnums=3), static_argnums=(0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss_adv_col(x_train, x_test, kernel_fn, c, t=None,\n",
    "                     ntk_train_train_inv=ntk_train_train_inv):\n",
    "    \"\"\"\n",
    "    Kernel -> matrix with constant cols\n",
    "    \n",
    "    c is a vector of constant. c.shape should be (1, test_size)\n",
    "    \n",
    "    \"\"\" \n",
    "    assert c.shape[0] == 1\n",
    "    assert len(c.shape) == 2\n",
    "    \n",
    "    # ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "    ntk_test_train = kernel_fn(x_test, x_train, 'ntk')\n",
    "    \n",
    "    # Θ(test, train)Θ(train, train)^-1\n",
    "    # inverse with diag_reg\n",
    "    # def inv(k):\n",
    "    #    return np.linalg.inv(k + diag_reg * np.eye(k.shape[0]))\n",
    "    \n",
    "    mean_predictor = np.einsum('ij,jk->ik', ntk_test_train, ntk_train_train_inv)\n",
    "    \n",
    "    # Loss\n",
    "    loss = - l2_loss_v1(mean_predictor, np.ones_like(mean_predictor)*c.T)\n",
    "    return loss\n",
    "\n",
    "# grad for x_test\n",
    "test_col_wise_grads_fn = jit(grad(test_loss_adv_col, argnums=1), static_argnums=(2,))\n",
    "test_col_wise_c_grads_fn = jit(grad(test_loss_adv_col, argnums=3), static_argnums=(2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss_adv_row(x_train, x_test, kernel_fn, c, t=None,\n",
    "                     ntk_train_train_inv=ntk_train_train_inv):\n",
    "    \"\"\"\n",
    "    Kernel -> matrix with constant rows\n",
    "    \n",
    "    c is a vector of constant. c.shape should be (1, train_size)\n",
    "    \n",
    "    \"\"\" \n",
    "    assert c.shape[0] == 1\n",
    "    assert len(c.shape) == 2\n",
    "    \n",
    "    # ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "    ntk_test_train = kernel_fn(x_test, x_train, 'ntk')\n",
    "    \n",
    "    # Θ(test, train)Θ(train, train)^-1\n",
    "    # inverse with diag_reg\n",
    "    # def inv(k):\n",
    "    #    return np.linalg.inv(k + diag_reg * np.eye(k.shape[0]))\n",
    "    \n",
    "    mean_predictor = np.einsum('ij,jk->ik', ntk_test_train, ntk_train_train_inv)\n",
    "    \n",
    "    # Loss\n",
    "    loss = - l2_loss_v1(mean_predictor, np.ones_like(mean_predictor)*c)\n",
    "    return loss\n",
    "\n",
    "# grad for x_test\n",
    "test_row_wise_grads_fn = jit(grad(test_loss_adv_row, argnums=1), static_argnums=(2,))\n",
    "test_row_wise_c_grads_fn = jit(grad(test_loss_adv_row, argnums=3), static_argnums=(2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.array([\n",
    "#     [1, 1, 1],\n",
    "#     [1, 1, 1]\n",
    "# ], dtype=np.float32)\n",
    "\n",
    "# b_col = np.array([\n",
    "#     [0.1, 10.]\n",
    "# ], dtype=np.float32)\n",
    "\n",
    "# b_row = np.array([\n",
    "#     [0.1, 10., -1]\n",
    "# ], dtype=np.float32)\n",
    "\n",
    "# print(a*b_col.T)\n",
    "# print()\n",
    "# print(a*b_row)\n",
    "\n",
    "# a.shape = (2, 3) test_size=2, train_size=3\n",
    "# col broadcast -> b.shape = (1, 2), a*b.T\n",
    "# row broadcast -> b.shape = (1, 3), a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.shape = (test, train)\n",
    "# col broadcast -> b.shape = (1, test), a*b.T\n",
    "# row broadcast -> b.shape = (1, train), a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, grads_c_fn=None, x_train=None, y_train=None, x_test=None, \n",
    "                         y=None, t=None, c=None, update_c=False, loss_weighting=None, phase=None, \n",
    "                         fx_train_0=0., fx_test_0=0., eps=0.3, norm=np.inf, clip_min=None, clip_max=None, targeted=False):\n",
    "    \"\"\"\n",
    "    JAX implementation of the Fast Gradient Method.\n",
    "    :param model_fn: a callable that takes an input tensor and returns the model logits.\n",
    "    :param x: input tensor.\n",
    "    :param eps: epsilon (input variation parameter); see https://arxiv.org/abs/1412.6572.\n",
    "    :param norm: Order of the norm (mimics NumPy). Possible values: np.inf or 2.\n",
    "    :param clip_min: (optional) float. Minimum float value for adversarial example components.\n",
    "    :param clip_max: (optional) float. Maximum float value for adversarial example components.\n",
    "    :param y: (optional) Tensor with one-hot true labels. If targeted is true, then provide the\n",
    "            target one-hot label. Otherwise, only provide this parameter if you'd like to use true\n",
    "            labels when crafting adversarial samples. Otherwise, model predictions are used\n",
    "            as labels to avoid the \"label leaking\" effect (explained in this paper:\n",
    "            https://arxiv.org/abs/1611.01236). Default is None. This argument does not have\n",
    "            to be a binary one-hot label (e.g., [0, 1, 0, 0]), it can be floating points values\n",
    "            that sum up to 1 (e.g., [0.05, 0.85, 0.05, 0.05]).\n",
    "    :param targeted: (optional) bool. Is the attack targeted or untargeted?\n",
    "            Untargeted, the default, will try to make the label incorrect.\n",
    "            Targeted will instead try to move in the direction of being more like y.\n",
    "    :return: a tensor for the adversarial example\n",
    "    \"\"\"\n",
    "    if norm not in [np.inf, 2]:\n",
    "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
    "        \n",
    "    x = x_test\n",
    "        \n",
    "    # Obj - Θ(test, train)Θ(train, train)^-1\n",
    "    # test independent\n",
    "    if obj_fn == 'test_col':\n",
    "        grads_c = 0\n",
    "        grads = grads_fn(x_train, x_test, kernel_fn, c)\n",
    "        if update_c is True:\n",
    "            grads_c += grads_c_fn(x_train, x_test, kernel_fn, c)\n",
    "                \n",
    "        grads_c = 1e-5 * np.sign(grads_c) # grads_c = 5e-2 * np.sign(grads_c)\n",
    "        \n",
    "        \n",
    "    # Obj - Θ(test, train)Θ(train, train)^-1 y_train\n",
    "    else:\n",
    "        raise ValueError(\"Objective function must be either train(ntk_train_train) or test(predict_fn)\")\n",
    "\n",
    "    axis = list(range(1, len(grads.shape)))\n",
    "    eps_div = 1e-12\n",
    "    \n",
    "    if norm == np.inf:\n",
    "        perturbation = eps * np.sign(grads)\n",
    "    elif norm == 1:\n",
    "        raise NotImplementedError(\"L_1 norm has not been implemented yet.\")\n",
    "    elif norm == 2:\n",
    "        square = np.maximum(eps_div, np.sum(np.square(grads), axis=axis, keepdims=True))\n",
    "        perturbation = grads / np.sqrt(square)\n",
    "    \n",
    "    # TODO\n",
    "    adv_x = x + perturbation\n",
    "    \n",
    "    # If clipping is needed, reset all values outside of [clip_min, clip_max]\n",
    "    if (clip_min is not None) or (clip_max is not None):\n",
    "        # We don't currently support one-sided clipping\n",
    "        assert clip_min is not None and clip_max is not None\n",
    "        adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "    \n",
    "    if obj_fn == 'test_col':\n",
    "        c += grads_c\n",
    "    \n",
    "    return adv_x, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_gradient_descent(model_fn, kernel_fn, obj_fn, grads_fn, grads_c_fn=None, x_train=None, y_train=None,\n",
    "                               x_test=None, y=None, t=None, c=None, update_c=None, loss_weighting=None, \n",
    "                               phase=None, fx_train_0=0., fx_test_0=0., eps=0.3, eps_iter=0.03, nb_iter=10, norm=np.inf, \n",
    "                               clip_min=None, clip_max=None, targeted=False, rand_init=None, rand_minmax=0.3):\n",
    "    \"\"\"\n",
    "    This class implements either the Basic Iterative Method\n",
    "    (Kurakin et al. 2016) when rand_init is set to 0. or the\n",
    "    Madry et al. (2017) method when rand_minmax is larger than 0.\n",
    "    Paper link (Kurakin et al. 2016): https://arxiv.org/pdf/1607.02533.pdf\n",
    "    Paper link (Madry et al. 2017): https://arxiv.org/pdf/1706.06083.pdf\n",
    "    :param model_fn: a callable that takes an input tensor and returns the model logits.\n",
    "    :param x: input tensor.\n",
    "    :param eps: epsilon (input variation parameter); see https://arxiv.org/abs/1412.6572.\n",
    "    :param eps_iter: step size for each attack iteration\n",
    "    :param nb_iter: Number of attack iterations.\n",
    "    :param norm: Order of the norm (mimics NumPy). Possible values: np.inf or 2.\n",
    "    :param clip_min: (optional) float. Minimum float value for adversarial example components.\n",
    "    :param clip_max: (optional) float. Maximum float value for adversarial example components.\n",
    "    :param y: (optional) Tensor with true labels. If targeted is true, then provide the\n",
    "            target label. Otherwise, only provide this parameter if you'd like to use true\n",
    "            labels when crafting adversarial samples. Otherwise, model predictions are used\n",
    "            as labels to avoid the \"label leaking\" effect (explained in this paper:\n",
    "            https://arxiv.org/abs/1611.01236). Default is None.\n",
    "    :param targeted: (optional) bool. Is the attack targeted or untargeted?\n",
    "            Untargeted, the default, will try to make the label incorrect.\n",
    "            Targeted will instead try to move in the direction of being more like y.\n",
    "    :return: a tensor for the adversarial example\n",
    "    \"\"\"\n",
    "\n",
    "    assert eps_iter <= eps, (eps_iter, eps)\n",
    "    if norm == 1:\n",
    "        raise NotImplementedError(\"It's not clear that FGM is a good inner loop\"\n",
    "                                  \" step for PGD when norm=1, because norm=1 FGM \"\n",
    "                                  \" changes only one pixel at a time. We need \"\n",
    "                                  \" to rigorously test a strong norm=1 PGD \"\n",
    "                                  \"before enabling this feature.\")\n",
    "    if norm not in [np.inf, 2]:\n",
    "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
    "        \n",
    "    x = x_test\n",
    "    \n",
    "    # Initialize loop variables\n",
    "    if rand_init:\n",
    "        rand_minmax = eps\n",
    "        eta = random.uniform(new_key, x.shape, minval=-rand_minmax, maxval=rand_minmax)\n",
    "    else:\n",
    "        eta = np.zeros_like(x)\n",
    "\n",
    "    # Clip eta\n",
    "    eta = clip_eta(eta, norm, eps)\n",
    "    adv_x = x + eta\n",
    "    if clip_min is not None or clip_max is not None:\n",
    "        adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "        \n",
    "    for i in range(nb_iter):\n",
    "        if update_c is not None and (i+1) % update_c == 0:\n",
    "            adv_x, c = fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, grads_c_fn, x_train, y_train, adv_x, \n",
    "                                            y, t, c, True, loss_weighting, phase, fx_train_0, fx_test_0, eps_iter, norm, \n",
    "                                            clip_min, clip_max, targeted)\n",
    "        else:\n",
    "            adv_x, c = fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, grads_c_fn, x_train, y_train, adv_x, \n",
    "                                            y, t, c, False, loss_weighting, phase, fx_train_0, fx_test_0, eps_iter, norm, \n",
    "                                            clip_min, clip_max, targeted)\n",
    "\n",
    "        # Clipping perturbation eta to norm norm ball\n",
    "        eta = adv_x - x\n",
    "        eta = clip_eta(eta, norm, eps)\n",
    "        adv_x = x + eta\n",
    "\n",
    "        # Redo the clipping.\n",
    "        # FGM already did it, but subtracting and re-adding eta can add some\n",
    "        # small numerical error.\n",
    "        if clip_min is not None or clip_max is not None:\n",
    "            adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "    \n",
    "    return adv_x, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_type = [\"Clean\", \"FGSM\", \"PGD-10\", \"PGD-100\"]\n",
    "\n",
    "####### MNIST #######\n",
    "eps = 0.3\n",
    "eps_iter_10 = (eps/10)*1.1\n",
    "eps_iter_100 = (eps/100)*1.1\n",
    "eps_iter_1000 = (eps/1000)*1.1\n",
    "####### MNIST #######\n",
    "\n",
    "####### CIFAR #######\n",
    "# eps = 16/255\n",
    "# eps_iter_10 = (eps/10)*1.1\n",
    "# eps_iter_100 = (eps/100)*1.1\n",
    "####### CIFAR #######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_predictor(x_train, x_test, kernel_fn, c=None, row=False, col=False, get_sample=False,\n",
    "                   ntk_train_train_inv=ntk_train_train_inv):\n",
    "    \"\"\"\n",
    "    return Θ(test, train)Θ(train, train)^-1 and \n",
    "    || # Θ(test, train)Θ(train, train)^-1 - target ||\n",
    "    \n",
    "    \"\"\"\n",
    "    if not row and not col:\n",
    "        raise ValueError(\"at least one of row or col should be true\")\n",
    "        \n",
    "    # Kernel\n",
    "    ntk_test_train = kernel_fn(x_test, x_train, 'ntk')\n",
    "    \n",
    "    # Θ(test, train)Θ(train, train)^-1\n",
    "    mean_predictor = np.einsum('ij,jk->ik', ntk_test_train, ntk_train_train_inv)\n",
    "    loss = 0.0\n",
    "    \n",
    "    if c is None:\n",
    "        if row ^ col:\n",
    "            if row:\n",
    "                c = np.mean(mean_predictor, axis=0)\n",
    "                c = np.reshape(c, (1, -1))\n",
    "                # print(c.shape)\n",
    "                # print(mean_predictor.shape)\n",
    "                loss = l2_loss_v1(mean_predictor, np.ones_like(mean_predictor)*c)\n",
    "            elif col:\n",
    "                c = np.mean(mean_predictor, axis=1)\n",
    "                c = np.reshape(c, (1, -1))\n",
    "                # print(c.shape)\n",
    "                # print(mean_predictor.shape)\n",
    "                loss = l2_loss_v1(mean_predictor, np.ones_like(mean_predictor)*c.T)\n",
    "        else:\n",
    "            raise ValueError(\"row xor col not true\")\n",
    "            \n",
    "    if get_sample:\n",
    "        return c\n",
    "    \n",
    "    return loss, mean_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(x_train, x_test, model_fn, kernel_fn, t=None, c=0, attack_type=None):\n",
    "    y_train_predict, y_test_predict = model_fn(kernel_fn, 'test', x_train, x_test, t=t)\n",
    "    acc = accuracy(y_test_predict, y_test)\n",
    "    print(\"Robustness({:s}): {:.2f}\".format(attack_type, acc))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix(adv_x, x_train=x_train, kernel_fn=kernel_fn):\n",
    "    ntk_test_train = kernel_fn(adv_x, x_train, 'ntk')\n",
    "    matrix = np.einsum('ij,jk->ik', ntk_test_train, ntk_train_train_inv)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train list gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = None\n",
    "d = 'col'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robustness(Clean): 0.88\n"
     ]
    }
   ],
   "source": [
    "evaluate(x_train, x_test, model_fn=model_fn, kernel_fn=kernel_fn, t=t, c=0, attack_type='Clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.03208873, dtype=float64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_clean = get_matrix(x_test)\n",
    "np.mean(np.std(m_clean, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if d == 'col':\n",
    "    c_sample = mean_predictor(x_train, x_test, kernel_fn, col=True, get_sample=True)\n",
    "elif d == 'row':\n",
    "    c_sample = mean_predictor(x_train, x_test, kernel_fn, row=True, get_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = c_sample.reshape((1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robustness(FGSM): 0.76\n"
     ]
    }
   ],
   "source": [
    "# FGSM\n",
    "adv_x_FGSM, c = fast_gradient_method(model_fn=model_fn, kernel_fn=kernel_fn, obj_fn='test_col', \n",
    "                                     grads_fn=test_col_wise_grads_fn, grads_c_fn=test_col_wise_c_grads_fn,\n",
    "                                     x_train=x_train, y_train=y_train, x_test=x_test, y=y_test, t=t, c=c,\n",
    "                                     eps=eps, clip_min=0, clip_max=1)\n",
    "\n",
    "evaluate(x_train, adv_x_FGSM, model_fn=model_fn, kernel_fn=kernel_fn, t=t, c=c, attack_type='FGSM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.03838172, dtype=float64)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_FGSM = get_matrix(adv_x_FGSM)\n",
    "np.mean(np.std(m_FGSM, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robustness(PGD-10): 0.89\n"
     ]
    }
   ],
   "source": [
    "# PGD 10\n",
    "key, new_key = random.split(key)\n",
    "c = c_sample.reshape((1, -1))\n",
    "adv_x_PGD_10 , c = projected_gradient_descent(model_fn=model_fn, kernel_fn=kernel_fn, obj_fn='test_col', \n",
    "                                              grads_fn=test_col_wise_grads_fn, grads_c_fn=test_col_wise_c_grads_fn,\n",
    "                                              x_train=x_train, y_train=y_train, x_test=x_test, y=y_test, t=t, c=c, update_c=3, \n",
    "                                              eps=eps, eps_iter=eps_iter_10, nb_iter=10, \n",
    "                                              clip_min=0, clip_max=1, rand_init=None, rand_minmax=eps)\n",
    "\n",
    "evaluate(x_train, adv_x_PGD_10, model_fn=model_fn, kernel_fn=kernel_fn, t=t, c=c, attack_type='PGD-10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.01525635, dtype=float64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_PGD_10 = get_matrix(adv_x_PGD_10)\n",
    "np.mean(np.std(m_PGD_10, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robustness(PGD-100): 0.84\n"
     ]
    }
   ],
   "source": [
    "# PGD 100\n",
    "key, new_key = random.split(key)\n",
    "c = c_sample.reshape((1, -1))\n",
    "adv_x_PGD_100, c = projected_gradient_descent(model_fn=model_fn, kernel_fn=kernel_fn, obj_fn='test_col', \n",
    "                                              grads_fn=test_col_wise_grads_fn, grads_c_fn=test_col_wise_c_grads_fn,\n",
    "                                              x_train=x_train, y_train=y_train, x_test=x_test, y=y_test, t=t, c=c, update_c=3, \n",
    "                                              eps=eps, eps_iter=eps_iter_100, nb_iter=100, \n",
    "                                              clip_min=0, clip_max=1, rand_init=None, rand_minmax=eps)\n",
    "\n",
    "evaluate(x_train, adv_x_PGD_100, model_fn=model_fn, kernel_fn=kernel_fn, t=t, c=c, attack_type='PGD-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.01139856, dtype=float64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_PGD_100 = get_matrix(adv_x_PGD_100)\n",
    "np.mean(np.std(m_PGD_100, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robustness(PGD-1000): 0.84\n"
     ]
    }
   ],
   "source": [
    "# PGD 1000\n",
    "key, new_key = random.split(key)\n",
    "c = c_sample.reshape((1, -1))\n",
    "adv_x_PGD_1000, c = projected_gradient_descent(model_fn=model_fn, kernel_fn=kernel_fn, obj_fn='test_col', \n",
    "                                              grads_fn=test_col_wise_grads_fn, grads_c_fn=test_col_wise_c_grads_fn,\n",
    "                                              x_train=x_train, y_train=y_train, x_test=x_test, y=y_test, t=t, c=c, update_c=3, \n",
    "                                              eps=eps, eps_iter=eps_iter_1000, nb_iter=1000, \n",
    "                                              clip_min=0, clip_max=1, rand_init=None, rand_minmax=eps)\n",
    "\n",
    "evaluate(x_train, adv_x_PGD_1000, model_fn=model_fn, kernel_fn=kernel_fn, t=t, c=c, attack_type='PGD-1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.01132659, dtype=float64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_PGD_1000 = get_matrix(adv_x_PGD_1000)\n",
    "np.mean(np.std(m_PGD_1000, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********\n",
      "layer:  0\n",
      "Robustness(Clean): 0.34\n",
      "Robustness(FGSM): 0.11\n",
      "Robustness(PGD-10): 0.15\n",
      "Robustness(PGD-100): 0.16\n",
      "Robustness(PGD-1000): 0.15\n",
      "***********\n",
      "layer:  1\n",
      "Robustness(Clean): 0.85\n",
      "Robustness(FGSM): 0.43\n",
      "Robustness(PGD-10): 0.75\n",
      "Robustness(PGD-100): 0.72\n",
      "Robustness(PGD-1000): 0.72\n",
      "***********\n",
      "layer:  2\n",
      "Robustness(Clean): 0.88\n",
      "Robustness(FGSM): 0.66\n",
      "Robustness(PGD-10): 0.87\n",
      "Robustness(PGD-100): 0.85\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  3\n",
      "Robustness(Clean): 0.88\n",
      "Robustness(FGSM): 0.76\n",
      "Robustness(PGD-10): 0.89\n",
      "Robustness(PGD-100): 0.84\n",
      "Robustness(PGD-1000): 0.84\n",
      "***********\n",
      "layer:  4\n",
      "Robustness(Clean): 0.90\n",
      "Robustness(FGSM): 0.78\n",
      "Robustness(PGD-10): 0.89\n",
      "Robustness(PGD-100): 0.84\n",
      "Robustness(PGD-1000): 0.82\n",
      "***********\n",
      "layer:  5\n",
      "Robustness(Clean): 0.90\n",
      "Robustness(FGSM): 0.80\n",
      "Robustness(PGD-10): 0.88\n",
      "Robustness(PGD-100): 0.83\n",
      "Robustness(PGD-1000): 0.81\n",
      "***********\n",
      "layer:  6\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.82\n",
      "Robustness(PGD-10): 0.89\n",
      "Robustness(PGD-100): 0.83\n",
      "Robustness(PGD-1000): 0.81\n",
      "***********\n",
      "layer:  7\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.83\n",
      "Robustness(PGD-10): 0.89\n",
      "Robustness(PGD-100): 0.83\n",
      "Robustness(PGD-1000): 0.82\n",
      "***********\n",
      "layer:  8\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.84\n",
      "Robustness(PGD-10): 0.90\n",
      "Robustness(PGD-100): 0.83\n",
      "Robustness(PGD-1000): 0.82\n",
      "***********\n",
      "layer:  9\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.85\n",
      "Robustness(PGD-10): 0.90\n",
      "Robustness(PGD-100): 0.83\n",
      "Robustness(PGD-1000): 0.83\n",
      "***********\n",
      "layer:  10\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.85\n",
      "Robustness(PGD-10): 0.90\n",
      "Robustness(PGD-100): 0.83\n",
      "Robustness(PGD-1000): 0.83\n",
      "***********\n",
      "layer:  11\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.85\n",
      "Robustness(PGD-10): 0.90\n",
      "Robustness(PGD-100): 0.83\n",
      "Robustness(PGD-1000): 0.83\n",
      "***********\n",
      "layer:  12\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.85\n",
      "Robustness(PGD-10): 0.90\n",
      "Robustness(PGD-100): 0.84\n",
      "Robustness(PGD-1000): 0.83\n",
      "***********\n",
      "layer:  13\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.85\n",
      "Robustness(PGD-10): 0.90\n",
      "Robustness(PGD-100): 0.84\n",
      "Robustness(PGD-1000): 0.83\n",
      "***********\n",
      "layer:  14\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.85\n",
      "Robustness(PGD-10): 0.90\n",
      "Robustness(PGD-100): 0.84\n",
      "Robustness(PGD-1000): 0.83\n",
      "***********\n",
      "layer:  15\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.86\n",
      "Robustness(PGD-10): 0.90\n",
      "Robustness(PGD-100): 0.84\n",
      "Robustness(PGD-1000): 0.83\n",
      "***********\n",
      "layer:  16\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.86\n",
      "Robustness(PGD-10): 0.90\n",
      "Robustness(PGD-100): 0.84\n",
      "Robustness(PGD-1000): 0.83\n",
      "***********\n",
      "layer:  17\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.86\n",
      "Robustness(PGD-10): 0.90\n",
      "Robustness(PGD-100): 0.84\n",
      "Robustness(PGD-1000): 0.83\n",
      "***********\n",
      "layer:  18\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.86\n",
      "Robustness(PGD-10): 0.90\n",
      "Robustness(PGD-100): 0.84\n",
      "Robustness(PGD-1000): 0.83\n",
      "***********\n",
      "layer:  19\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.86\n",
      "Robustness(PGD-10): 0.90\n",
      "Robustness(PGD-100): 0.84\n",
      "Robustness(PGD-1000): 0.84\n",
      "***********\n",
      "layer:  20\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.86\n",
      "Robustness(PGD-10): 0.90\n",
      "Robustness(PGD-100): 0.84\n",
      "Robustness(PGD-1000): 0.84\n",
      "***********\n",
      "layer:  21\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.86\n",
      "Robustness(PGD-10): 0.90\n",
      "Robustness(PGD-100): 0.84\n",
      "Robustness(PGD-1000): 0.84\n",
      "***********\n",
      "layer:  22\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.86\n",
      "Robustness(PGD-10): 0.90\n",
      "Robustness(PGD-100): 0.84\n",
      "Robustness(PGD-1000): 0.84\n",
      "***********\n",
      "layer:  23\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.86\n",
      "Robustness(PGD-10): 0.90\n",
      "Robustness(PGD-100): 0.84\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  24\n",
      "Robustness(Clean): 0.92\n",
      "Robustness(FGSM): 0.86\n",
      "Robustness(PGD-10): 0.90\n",
      "Robustness(PGD-100): 0.84\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  25\n",
      "Robustness(Clean): 0.92\n",
      "Robustness(FGSM): 0.86\n",
      "Robustness(PGD-10): 0.90\n",
      "Robustness(PGD-100): 0.84\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  26\n",
      "Robustness(Clean): 0.92\n",
      "Robustness(FGSM): 0.86\n",
      "Robustness(PGD-10): 0.90\n",
      "Robustness(PGD-100): 0.84\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  27\n",
      "Robustness(Clean): 0.92\n",
      "Robustness(FGSM): 0.86\n",
      "Robustness(PGD-10): 0.90\n",
      "Robustness(PGD-100): 0.85\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  28\n",
      "Robustness(Clean): 0.92\n",
      "Robustness(FGSM): 0.86\n",
      "Robustness(PGD-10): 0.90\n",
      "Robustness(PGD-100): 0.85\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  29\n",
      "Robustness(Clean): 0.92\n",
      "Robustness(FGSM): 0.86\n",
      "Robustness(PGD-10): 0.90\n",
      "Robustness(PGD-100): 0.85\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  30\n",
      "Robustness(Clean): 0.92\n",
      "Robustness(FGSM): 0.86\n",
      "Robustness(PGD-10): 0.90\n",
      "Robustness(PGD-100): 0.85\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  31\n",
      "Robustness(Clean): 0.92\n",
      "Robustness(FGSM): 0.86\n",
      "Robustness(PGD-10): 0.90\n",
      "Robustness(PGD-100): 0.85\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  32\n",
      "Robustness(Clean): 0.92\n",
      "Robustness(FGSM): 0.86\n",
      "Robustness(PGD-10): 0.89\n",
      "Robustness(PGD-100): 0.85\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  33\n",
      "Robustness(Clean): 0.92\n",
      "Robustness(FGSM): 0.86\n",
      "Robustness(PGD-10): 0.89\n",
      "Robustness(PGD-100): 0.85\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  34\n",
      "Robustness(Clean): 0.92\n",
      "Robustness(FGSM): 0.86\n",
      "Robustness(PGD-10): 0.89\n",
      "Robustness(PGD-100): 0.85\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  35\n",
      "Robustness(Clean): 0.92\n",
      "Robustness(FGSM): 0.86\n",
      "Robustness(PGD-10): 0.89\n",
      "Robustness(PGD-100): 0.85\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  36\n",
      "Robustness(Clean): 0.92\n",
      "Robustness(FGSM): 0.86\n",
      "Robustness(PGD-10): 0.89\n",
      "Robustness(PGD-100): 0.85\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  37\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.86\n",
      "Robustness(PGD-10): 0.89\n",
      "Robustness(PGD-100): 0.85\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  38\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.85\n",
      "Robustness(PGD-10): 0.89\n",
      "Robustness(PGD-100): 0.85\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  39\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.85\n",
      "Robustness(PGD-10): 0.89\n",
      "Robustness(PGD-100): 0.85\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  40\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.85\n",
      "Robustness(PGD-10): 0.89\n",
      "Robustness(PGD-100): 0.85\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  41\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.85\n",
      "Robustness(PGD-10): 0.89\n",
      "Robustness(PGD-100): 0.85\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  42\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.85\n",
      "Robustness(PGD-10): 0.89\n",
      "Robustness(PGD-100): 0.85\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  43\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.84\n",
      "Robustness(PGD-10): 0.89\n",
      "Robustness(PGD-100): 0.85\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  44\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.85\n",
      "Robustness(PGD-10): 0.89\n",
      "Robustness(PGD-100): 0.85\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  45\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.85\n",
      "Robustness(PGD-10): 0.89\n",
      "Robustness(PGD-100): 0.85\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  46\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.85\n",
      "Robustness(PGD-10): 0.89\n",
      "Robustness(PGD-100): 0.85\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  47\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.85\n",
      "Robustness(PGD-10): 0.89\n",
      "Robustness(PGD-100): 0.85\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  48\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.85\n",
      "Robustness(PGD-10): 0.89\n",
      "Robustness(PGD-100): 0.85\n",
      "Robustness(PGD-1000): 0.85\n",
      "***********\n",
      "layer:  49\n",
      "Robustness(Clean): 0.91\n",
      "Robustness(FGSM): 0.85\n",
      "Robustness(PGD-10): 0.89\n",
      "Robustness(PGD-100): 0.85\n",
      "Robustness(PGD-1000): 0.85\n"
     ]
    }
   ],
   "source": [
    "b = 0.18\n",
    "W = 1.76\n",
    "num_classes = 10\n",
    "\n",
    "# layer = 5\n",
    "clean_acc    = []\n",
    "fgsm_acc     = []\n",
    "pgd_10_acc   = []\n",
    "pgd_100_acc  = []\n",
    "pgd_1000_acc = []\n",
    "\n",
    "for layer in range(50):\n",
    "    print(\"***********\")\n",
    "    print(\"layer: \", layer)\n",
    "    key = random.PRNGKey(0)\n",
    "\n",
    "    b_std = np.sqrt(b)\n",
    "    W_std = np.sqrt(W)\n",
    "\n",
    "    init_fn, apply_fn, kernel_fn_trans = stax.serial(DenseGroup(layer, 1024, W_std, b_std))\n",
    "    kernel_fn_trans = jit(kernel_fn_trans, static_argnums=(2,))\n",
    "\n",
    "    ntk_train_train_trans = kernel_fn_trans(x_train, x_train, 'ntk')\n",
    "    ntk_train_train_inv_trans = inv(ntk_train_train_trans)\n",
    "\n",
    "    clean_acc.append(evaluate(x_train, x_test, model_fn=model_fn, kernel_fn=kernel_fn_trans, t=t, c=0, attack_type='Clean'))\n",
    "    fgsm_acc.append(evaluate(x_train, adv_x_FGSM, model_fn=model_fn, kernel_fn=kernel_fn_trans, t=t, c=0, attack_type='FGSM'))\n",
    "    pgd_10_acc.append(evaluate(x_train, adv_x_PGD_10, model_fn=model_fn, kernel_fn=kernel_fn_trans, t=t, c=0, attack_type='PGD-10'))\n",
    "    pgd_100_acc.append(evaluate(x_train, adv_x_PGD_100, model_fn=model_fn, kernel_fn=kernel_fn_trans, t=t, c=0, attack_type='PGD-100'))\n",
    "    pgd_100_acc.append(evaluate(x_train, adv_x_PGD_1000, model_fn=model_fn, kernel_fn=kernel_fn_trans, t=t, c=0, attack_type='PGD-1000'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd2f7543940>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[1].reshape((28 ,28)), cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd2efe196d0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(adv_x_FGSM[1].reshape((28, 28)), cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd2f7a7efa0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(adv_x_PGD_10[1].reshape((28, 28)), cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd2f6879100>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUXklEQVR4nO3de0zV5R8H8Ldgmok/yUMHEAGH4UzyUomFmiYom8MjDEebpUtGo5qDGa0SnW252aw2p7m1ifaHNTNaCTjN4tK81kRNRcwLXhAkLoqggBcEvr8/TJbF9/PY+Z4L+bxfm1vy7jnn4cDHL5zP93mePoZhGCCih56PtydARJ7BYifSBIudSBMsdiJNsNiJNNHXk082YsQIdHR0ePIpibTSt29fnDt3rufMygPv2bMHK1euRFdXF1JSUpCeni7+/x0dHaiqqrLylEQkCAsLMw8NJ3V0dBhxcXFGVVWVcfv2bcPhcBgVFRXimLCwMAMA//AP/7jpT1hYmGn9Of07e1lZGcLDwxEaGop+/fohISEBJSUlzj4cEbmZ08VeX1+PoKCg7r8HBgaivr7eJZMiItdzuth7usu2T58+liZDRO7jdLEHBQWhrq6u++/19fWw2+0umRQRuZ7TxT5mzBhUVlaiuroa7e3t2LFjB2JjY105NyJyIadbb3379sUHH3yA119/HZ2dnZg7dy4iIyNdOTf606BBg8S8paXFNAsICBDHvvDCC2JeWFgo5p2dnWI+a9Ys02z79u3iWJWZM2eKeUNDg2kWGhoqjrU6t97IUp992rRpmDZtmqvmQkRuxNtliTTBYifSBIudSBMsdiJNsNiJNMFiJ9KER9ez/5dNmDDBbY/d1NQk5mbrkx9EV1eX02MBoL29Xcwff/xxMXdnv/rChQtifvbsWdNM1Wd/GPHKTqQJFjuRJljsRJpgsRNpgsVOpAkWO5Em2Hr70+zZs50ea7W9pHru6upqMZeWmU6aNMmpOT0oVdtwyJAhpplqbqrXVWqtqfz0009iPmXKFDH39/cX8964RJZXdiJNsNiJNMFiJ9IEi51IEyx2Ik2w2Ik0wWIn0gT77A/InX3TH3/8Ucy/+OILMZeWmebl5YljfX19xVzaChoA+vfv7/TjX7x4URyr6mU3NzeL+ahRo0yzU6dOiWP37dsn5n89+swZ0r0V7vpe45WdSBMsdiJNsNiJNMFiJ9IEi51IEyx2Ik2w2Ik0wT77n6z0NlW96J07d4r5559/LuaqfrNhGKbZhx9+KI49cuSImEvr0QF1n15ai19UVCSOVX3edXV1Yt7R0WGa9e0rf+sPGDDA0nP3RpaKPTY2FgMHDoSPjw98fX2xdetWV82LiFzM8pV906ZNyn/9icj7+Ds7kSYsF3taWhqSk5ORm5vrivkQkZtY+jF+y5YtCAwMRGNjI1JTUxEREYHo6GhXzY2IXMjSlT0wMBAAYLPZMHPmTJSVlblkUkTkek4X+40bN9Da2tr93/v370dkZKTLJkZEruX0j/GNjY1YtGgRgLv7ls+ePRtTp0512cRczcq+8IDch3/uuefEsao+/L2fkJw1fvx40+z48ePi2Dlz5oj5tm3bnJlSt7CwMNPM4XBYeuzff/9dzAsLC00zaa07ALS1tYl5TU2NmKvmptpv3x2cLvbQ0FDL3whE5DlsvRFpgsVOpAkWO5EmWOxEmmCxE2lCmyWuqlaKqlUisdlsYh4eHi7m0lbQAGC328Vcam8dPXpUHKvqqKgWOV29elXML1++bJpFRUWJY2tra8V89OjRTueqZceq27/HjBkj5i+99JKY79q1S8zdgVd2Ik2w2Ik0wWIn0gSLnUgTLHYiTbDYiTTBYifShDZ99oEDB4r52LFjxVza9vjGjRvi2GHDhol5cHCwmF+/fl3MpV65ainnm2++KeaqPrvqdU1ISDDNNmzYII6dNGmSmEv3FwBAVVWVaRYfHy+OPXv2rJgfPHhQzJ966ikxl6juH3D2nhBe2Yk0wWIn0gSLnUgTLHYiTbDYiTTBYifSBIudSBPa9Nl/+eUXMb9z546YS1tRq3rZI0aMEPPdu3eLuYq0Zvznn38Wx6r65Hv27LE0Xlo3rtpiW3WU9fz588V86NChptm9Mw/MZGRkiHliYqKYBwQEiLkkIiJCzNlnJyIRi51IEyx2Ik2w2Ik0wWIn0gSLnUgTLHYiTfQxDMPw1JOFh4eLa4y96cknnxTzW7dumWb79u0Txx47dsypOd2j2ld+/fr1ptm1a9csPbfq/gTV3KR7DA4fPiyOjYyMFPOWlhYxz8vLM81URy6r9hBQ7Zf/6aefirnUK1edQ9DY2GiahYWF4eLFiz1myit7dnY2YmJi7ruppLm5GampqYiPj0dqaqrlbygicj9lsScnJ2Pjxo33fSwnJwcxMTEoLCxETEwMcnJy3DZBInINZbFHR0dj8ODB932spKQESUlJAICkpCQUFxe7ZXJE5DpOvUHX2NjYff6Y3W5X/v5CRN7Hd+OJNOFUsdtsNjQ0NAAAGhoalDuQEpH3OVXssbGxyM/PBwDk5+cjLi7OlXMiIjdQ9tmzsrJQWlqKpqYm2Gw2ZGRkYMaMGVi8eDFqa2sRHByMtWvXwt/fX/lk7uyzDxo0SMxVPVlpvTpw93Vw9rl9fX3FXLWWvq6uTszPnDljmpWUlIhjVblqbirjxo0zzUJDQ8Wxzc3NYn7lyhUxf+WVV0wzh8MhjrX6fSp9TQDg3XfftfT4ZqQ+u3LzitWrV/f48U2bNlmbFRF5FN+gI9IEi51IEyx2Ik2w2Ik0wWIn0kSv2kpa1f6S7N2719Jzl5WVibm09bCqrafi4yP/mxsUFCTmt2/fNs1US1AnT54s5ufOnRNzqbWmUl5eLubS5wWoj5MuKCgwzZYvXy6OVX1Nm5qaxPx///ufmHsDr+xEmmCxE2mCxU6kCRY7kSZY7ESaYLETaYLFTqSJXtVn3759u9NjVT161TLRQ4cOifnEiRNNswMHDohjVbq6usRcNff+/fs7PdbPz0/MR44cKealpaViLr1unZ2d4ti/7334d9KRzID8urzxxhvi2ISEBDFXube5S2/CKzuRJljsRJpgsRNpgsVOpAkWO5EmWOxEmmCxE2miV/XZ3Um1Jlzl4MGDplmfPn3EsapttlVbJqtI/WRVL1p1nLTqdevXr5+YX7p0yTQLDAy09Nwq0j0Gqh6+6muqylXr/IcNG2aaqeZ24sQJMTfDKzuRJljsRJpgsRNpgsVOpAkWO5EmWOxEmmCxE2nioemzW1kLD6jXbZ8/f940Gz58uDhW1TdV7UF++vRpMZf67Ldu3RLHqnLVOv+AgAAxv3Dhgmk2adIkcax0bwMg96oBIDg42DSrqKgQx6qO4b5+/bqYqyhOSncL5ZU9OzsbMTEx920OsW7dOrz44otITExEYmIidu/e7dZJEpF1yit7cnIy5s+fj/fff/++jy9cuBBpaWlumxgRuZbyyh4dHa38MZSIej+n36DbvHkzHA4HsrOzce3aNVfOiYjcwKlinzdvHoqKilBQUAC73Y5Vq1a5el5E5GJOFXtAQAB8fX3h4+ODlJQUHD9+3NXzIiIXc6rY/7pNbnFxMSIjI102ISJyD+W78VlZWSgtLUVTUxOmTp2KjIwMlJaW4tSpUwCAkJAQrFixwu0TtcrK2e8AcPXqVdNM1WevrKy09NxFRUViLp1jrvqpS9XjV91/oOpH37x50zRT9fBVZ8tLfXSVBQsWiLnVPnpVVZWY19TUOJVZoSz21atX/+NjKSkpbpkMEbkPb5cl0gSLnUgTLHYiTbDYiTTBYifSxEOzxFVF2tIYUC+XHDVqlNPPHRUVJeYnT54Uc9WRztIyUrvdLo5VbXmsakGpjl2Wtly+fPmyOLatrU3MrSxLllqCgLql2NLSIuaq7cHHjh1rmoWFhYljnV3OzSs7kSZY7ESaYLETaYLFTqQJFjuRJljsRJpgsRNpQps++9GjR8Vc1WfPy8szzSZMmCCOjY6OFnObzSbmU6dOFXPJgAEDxFy1xFW1/+CRI0fEXHpdVdtYq45FVt07IfWjpWXBAJRbran67OXl5WJeVlbmVGYFr+xEmmCxE2mCxU6kCRY7kSZY7ESaYLETaYLFTqQJbfrsqq2kd+3aJeYvvfSSaRYbGyuOVR09rDJ06FAxl9as+/hY+/f87NmzYj5+/Hgxl46bnjJlijhW1Yd/5513xPzRRx81zawebNLR0SHmubm5Ym5la3OuZyciEYudSBMsdiJNsNiJNMFiJ9IEi51IEyx2Ik30MQzD8NSThYeHK4+ylUi9SWd7j6547ieeeEIcu3z5cjFX9bJVjy+9pqq92VtbW8VctXe7as25lKs+r5CQEDEfPXq0mEtr+S9evCiO/eOPP8Q8MTFRzFX78R87dsw0GzhwoDhW+pqEhYWZfm7KK3ttbS0WLFiAWbNmISEhAZs2bQJwdxP81NRUxMfHIzU1VbnYn4i8S1nsvr6+WLJkCXbu3Inc3Fx8/fXXOHv2LHJychATE4PCwkLExMQgJyfHE/MlIicpi91ut3cfX+Tn54eIiAjU19ejpKQESUlJAICkpCQUFxe7daJEZM2/eoPu0qVLOHnyJMaNG4fGxsbue7LtdjuuXr3qlgkSkWs8cLG3tbUhMzMTS5cuhZ+fnzvnRERu8EDFfufOHWRmZsLhcCA+Ph7A3R1RGxoaAAANDQ0YMmSI+2ZJRJYpl7gahoFly5YhIiICqamp3R+PjY1Ffn4+0tPTkZ+fj7i4OLdOVEX1j43q1wx/f38x/+GHH0yzOXPmiGO/+eYbMXc4HGL+9NNPi7m0FFR1rPGZM2ecfmxAXkYKyEtgpfYTAAQGBor5I488Iua//vqrafb999+LY7/88ksxV33NVB1t6XOfPn26ONbZNrOy2A8fPoyCggKMHDmyu7eYlZWF9PR0LF68GN999x2Cg4Oxdu1apyZARJ6hLPYJEyaYbkBwr+dORL0fb5cl0gSLnUgTLHYiTbDYiTTBYifSxEOzlbSqH6zS3Nws5pMnTzbNduzYIY5VLSOdOHGipfFSL/38+fPiWFUPPyAgQMxVS2ilfrKqV71t2zYx/+2338R87969ptlXX30ljlVRPXdNTY2Ye2O5Nq/sRJpgsRNpgsVOpAkWO5EmWOxEmmCxE2mCxU6kiYemz37jxg23Pv7+/fudHltZWSnma9asEXPVXgG1tbWm2WOPPSaOVd2fUF5eLuaqXYukraRXr14tji0rKxPzEydOiPmhQ4dMs+HDh4tjVV+zZ555xlLu7q3Pe8IrO5EmWOxEmmCxE2mCxU6kCRY7kSZY7ESaYLETaeI/dWSzFdL6YUDd95T2R+/q6hLHDho0SMxVX4KWlhYxj4iIMM0uXLggjr136IcZVZ9d9bn7+JhfT1Rr6VVfE6u9cney+v3mLEtHNhPRw4HFTqQJFjuRJljsRJpgsRNpgsVOpAkWO5EmlOvZa2tr8d577+HKlSvw8fHByy+/jNdeew3r1q3Dt99+230uelZWFqZNm+b2CbtLQkKCmKv2hpc0NjY6PRa42zuV3Lx50zRrb28Xx6r6vQMGDBDz/v37i7m0H7/VPriqT6/KrfDGenSrlMXu6+uLJUuWICoqCq2trZg7d273gQkLFy5EWlqa2ydJRNYpi91ut8NutwO4uytJREQE6uvr3T4xInKtf/U7+6VLl3Dy5EmMGzcOALB582Y4HA5kZ2fj2rVrbpkgEbnGAxd7W1sbMjMzsXTpUvj5+WHevHkoKipCQUEB7HY7Vq1a5c55EpFFD1Tsd+7cQWZmJhwOR/fCiYCAAPj6+sLHxwcpKSk4fvy4WydKRNYoi90wDCxbtgwRERFITU3t/nhDQ0P3fxcXFyMyMtI9MyQil1AucT106BBeffVVjBw5snu5YlZWFrZv345Tp04BAEJCQrBixYruN/LMeHOJqzfde4/DTF1dnZir3g+RtoP29/cXx6qOqlZ59tlnxVx1tLEV3lpG2ptJS1yV78ZPmDABp0+f/sfH/8s9dSId8Q46Ik2w2Ik0wWIn0gSLnUgTLHYiTbDYiTShzVbS7uTufi/7yfSguJU0EbHYiXTBYifSBIudSBMsdiJNsNiJNMFiJ9KER/vszz//PEJCQjz1dETaqampwYEDB3rMPFrsROQ9/DGeSBMsdiJNsNiJNMFiJ9IEi51IEyx2Ik0ot5J2hz179mDlypXo6upCSkoK0tPTvTGNHsXGxmLgwIHw8fGBr68vtm7d6rW5ZGdnY9euXbDZbN1r1pubm/H222+jpqYGISEhWLNmDQYPHtwr5tZbjvE2O2bc26+d148/Nzyso6PDiIuLM6qqqozbt28bDofDqKio8PQ0TE2fPt1obGz09jQMwzCM0tJSo7y83EhISOj+2Mcff2ysX7/eMAzDWL9+vfHJJ5/0mrl99tlnxsaNG70yn7+qr683ysvLDcMwjJaWFiM+Pt6oqKjw+mtnNi9PvW4e/zG+rKwM4eHhCA0NRb9+/ZCQkICSkhJPT+M/ITo6+h9XnpKSEiQlJQEAkpKSUFxc7IWZ9Ty33sJutyMqKgrA/ceMe/u1M5uXp3i82Ovr6xEUFNT998DAwF533ntaWhqSk5ORm5vr7an8Q2NjY/cxW3a7HVevXvXyjO7X247x/usx473ptfPG8eceL3ajh7tz+/Tp4+lpmNqyZQvy8vKwYcMGbN68GQcPHvT2lP4zetsx3n8/Zry38Nbx5x4v9qCgoPsOMqyvr1ceCOlJgYGBAACbzYaZM2eirKzMyzO6n81m6z5Bt6GhoftNnd6gNx3j3dMx473htfPm8eceL/YxY8agsrIS1dXVaG9vx44dOxAbG+vpafToxo0baG1t7f7v/fv397qjqGNjY5Gfnw8AyM/PR1xcnHcn9Be95Rhvw+SYcW+/dmbz8tTr5pVVb7t378ZHH32Ezs5OzJ07F2+99Zanp9Cj6upqLFq0CADQ2dmJ2bNne3VuWVlZKC0tRVNTE2w2GzIyMjBjxgwsXrwYtbW1CA4Oxtq1a5XHMntqbqWlpf/6GG93MDtmfOzYsV597Vx5/LkzuMSVSBO8g45IEyx2Ik2w2Ik0wWIn0gSLnUgTLHYiTbDYiTTxf1d8kbW1KyxDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(adv_x_PGD_100[1].reshape((28, 28)), cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd2f6b9f820>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(adv_x_PGD_1000[1].reshape((28, 28)), cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "Robustness(Clean): 0.86\n",
      "[[ 0.00112736  0.01011385  0.00287164  0.001241    0.00544948]\n",
      " [-0.0015968   0.00132874  0.01696907 -0.00243109 -0.00083482]\n",
      " [-0.00199748  0.01193599 -0.00562409 -0.00149281  0.00288802]\n",
      " [-0.00396522  0.012832    0.0027059  -0.00103998  0.00695626]\n",
      " [ 0.04279609 -0.00182015 -0.00166272 -0.00105851 -0.00062796]]\n",
      "\n",
      "256\n",
      "Robustness(FGSM): 0.76\n",
      "[[-7.31078699e-04  2.92903614e-03  5.43250651e-03  6.67594540e-04\n",
      "   2.25524591e-03]\n",
      " [-1.96649544e-03 -8.66053704e-04  1.40917228e-02 -1.44332758e-03\n",
      "  -1.68834972e-03]\n",
      " [-1.60995693e-03  8.38280497e-03 -1.84292881e-03 -2.41019468e-03\n",
      "   3.41287823e-03]\n",
      " [-9.78804255e-04  1.15932265e-02  6.06483282e-03 -3.99598156e-03\n",
      "   6.17756399e-03]\n",
      " [ 2.24459080e-02  5.36157801e-05 -2.52951109e-03 -2.44209142e-03\n",
      "  -3.94823496e-03]]\n",
      "\n",
      "update_c: 1\n",
      "256\n",
      "Robustness(PGD-10): 0.78\n",
      "[[-4.26988120e-03  5.04486080e-03  8.86506402e-03 -5.67124272e-05\n",
      "   3.58916334e-03]\n",
      " [-2.03793332e-03 -2.66613768e-03  1.48294663e-02 -1.70671905e-03\n",
      "  -1.44862185e-03]\n",
      " [-1.92157298e-03  8.91376827e-03  2.74549616e-03 -1.89186644e-03\n",
      "   7.56701282e-04]\n",
      " [-3.09007220e-03  9.22607686e-03  9.98459872e-03 -2.91234233e-03\n",
      "   5.38304388e-03]\n",
      " [ 4.65576209e-02 -4.69364258e-03 -3.97419663e-03  9.19339227e-04\n",
      "   5.44150136e-03]]\n",
      "\n",
      "update_c: 2\n",
      "256\n",
      "Robustness(PGD-10): 0.79\n",
      "[[-4.23890643e-03  5.00793209e-03  9.01556432e-03 -8.31752146e-05\n",
      "   3.54937612e-03]\n",
      " [-2.11184395e-03 -2.85493452e-03  1.52173603e-02 -1.81917791e-03\n",
      "  -1.44862247e-03]\n",
      " [-2.00062343e-03  9.05618524e-03  2.89466672e-03 -1.84878319e-03\n",
      "   8.08477212e-04]\n",
      " [-3.21715270e-03  9.13640904e-03  1.04618002e-02 -2.97854894e-03\n",
      "   5.52078185e-03]\n",
      " [ 4.67493395e-02 -4.75178897e-03 -3.88549446e-03  8.00766522e-04\n",
      "   5.30035378e-03]]\n",
      "\n",
      "update_c: 4\n",
      "256\n",
      "Robustness(PGD-10): 0.79\n",
      "[[-4.30449133e-03  5.09744317e-03  8.98970467e-03 -7.10082384e-05\n",
      "   3.62691650e-03]\n",
      " [-2.09970186e-03 -2.82112209e-03  1.52381998e-02 -1.81803101e-03\n",
      "  -1.44262621e-03]\n",
      " [-2.01511406e-03  9.07160562e-03  2.93156739e-03 -1.94548526e-03\n",
      "   8.12340118e-04]\n",
      " [-3.34015358e-03  9.14335360e-03  1.04837958e-02 -2.89827412e-03\n",
      "   5.42406386e-03]\n",
      " [ 4.66274211e-02 -4.78049277e-03 -3.91254671e-03  7.55464460e-04\n",
      "   5.20295050e-03]]\n",
      "\n",
      "update_c: 8\n",
      "256\n",
      "Robustness(PGD-10): 0.79\n",
      "[[-4.27901894e-03  5.11996813e-03  9.01032512e-03 -5.15367829e-05\n",
      "   3.60930627e-03]\n",
      " [-2.08461823e-03 -2.78891584e-03  1.53403469e-02 -1.79821123e-03\n",
      "  -1.45205895e-03]\n",
      " [-1.95834147e-03  9.16331273e-03  2.94486954e-03 -2.00664169e-03\n",
      "   8.58974457e-04]\n",
      " [-3.35762308e-03  9.05507455e-03  1.04768743e-02 -2.90605518e-03\n",
      "   5.41072238e-03]\n",
      " [ 4.66775449e-02 -4.74771012e-03 -3.87424073e-03  7.53414041e-04\n",
      "   5.25770315e-03]]\n",
      "\n",
      "update_c: 16\n",
      "256\n",
      "Robustness(PGD-10): 0.79\n",
      "[[-4.27901894e-03  5.11996813e-03  9.01032512e-03 -5.15367829e-05\n",
      "   3.60930627e-03]\n",
      " [-2.09015473e-03 -2.78919172e-03  1.53347966e-02 -1.79972713e-03\n",
      "  -1.45281543e-03]\n",
      " [-1.95834147e-03  9.16331273e-03  2.94486954e-03 -2.00664169e-03\n",
      "   8.58974457e-04]\n",
      " [-3.35598291e-03  9.05261816e-03  1.04781300e-02 -2.90572508e-03\n",
      "   5.41068106e-03]\n",
      " [ 4.66775449e-02 -4.74771012e-03 -3.87424073e-03  7.53414041e-04\n",
      "   5.25770315e-03]]\n",
      "\n",
      "update_c: 32\n",
      "256\n",
      "Robustness(PGD-10): 0.79\n",
      "[[-4.27901894e-03  5.11996813e-03  9.01032512e-03 -5.15367829e-05\n",
      "   3.60930627e-03]\n",
      " [-2.09015473e-03 -2.78919172e-03  1.53347966e-02 -1.79972713e-03\n",
      "  -1.45281543e-03]\n",
      " [-1.95834147e-03  9.16331273e-03  2.94486954e-03 -2.00664169e-03\n",
      "   8.58974457e-04]\n",
      " [-3.35598291e-03  9.05261816e-03  1.04781300e-02 -2.90572508e-03\n",
      "   5.41068106e-03]\n",
      " [ 4.66775449e-02 -4.74771012e-03 -3.87424073e-03  7.53414041e-04\n",
      "   5.25770315e-03]]\n",
      "\n",
      "update_c: 64\n",
      "256\n",
      "Robustness(PGD-10): 0.79\n",
      "[[-4.27901894e-03  5.11996813e-03  9.01032512e-03 -5.15367829e-05\n",
      "   3.60930627e-03]\n",
      " [-2.09015473e-03 -2.78919172e-03  1.53347966e-02 -1.79972713e-03\n",
      "  -1.45281543e-03]\n",
      " [-1.95834147e-03  9.16331273e-03  2.94486954e-03 -2.00664169e-03\n",
      "   8.58974457e-04]\n",
      " [-3.35598291e-03  9.05261816e-03  1.04781300e-02 -2.90572508e-03\n",
      "   5.41068106e-03]\n",
      " [ 4.66775449e-02 -4.74771012e-03 -3.87424073e-03  7.53414041e-04\n",
      "   5.25770315e-03]]\n",
      "\n",
      "update_c: 1\n",
      "256\n",
      "Robustness(PGD-100): 0.77\n",
      "[[-0.00379634  0.004873    0.00920179 -0.0001084   0.00369985]\n",
      " [-0.00205058 -0.00293253  0.0149287  -0.00153152 -0.00136389]\n",
      " [-0.00203944  0.00922     0.00358305 -0.00179971  0.00099833]\n",
      " [-0.0027543   0.00869902  0.00972733 -0.00280783  0.00487334]\n",
      " [ 0.04555639 -0.00518328 -0.0036908   0.00017563  0.00487758]]\n",
      "\n",
      "update_c: 2\n",
      "256\n",
      "Robustness(PGD-100): 0.77\n",
      "[[-0.0038381   0.00494069  0.00922167 -0.00018848  0.00363441]\n",
      " [-0.00215703 -0.0031069   0.0151841  -0.0016551  -0.00130205]\n",
      " [-0.00205443  0.00953181  0.00361906 -0.00186893  0.00099636]\n",
      " [-0.00299801  0.00866338  0.00987098 -0.00290066  0.00494965]\n",
      " [ 0.04589252 -0.00513145 -0.00390918 -0.00030252  0.00495618]]\n",
      "\n",
      "update_c: 4\n",
      "256\n",
      "Robustness(PGD-100): 0.77\n",
      "[[-0.00382899  0.00493865  0.00927264 -0.00020785  0.00363281]\n",
      " [-0.00218852 -0.0031856   0.01543218 -0.00171109 -0.00125164]\n",
      " [-0.00207775  0.00962425  0.00364682 -0.00183234  0.00096962]\n",
      " [-0.00302352  0.00849012  0.00973703 -0.00288255  0.00478772]\n",
      " [ 0.04549268 -0.00515077 -0.00390251 -0.0003046   0.00505462]]\n",
      "\n",
      "update_c: 8\n",
      "256\n",
      "Robustness(PGD-100): 0.77\n",
      "[[-0.00381911  0.00501875  0.00930746 -0.00023109  0.003635  ]\n",
      " [-0.00219616 -0.003303    0.0157598  -0.0017953  -0.00119907]\n",
      " [-0.0020495   0.00969527  0.00359163 -0.00176872  0.0009942 ]\n",
      " [-0.00304263  0.00843654  0.0098035  -0.00287393  0.00489788]\n",
      " [ 0.04576252 -0.00513862 -0.00394466 -0.00013327  0.00516404]]\n",
      "\n",
      "update_c: 16\n",
      "256\n",
      "Robustness(PGD-100): 0.77\n",
      "[[-0.00382373  0.00497752  0.00936606 -0.00022318  0.00365272]\n",
      " [-0.00220579 -0.00331299  0.01585182 -0.00182191 -0.00118829]\n",
      " [-0.00204699  0.00976228  0.00361869 -0.0017937   0.00101154]\n",
      " [-0.00305109  0.00829408  0.00953219 -0.00288099  0.00468737]\n",
      " [ 0.04561516 -0.005193   -0.00395485 -0.00017675  0.00518039]]\n",
      "\n",
      "update_c: 32\n",
      "256\n",
      "Robustness(PGD-100): 0.77\n",
      "[[-0.00381086  0.00496071  0.00935992 -0.00022003  0.00365876]\n",
      " [-0.00221109 -0.00332996  0.01585289 -0.00184543 -0.00118151]\n",
      " [-0.00207583  0.00980521  0.00362698 -0.00178276  0.00101474]\n",
      " [-0.00303913  0.00833587  0.00960523 -0.00288978  0.00478575]\n",
      " [ 0.04556293 -0.00516235 -0.00395271 -0.00016509  0.00512274]]\n",
      "\n",
      "update_c: 64\n",
      "256\n",
      "Robustness(PGD-100): 0.77\n",
      "[[-0.00379231  0.00499878  0.00935022 -0.00023672  0.00367542]\n",
      " [-0.00221304 -0.00333053  0.01585969 -0.00184167 -0.001178  ]\n",
      " [-0.00207505  0.00982344  0.00361201 -0.00178757  0.00101577]\n",
      " [-0.00307438  0.00837018  0.00967654 -0.00285265  0.00483599]\n",
      " [ 0.04555642 -0.00515465 -0.003959   -0.00015984  0.0051606 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = None\n",
    "c = np.zeros((1, train_size)) + 0.00387526\n",
    "update_c_list = [1, 2, 4, 8, 16, 32, 64]\n",
    "loss_type = ['cross-entropy', 'mse']\n",
    "\n",
    "x_test_list = []\n",
    "c_list = []\n",
    "    \n",
    "\n",
    "# Clean    \n",
    "evaluate(x_train, x_test, model_fn=model_fn, kernel_fn=kernel_list[1], t=t, c=0, attack_type='Clean')\n",
    "x_test_list.append(x_test)\n",
    "c_list.append(c)\n",
    "\n",
    "\n",
    "# FGSM\n",
    "adv_x, c = fast_gradient_method(model_fn=model_fn, kernel_fn=kernel_list[1], obj_fn='test_c', \n",
    "                                grads_fn=test_col_wise_grads_fn, grads_c_fn=test_col_wise_c_grads_fn,\n",
    "                                x_train=x_train, y_train=y_train, x_test=x_test, y=y_test, t=t, c=c,\n",
    "                                loss=loss_type[0], eps=eps, clip_min=0, clip_max=1)\n",
    "\n",
    "evaluate(x_train, adv_x, model_fn=model_fn, kernel_fn=kernel_list[1], t=t, c=c, attack_type='FGSM')\n",
    "x_test_list.append(adv_x)\n",
    "c_list.append(c)\n",
    "\n",
    "for update_c in update_c_list:\n",
    "    # PGD 10\n",
    "    print('update_c:', update_c)\n",
    "    key, new_key = random.split(key)\n",
    "    c = np.zeros((1, train_size)) + 0.00387526\n",
    "    adv_x, c = projected_gradient_descent(model_fn=model_fn, kernel_fn=kernel_list[1], obj_fn='test_c', \n",
    "                                          grads_fn=test_col_wise_grads_fn, grads_c_fn=test_col_wise_c_grads_fn,\n",
    "                                          x_train=x_train, y_train=y_train, x_test=x_test, y=y_test, t=t, c=c, update_c=update_c, \n",
    "                                          loss=loss_type[0], eps=eps, eps_iter=eps_iter_10, nb_iter=10, \n",
    "                                          clip_min=0, clip_max=1, rand_init=None, rand_minmax=eps)\n",
    "    \n",
    "    evaluate(x_train, adv_x, model_fn=model_fn, kernel_fn=kernel_list[1], t=t, c=c, attack_type='PGD-10')\n",
    "    x_test_list.append(adv_x)\n",
    "    c_list.append(c)\n",
    "\n",
    "for update_c in update_c_list:\n",
    "    # PGD 100\n",
    "    print('update_c:', update_c)\n",
    "    key, new_key = random.split(key)\n",
    "    c = np.zeros((1, train_size)) + 0.00387526\n",
    "    adv_x, c = projected_gradient_descent(model_fn=model_fn, kernel_fn=kernel_list[1], obj_fn='test_c', \n",
    "                                          grads_fn=test_col_wise_grads_fn, grads_c_fn=test_col_wise_c_grads_fn,\n",
    "                                          x_train=x_train, y_train=y_train, x_test=x_test, y=y_test, t=t, c=c, update_c=update_c, \n",
    "                                          loss=loss_type[0], eps=eps, eps_iter=eps_iter_100, nb_iter=100, \n",
    "                                          clip_min=0, clip_max=1, rand_init=None, rand_minmax=eps)\n",
    "    \n",
    "    evaluate(x_train, adv_x, model_fn=model_fn, kernel_fn=kernel_list[1], t=t, c=c, attack_type='PGD-100')\n",
    "    x_test_list.append(adv_x)\n",
    "    c_list.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
