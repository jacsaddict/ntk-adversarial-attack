{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "DATASET = 'cifar10'\n",
    "class_num   = 10\n",
    "test_size   = 2048\n",
    "train_size  = 4096\n",
    "valid_size  = 256\n",
    "image_shape = None\n",
    "\n",
    "if DATASET =='mnist':\n",
    "    image_shape = (28, 28, 1)\n",
    "elif DATASET == 'cifar10':\n",
    "    image_shape = (32, 32, 3)\n",
    "\n",
    "#training\n",
    "run = 10\n",
    "batch_size = 256\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    }
   ],
   "source": [
    "x_train_all, y_train_all, x_test_all, y_test_all = tuple(onp.array(x) for x in get_dataset(DATASET, None, None, \n",
    "                                                                                  do_flatten_and_normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "seed = 0\n",
    "x_train_all, y_train_all = shaffle(x_train_all, y_train_all, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# down sample\n",
    "x_train = x_train_all[:train_size]\n",
    "y_train = y_train_all[:train_size]\n",
    "\n",
    "x_valid = x_train_all[:valid_size]\n",
    "y_valid = y_train_all[:valid_size]\n",
    "\n",
    "x_test = x_test_all[:test_size]\n",
    "y_test = y_test_all[:test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, x_test = x_train.reshape((-1, *image_shape)), x_valid.reshape((-1, *image_shape)) ,x_test.reshape((-1, *image_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.applications.ResNet50(include_top=True, weights=None, input_shape=(32, 32, 3), classes=class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 38, 38, 3)    0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 16, 16, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 16, 16, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 16, 16, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 18, 18, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 8, 8, 64)     0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 8, 8, 64)     4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 8, 8, 64)     0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 8, 8, 64)     0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 8, 8, 256)    16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 8, 8, 256)    0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 8, 8, 256)    0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 8, 8, 64)     0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 8, 8, 64)     0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 8, 8, 256)    0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 8, 8, 256)    0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 8, 8, 64)     16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 8, 8, 64)     0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 8, 8, 64)     36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 8, 8, 64)     256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 8, 8, 64)     0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 8, 8, 256)    16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 8, 8, 256)    1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 8, 8, 256)    0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 8, 8, 256)    0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 4, 4, 128)    32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 4, 4, 128)    0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 4, 4, 128)    0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 4, 4, 512)    131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 4, 4, 512)    0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 4, 4, 512)    0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 4, 4, 128)    0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 4, 4, 128)    0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 4, 4, 512)    0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 4, 4, 512)    0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 4, 4, 128)    0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 4, 4, 128)    0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 4, 4, 512)    0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 4, 4, 512)    0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 4, 4, 128)    65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 4, 4, 128)    0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 4, 4, 128)    147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 4, 4, 128)    512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 4, 4, 128)    0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 4, 4, 512)    66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 4, 4, 512)    2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 4, 4, 512)    0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 4, 4, 512)    0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 2, 2, 256)    131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 2, 2, 256)    0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 2, 2, 256)    0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 2, 2, 1024)   525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 2, 2, 1024)   0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 2, 2, 256)    0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 2, 2, 256)    0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 2, 2, 1024)   0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 2, 2, 1024)   0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 2, 2, 256)    0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 2, 2, 256)    0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 2, 2, 1024)   0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 2, 2, 1024)   0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 2, 2, 256)    0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 2, 2, 256)    0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 2, 2, 1024)   0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 2, 2, 1024)   0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 2, 2, 256)    0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 2, 2, 256)    0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 2, 2, 1024)   0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 2, 2, 1024)   0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 2, 2, 256)    262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 2, 2, 256)    0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 2, 2, 256)    590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 2, 2, 256)    1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 2, 2, 256)    0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 2, 2, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 2, 2, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 2, 2, 1024)   0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 2, 2, 1024)   0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 1, 1, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 1, 1, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 1, 1, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 1, 1, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 1, 1, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 1, 1, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 1, 1, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 1, 1, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 1, 1, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 1, 1, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 1, 1, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 1, 1, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 1, 1, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 1, 1, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 1, 1, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 1, 1, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 1, 1, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 1, 1, 2048)   0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 10)           20490       avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 23,608,202\n",
      "Trainable params: 23,555,082\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "15/15 [==============================] - 22s 1s/step - loss: 2.2907 - accuracy: 0.1565 - val_loss: 2.3052 - val_accuracy: 0.0902\n",
      "Epoch 2/120\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 2.2790 - accuracy: 0.1793 - val_loss: 2.3213 - val_accuracy: 0.0902\n",
      "Epoch 3/120\n",
      "15/15 [==============================] - 29s 2s/step - loss: 2.2514 - accuracy: 0.2073 - val_loss: 2.3705 - val_accuracy: 0.0902\n",
      "Epoch 4/120\n",
      "15/15 [==============================] - 1s 67ms/step - loss: 2.2359 - accuracy: 0.2225 - val_loss: 2.3706 - val_accuracy: 0.0902\n",
      "Epoch 5/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.2238 - accuracy: 0.2349 - val_loss: 2.3709 - val_accuracy: 0.0902\n",
      "Epoch 6/120\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 2.2165 - accuracy: 0.2434 - val_loss: 2.3707 - val_accuracy: 0.0902\n",
      "Epoch 7/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.2111 - accuracy: 0.2463 - val_loss: 2.3704 - val_accuracy: 0.0902\n",
      "Epoch 8/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.2275 - accuracy: 0.2295 - val_loss: 2.3709 - val_accuracy: 0.0902\n",
      "Epoch 9/120\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 2.2059 - accuracy: 0.2531 - val_loss: 2.3709 - val_accuracy: 0.0902\n",
      "Epoch 10/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.2013 - accuracy: 0.2545 - val_loss: 2.3709 - val_accuracy: 0.0902\n",
      "Epoch 11/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.1899 - accuracy: 0.2664 - val_loss: 2.2857 - val_accuracy: 0.1780\n",
      "Epoch 12/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.1904 - accuracy: 0.2661 - val_loss: 2.3709 - val_accuracy: 0.0902\n",
      "Epoch 13/120\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 2.2091 - accuracy: 0.2507 - val_loss: 2.3709 - val_accuracy: 0.0902\n",
      "Epoch 14/120\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 2.2032 - accuracy: 0.2553 - val_loss: 2.3709 - val_accuracy: 0.0902\n",
      "Epoch 15/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.1875 - accuracy: 0.2727 - val_loss: 2.3538 - val_accuracy: 0.1073\n",
      "Epoch 16/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.1994 - accuracy: 0.2602 - val_loss: 2.3489 - val_accuracy: 0.1122\n",
      "Epoch 17/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.2255 - accuracy: 0.2355 - val_loss: 2.3610 - val_accuracy: 0.1000\n",
      "Epoch 18/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.2637 - accuracy: 0.1964 - val_loss: 2.3064 - val_accuracy: 0.1537\n",
      "Epoch 19/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.2580 - accuracy: 0.2021 - val_loss: 2.3026 - val_accuracy: 0.1585\n",
      "Epoch 20/120\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 2.2419 - accuracy: 0.2189 - val_loss: 2.3956 - val_accuracy: 0.0610\n",
      "Epoch 21/120\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 2.2604 - accuracy: 0.1999 - val_loss: 2.3522 - val_accuracy: 0.1073\n",
      "Epoch 22/120\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 2.2388 - accuracy: 0.2219 - val_loss: 2.3408 - val_accuracy: 0.1195\n",
      "Epoch 23/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.2334 - accuracy: 0.2273 - val_loss: 2.3413 - val_accuracy: 0.1195\n",
      "Epoch 24/120\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 2.2077 - accuracy: 0.2528 - val_loss: 2.3526 - val_accuracy: 0.1073\n",
      "Epoch 25/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.2349 - accuracy: 0.2252 - val_loss: 2.3632 - val_accuracy: 0.0976\n",
      "Epoch 26/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.2498 - accuracy: 0.2103 - val_loss: 2.3777 - val_accuracy: 0.0829\n",
      "Epoch 27/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.2396 - accuracy: 0.2214 - val_loss: 2.3988 - val_accuracy: 0.0610\n",
      "Epoch 28/120\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 2.2186 - accuracy: 0.2420 - val_loss: 2.3460 - val_accuracy: 0.1146\n",
      "Epoch 29/120\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 2.1992 - accuracy: 0.2618 - val_loss: 2.3127 - val_accuracy: 0.1488\n",
      "Epoch 30/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.1986 - accuracy: 0.2618 - val_loss: 2.3244 - val_accuracy: 0.1366\n",
      "Epoch 31/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.2168 - accuracy: 0.2442 - val_loss: 2.2755 - val_accuracy: 0.1854\n",
      "Epoch 32/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.2221 - accuracy: 0.2390 - val_loss: 2.2487 - val_accuracy: 0.2122\n",
      "Epoch 33/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.2146 - accuracy: 0.2466 - val_loss: 2.2682 - val_accuracy: 0.1927\n",
      "Epoch 34/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.2080 - accuracy: 0.2528 - val_loss: 2.2326 - val_accuracy: 0.2293\n",
      "Epoch 35/120\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 2.2039 - accuracy: 0.2566 - val_loss: 2.2154 - val_accuracy: 0.2463\n",
      "Epoch 36/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.2003 - accuracy: 0.2604 - val_loss: 2.2172 - val_accuracy: 0.2439\n",
      "Epoch 37/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.1979 - accuracy: 0.2629 - val_loss: 2.2691 - val_accuracy: 0.1927\n",
      "Epoch 38/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.2197 - accuracy: 0.2412 - val_loss: 2.2521 - val_accuracy: 0.2098\n",
      "Epoch 39/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.2041 - accuracy: 0.2564 - val_loss: 2.2304 - val_accuracy: 0.2293\n",
      "Epoch 40/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.1997 - accuracy: 0.2615 - val_loss: 2.2588 - val_accuracy: 0.2024\n",
      "Epoch 41/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.2105 - accuracy: 0.2499 - val_loss: 2.2880 - val_accuracy: 0.1732\n",
      "Epoch 42/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.2126 - accuracy: 0.2482 - val_loss: 2.2906 - val_accuracy: 0.1707\n",
      "Epoch 43/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.2288 - accuracy: 0.2320 - val_loss: 2.3111 - val_accuracy: 0.1488\n",
      "Epoch 44/120\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 2.2276 - accuracy: 0.2330 - val_loss: 2.2949 - val_accuracy: 0.1659\n",
      "Epoch 45/120\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 2.2318 - accuracy: 0.2292 - val_loss: 2.2832 - val_accuracy: 0.1780\n",
      "Epoch 46/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.2531 - accuracy: 0.2078 - val_loss: 2.2999 - val_accuracy: 0.1610\n",
      "Epoch 47/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.2500 - accuracy: 0.2113 - val_loss: 2.2958 - val_accuracy: 0.1634\n",
      "Epoch 48/120\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 2.2466 - accuracy: 0.2146 - val_loss: 2.2734 - val_accuracy: 0.1878\n",
      "Epoch 49/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.2449 - accuracy: 0.2160 - val_loss: 2.2484 - val_accuracy: 0.2122\n",
      "Epoch 50/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.2305 - accuracy: 0.2303 - val_loss: 2.2732 - val_accuracy: 0.1878\n",
      "Epoch 51/120\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 2.2273 - accuracy: 0.2336 - val_loss: 2.3179 - val_accuracy: 0.1439\n",
      "Epoch 52/120\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 2.2456 - accuracy: 0.2151 - val_loss: 2.3709 - val_accuracy: 0.0902\n",
      "Epoch 53/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.2669 - accuracy: 0.1940 - val_loss: 2.3684 - val_accuracy: 0.0927\n",
      "Epoch 54/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.2719 - accuracy: 0.1891 - val_loss: 2.3563 - val_accuracy: 0.1049\n",
      "Epoch 55/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.2708 - accuracy: 0.1899 - val_loss: 2.3219 - val_accuracy: 0.1390\n",
      "Epoch 56/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.2603 - accuracy: 0.1999 - val_loss: 2.2830 - val_accuracy: 0.1780\n",
      "Epoch 57/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.2488 - accuracy: 0.2119 - val_loss: 2.3112 - val_accuracy: 0.1512\n",
      "Epoch 58/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 1s 64ms/step - loss: 2.2518 - accuracy: 0.2092 - val_loss: 2.3184 - val_accuracy: 0.1415\n",
      "Epoch 59/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.2727 - accuracy: 0.1872 - val_loss: 2.3173 - val_accuracy: 0.1439\n",
      "Epoch 60/120\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 2.2652 - accuracy: 0.1953 - val_loss: 2.3207 - val_accuracy: 0.1390\n",
      "Epoch 61/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.2595 - accuracy: 0.2008 - val_loss: 2.3024 - val_accuracy: 0.1585\n",
      "Epoch 62/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.2573 - accuracy: 0.2032 - val_loss: 2.2877 - val_accuracy: 0.1732\n",
      "Epoch 63/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.2489 - accuracy: 0.2105 - val_loss: 2.2954 - val_accuracy: 0.1659\n",
      "Epoch 64/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.2345 - accuracy: 0.2249 - val_loss: 2.2949 - val_accuracy: 0.1634\n",
      "Epoch 65/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.2042 - accuracy: 0.2547 - val_loss: 2.2784 - val_accuracy: 0.1829\n",
      "Epoch 66/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.2027 - accuracy: 0.2561 - val_loss: 2.2727 - val_accuracy: 0.1854\n",
      "Epoch 67/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.2031 - accuracy: 0.2537 - val_loss: 2.2813 - val_accuracy: 0.1780\n",
      "Epoch 68/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.1975 - accuracy: 0.2615 - val_loss: 2.2626 - val_accuracy: 0.1976\n",
      "Epoch 69/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.1903 - accuracy: 0.2694 - val_loss: 2.2606 - val_accuracy: 0.1976\n",
      "Epoch 70/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.1897 - accuracy: 0.2694 - val_loss: 2.2530 - val_accuracy: 0.2073\n",
      "Epoch 71/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.1807 - accuracy: 0.2786 - val_loss: 2.2384 - val_accuracy: 0.2220\n",
      "Epoch 72/120\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 2.1782 - accuracy: 0.2808 - val_loss: 2.2247 - val_accuracy: 0.2341\n",
      "Epoch 73/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.1734 - accuracy: 0.2868 - val_loss: 2.2537 - val_accuracy: 0.2049\n",
      "Epoch 74/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.1797 - accuracy: 0.2797 - val_loss: 2.2478 - val_accuracy: 0.2098\n",
      "Epoch 75/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.1683 - accuracy: 0.2919 - val_loss: 2.2215 - val_accuracy: 0.2390\n",
      "Epoch 76/120\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 2.1663 - accuracy: 0.2916 - val_loss: 2.2048 - val_accuracy: 0.2488\n",
      "Epoch 77/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.1556 - accuracy: 0.2987 - val_loss: 2.2002 - val_accuracy: 0.2512\n",
      "Epoch 78/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.1573 - accuracy: 0.2976 - val_loss: 2.2443 - val_accuracy: 0.2146\n",
      "Epoch 79/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.1496 - accuracy: 0.3082 - val_loss: 2.2545 - val_accuracy: 0.2073\n",
      "Epoch 80/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.1555 - accuracy: 0.3003 - val_loss: 2.2270 - val_accuracy: 0.2317\n",
      "Epoch 81/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.1469 - accuracy: 0.3095 - val_loss: 2.2033 - val_accuracy: 0.2537\n",
      "Epoch 82/120\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 2.1363 - accuracy: 0.3209 - val_loss: 2.2282 - val_accuracy: 0.2293\n",
      "Epoch 83/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.1534 - accuracy: 0.3030 - val_loss: 2.3259 - val_accuracy: 0.1293\n",
      "Epoch 84/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.1383 - accuracy: 0.3196 - val_loss: 2.2265 - val_accuracy: 0.2268\n",
      "Epoch 85/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.1382 - accuracy: 0.3190 - val_loss: 2.1996 - val_accuracy: 0.2537\n",
      "Epoch 86/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.1375 - accuracy: 0.3199 - val_loss: 2.2221 - val_accuracy: 0.2293\n",
      "Epoch 87/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.1398 - accuracy: 0.3169 - val_loss: 2.1953 - val_accuracy: 0.2561\n",
      "Epoch 88/120\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 2.1286 - accuracy: 0.3285 - val_loss: 2.1960 - val_accuracy: 0.2610\n",
      "Epoch 89/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.1203 - accuracy: 0.3397 - val_loss: 2.1891 - val_accuracy: 0.2707\n",
      "Epoch 90/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.1068 - accuracy: 0.3519 - val_loss: 2.2278 - val_accuracy: 0.2268\n",
      "Epoch 91/120\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 2.1414 - accuracy: 0.3147 - val_loss: 2.2048 - val_accuracy: 0.2463\n",
      "Epoch 92/120\n",
      "15/15 [==============================] - 1s 66ms/step - loss: 2.1209 - accuracy: 0.3388 - val_loss: 2.2578 - val_accuracy: 0.2000\n",
      "Epoch 93/120\n",
      "15/15 [==============================] - 1s 62ms/step - loss: 2.1280 - accuracy: 0.3296 - val_loss: 2.2155 - val_accuracy: 0.2366\n",
      "Epoch 94/120\n",
      "15/15 [==============================] - 1s 65ms/step - loss: 2.1136 - accuracy: 0.3448 - val_loss: 2.1947 - val_accuracy: 0.2610\n",
      "Epoch 95/120\n",
      "15/15 [==============================] - 1s 63ms/step - loss: 2.0928 - accuracy: 0.3673 - val_loss: 2.1437 - val_accuracy: 0.3073\n",
      "Epoch 96/120\n",
      "15/15 [==============================] - 1s 64ms/step - loss: 2.1007 - accuracy: 0.3570 - val_loss: 2.1979 - val_accuracy: 0.2561\n",
      "Epoch 97/120\n",
      "11/15 [=====================>........] - ETA: 0s - loss: 2.0955 - accuracy: 0.3640"
     ]
    }
   ],
   "source": [
    "model.fit(x=x_train,\n",
    "          y=y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=120,\n",
    "          callbacks=None,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========NTK============\n",
      "4/4 [==============================] - 1s 358ms/step - loss: 2.1563 - accuracy: 0.3047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.1563456058502197, 0.3046875]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = onp.load('./npy/cifar-eps-time-any-npy/cifar-fgsm-eps-0.03-time-None.npy')\n",
    "print(\"==========NTK============\")\n",
    "model.evaluate(tmp, y_test[:128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========MSE============\n",
      "64/64 [==============================] - 1s 9ms/step - loss: 2.1528 - accuracy: 0.3018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.1527912616729736, 0.3017578125]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = onp.load('./npy/cifar-untargeted-cifar-nn-grey-box-train=4096-mse.npy')\n",
    "print(\"==========MSE============\")\n",
    "model.evaluate(tmp, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 6ms/step - loss: 2.1674 - accuracy: 0.2578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.1674437522888184, 0.2578125]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = onp.load('./npy/cifar-fgsm-eps-0.03-time-None-nngp.npy')\n",
    "model.evaluate(tmp, y_test[:128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
