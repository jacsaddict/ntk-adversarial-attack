{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "import jax.numpy as np\n",
    "\n",
    "from jax import lax, random\n",
    "from jax.api import grad, jit, vmap\n",
    "from jax.config import config\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "config.update('jax_enable_x64', True)\n",
    "\n",
    "from functools import partial\n",
    "from jax import random\n",
    "\n",
    "from neural_tangents import stax\n",
    "\n",
    "# Attacking\n",
    "from jax.experimental.stax import logsoftmax\n",
    "from cleverhans.utils import clip_eta, one_hot\n",
    "\n",
    "# Plotting\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from utils import *\n",
    "sns.set_style(style='white')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "\"\"\"\n",
    "diag_reg:\n",
    "    a scalar representing the strength of the diagonal regularization for\n",
    "    `k_train_train`, i.e. computing `k_train_train + diag_reg * I` during\n",
    "    Cholesky factorization or eigendecomposition.\n",
    "\"\"\"\n",
    "diag_reg = 1e-4\n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_all, y_train_all, x_test_all, y_test_all = tuple(onp.array(x) for x in get_dataset('mnist', None, None, \n",
    "                                                                                  do_flatten_and_normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "x_train_all, y_train_all = shaffle(x_train_all, y_train_all, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 256\n",
    "x_train = x_train_all[:train_size]\n",
    "y_train = y_train_all[:train_size]\n",
    "\n",
    "test_size = 256\n",
    "x_test = x_test_all[:test_size]\n",
    "y_test = y_test_all[:test_size]\n",
    "\n",
    "shape = (x_train.shape[0], 32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "x_test = np.asarray(x_test)\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective - Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(mean, ys):\n",
    "    return np.mean(np.argmax(mean, axis=-1) == np.argmax(ys, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DenseBlock(neurons, W_std, b_std):\n",
    "    return stax.serial(stax.Dense(neurons, W_std, b_std), \n",
    "                       stax.Erf())\n",
    "\n",
    "def DenseGroup(n, neurons, W_std, b_std):\n",
    "    blocks = []\n",
    "    for _ in range(n):\n",
    "        blocks += [DenseBlock(neurons, W_std, b_std)]\n",
    "    return stax.serial(*blocks)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# b = 0.05\n",
    "b = 0.18\n",
    "W = [1., 1.76, 2.5]\n",
    "phase_list = ['Ordered', 'Critical', 'Chaotic']\n",
    "layers = np.arange(19, 100, 20)\n",
    "layer = 5\n",
    "num_classes = 10\n",
    "\n",
    "kernel_list = []\n",
    "fx_train_0_list = []\n",
    "fx_test_0_list = []\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "b_std = np.sqrt(b)\n",
    "for w in W:\n",
    "    W_std = np.sqrt(w)\n",
    "\n",
    "    init_fn, apply_fn, kernel_fn = stax.serial(DenseGroup(layer, 1024, W_std, b_std))\n",
    "    \n",
    "    # Inference with a single infinite width / linearized network\n",
    "    apply_fn = jit(apply_fn)\n",
    "    kernel_fn = jit(kernel_fn, static_argnums=(2,))\n",
    "    kernel_list.append(kernel_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(kernel_fn, obj_fn, x_train=None, x_test=None, fx_train_0=0., fx_test_0=0., t=None):\n",
    "    # Kernel\n",
    "    ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "    \n",
    "    if obj_fn == 'train':\n",
    "        return ntk_train_train\n",
    "    elif obj_fn == 'test':\n",
    "        ntk_test_train = kernel_fn(x_test, x_train, 'ntk')\n",
    "        # Prediction\n",
    "        predict_fn = nt.predict.gradient_descent_mse(ntk_train_train, y_train, diag_reg=diag_reg) # no convariance\n",
    "        return predict_fn(t, fx_train_0, fx_test_0, ntk_test_train) # fx_train_0, fx_test_0 = (0, 0) for infinite width\n",
    "    else:\n",
    "        raise ValueError(\"Objective function must be either train(ntk_train_train) or test(predict_fn)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def l2_loss_v1(logits, labels, weighting=1):\n",
    "    \"\"\"\n",
    "    Tensorflow version of L2 loss (without sqrt)\n",
    "    \"\"\"\n",
    "    return np.sum(((logits - labels)**2) * weighting) / 2\n",
    "    \n",
    "@jit\n",
    "def l2_loss_v2(logits, lables):\n",
    "    \"\"\"\n",
    "    Normal L2 loss\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(logits - labels)\n",
    "\n",
    "@jit\n",
    "def cross_entropy_loss(logits, lables):\n",
    "    return -np.sum(logsoftmax(logits) * lables)\n",
    "    \n",
    "@jit\n",
    "def mse_loss(logits, lables):\n",
    "    return 0.5 * np.mean((logits - lables) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loss_adv(x_train, y, kernel_fn, weighting):\n",
    "    # Compute NTK on training data\n",
    "    ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "    loss = - l2_loss_v1(ntk_train_train, y, weighting) # y = matrix of 1 / diagnal\n",
    "    return loss\n",
    "\n",
    "# train_grads_fn = grad(train_loss_adv)\n",
    "train_grads_fn = jit(grad(train_loss_adv), static_argnums=(2,)) # static arg: expanding {if / else} loops for graph mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss_adv_matrix(x_train, x_test, kernel_fn, c, t=None):\n",
    "    # Kernel -> matrix of constant c\n",
    "    # assert type(c) == int\n",
    "    \n",
    "    ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "    ntk_test_train = kernel_fn(x_test, x_train, 'ntk')\n",
    "    \n",
    "    # Θ(test, train)Θ(train, train)^-1\n",
    "    def inv(k):\n",
    "        #inverse with diag_reg\n",
    "        return np.linalg.inv(k + diag_reg * np.eye(k.shape[0]))\n",
    "    \n",
    "    mean_predictor = np.einsum('ij,jk->ik', ntk_test_train, inv(ntk_train_train))\n",
    "    \n",
    "    # Loss\n",
    "    loss = - l2_loss_v1(mean_predictor, np.ones_like(mean_predictor)*c)\n",
    "    return loss\n",
    "\n",
    "test_grads_fn = jit(grad(test_loss_adv_matrix, argnums=0), static_argnums=(2,))\n",
    "test_c_grads_fn = jit(grad(test_loss_adv_matrix, argnums=3), static_argnums=(2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pred_loss_adv(x_train, x_test, y_train, y, kernel_fn, loss='mse', t=None):\n",
    "    \"\"\" update {Kernel_M,N x Kernel_N,N x (dynamic of t)} \"\"\"\n",
    "    \n",
    "    ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "    ntk_test_train = kernel_fn(x_test, x_train, 'ntk')\n",
    "    \n",
    "    # Prediction\n",
    "    predict_fn = nt.predict.gradient_descent_mse(ntk_train_train, y_train, diag_reg=diag_reg) # diag_reg: add to easier inverse\n",
    "    fx = predict_fn(t, 0., 0., ntk_test_train)[1]\n",
    "    \n",
    "    # Loss\n",
    "    if loss == 'cross-entropy':\n",
    "        loss = cross_entropy_loss(fx, y)\n",
    "    elif loss == 'mse':\n",
    "        loss = mse_loss(fx, y)\n",
    "    return loss\n",
    "\n",
    "test_pred_grads_fn = jit(grad(test_pred_loss_adv, argnums=0), static_argnums=(4, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, grads_c_fn=None, x_train=None, y_train=None, x_test=None, \n",
    "                         y=None, t=None, c=None, update_c=False, loss='cross-entropy', loss_weighting=None, phase=None, \n",
    "                         fx_train_0=0., fx_test_0=0., eps=0.3, norm=np.inf, clip_min=None, clip_max=None, targeted=False):\n",
    "    \"\"\"\n",
    "    JAX implementation of the Fast Gradient Method.\n",
    "    :param model_fn: a callable that takes an input tensor and returns the model logits.\n",
    "    :param x: input tensor.\n",
    "    :param eps: epsilon (input variation parameter); see https://arxiv.org/abs/1412.6572.\n",
    "    :param norm: Order of the norm (mimics NumPy). Possible values: np.inf or 2.\n",
    "    :param clip_min: (optional) float. Minimum float value for adversarial example components.\n",
    "    :param clip_max: (optional) float. Maximum float value for adversarial example components.\n",
    "    :param y: (optional) Tensor with one-hot true labels. If targeted is true, then provide the\n",
    "            target one-hot label. Otherwise, only provide this parameter if you'd like to use true\n",
    "            labels when crafting adversarial samples. Otherwise, model predictions are used\n",
    "            as labels to avoid the \"label leaking\" effect (explained in this paper:\n",
    "            https://arxiv.org/abs/1611.01236). Default is None. This argument does not have\n",
    "            to be a binary one-hot label (e.g., [0, 1, 0, 0]), it can be floating points values\n",
    "            that sum up to 1 (e.g., [0.05, 0.85, 0.05, 0.05]).\n",
    "    :param targeted: (optional) bool. Is the attack targeted or untargeted?\n",
    "            Untargeted, the default, will try to make the label incorrect.\n",
    "            Targeted will instead try to move in the direction of being more like y.\n",
    "    :return: a tensor for the adversarial example\n",
    "    \"\"\"\n",
    "    if norm not in [np.inf, 2]:\n",
    "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
    "        \n",
    "    x = x_test\n",
    "    # Obtain y\n",
    "    if obj_fn == 'test':\n",
    "        if y is None:\n",
    "            # Using model predictions as ground truth to avoid label leaking\n",
    "            x_labels = np.argmax(model_fn(kernel_fn, 'test', x_train, x_test, fx_train_0, fx_test_0)[1], 1)\n",
    "            y = one_hot(x_labels, 10)\n",
    "            \n",
    "    elif obj_fn == 'train':\n",
    "        if y is None:\n",
    "            # Compute NTK on training data\n",
    "            ntk_train_train = model_fn(kernel_fn=kernel_fn, obj_fn='train', x_train=x_train)\n",
    "            \n",
    "            # Construct diagonal\n",
    "            if phase == 'ordered':\n",
    "                y = np.ones(ntk_train_train.shape)*100\n",
    "            elif phase == 'chaotic':\n",
    "                y = np.eye(ntk_train_train.shape[0])*100\n",
    "            else:\n",
    "                raise ValueError(\"Phase must be either 'ordered' or 'critical'\")\n",
    "    \n",
    "    # Obtain gradient\n",
    "    # Obj - Θ(train, train)\n",
    "    if obj_fn == 'train':\n",
    "        grads = grads_fn(x_train, y, kernel_fn, loss_weighting)\n",
    "        \n",
    "    # Obj - Θ(test, train)Θ(train, train)^-1\n",
    "    elif obj_fn == 'test_c':\n",
    "        grads = 0\n",
    "        grads_c = 0\n",
    "        for i in range(int(len(x_test)/batch_size)):\n",
    "            grads += grads_fn(x_train, x_test[batch_size*i:batch_size*(i+1)], kernel_fn, c, t)\n",
    "            if update_c is True:\n",
    "                grads_c += grads_c_fn(x_train, x_test[batch_size*i:batch_size*(i+1)], kernel_fn, c, t)\n",
    "                \n",
    "        grads_c = 3e-6 * np.sign(grads_c) # grads_c = 5e-2 * np.sign(grads_c)\n",
    "        \n",
    "        \n",
    "    # Obj - Θ(test, train)Θ(train, train)^-1 y_train\n",
    "    elif obj_fn == 'test':\n",
    "        grads = 0\n",
    "        for i in range(int(len(x_test)/batch_size)):\n",
    "            batch_grads = grads_fn(x_train, \n",
    "                                   x_test[batch_size*i:batch_size*(i+1)], \n",
    "                                   y_train, \n",
    "                                   y[batch_size*i:batch_size*(i+1)], \n",
    "                                   kernel_fn, \n",
    "                                   loss,\n",
    "                                   t)\n",
    "            grads += batch_grads\n",
    "    else:\n",
    "        raise ValueError(\"Objective function must be either train(ntk_train_train) or test(predict_fn)\")\n",
    "\n",
    "    axis = list(range(1, len(grads.shape)))\n",
    "    eps_div = 1e-12\n",
    "    \n",
    "    if norm == np.inf:\n",
    "        perturbation = eps * np.sign(grads)\n",
    "    elif norm == 1:\n",
    "        raise NotImplementedError(\"L_1 norm has not been implemented yet.\")\n",
    "    elif norm == 2:\n",
    "        square = np.maximum(eps_div, np.sum(np.square(grads), axis=axis, keepdims=True))\n",
    "        perturbation = grads / np.sqrt(square)\n",
    "    \n",
    "    adv_x = x + perturbation\n",
    "    \n",
    "    # If clipping is needed, reset all values outside of [clip_min, clip_max]\n",
    "    if (clip_min is not None) or (clip_max is not None):\n",
    "        # We don't currently support one-sided clipping\n",
    "        assert clip_min is not None and clip_max is not None\n",
    "        adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "    \n",
    "    if obj_fn == 'test_c':\n",
    "        c += grads_c\n",
    "        \n",
    "        return adv_x, c\n",
    "    \n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_gradient_descent(model_fn, kernel_fn, obj_fn, grads_fn, grads_c_fn=None, x_train=None, y_train=None,\n",
    "                               x_test=None, y=None, t=None, c=None, update_c=None, loss='cross-entropy', loss_weighting=None, \n",
    "                               phase=None, fx_train_0=0., fx_test_0=0., eps=0.3, eps_iter=0.03, nb_iter=10, norm=np.inf, \n",
    "                               clip_min=None, clip_max=None, targeted=False, rand_init=None, rand_minmax=0.3):\n",
    "    \"\"\"\n",
    "    This class implements either the Basic Iterative Method\n",
    "    (Kurakin et al. 2016) when rand_init is set to 0. or the\n",
    "    Madry et al. (2017) method when rand_minmax is larger than 0.\n",
    "    Paper link (Kurakin et al. 2016): https://arxiv.org/pdf/1607.02533.pdf\n",
    "    Paper link (Madry et al. 2017): https://arxiv.org/pdf/1706.06083.pdf\n",
    "    :param model_fn: a callable that takes an input tensor and returns the model logits.\n",
    "    :param x: input tensor.\n",
    "    :param eps: epsilon (input variation parameter); see https://arxiv.org/abs/1412.6572.\n",
    "    :param eps_iter: step size for each attack iteration\n",
    "    :param nb_iter: Number of attack iterations.\n",
    "    :param norm: Order of the norm (mimics NumPy). Possible values: np.inf or 2.\n",
    "    :param clip_min: (optional) float. Minimum float value for adversarial example components.\n",
    "    :param clip_max: (optional) float. Maximum float value for adversarial example components.\n",
    "    :param y: (optional) Tensor with true labels. If targeted is true, then provide the\n",
    "            target label. Otherwise, only provide this parameter if you'd like to use true\n",
    "            labels when crafting adversarial samples. Otherwise, model predictions are used\n",
    "            as labels to avoid the \"label leaking\" effect (explained in this paper:\n",
    "            https://arxiv.org/abs/1611.01236). Default is None.\n",
    "    :param targeted: (optional) bool. Is the attack targeted or untargeted?\n",
    "            Untargeted, the default, will try to make the label incorrect.\n",
    "            Targeted will instead try to move in the direction of being more like y.\n",
    "    :return: a tensor for the adversarial example\n",
    "    \"\"\"\n",
    "\n",
    "    assert eps_iter <= eps, (eps_iter, eps)\n",
    "    if norm == 1:\n",
    "        raise NotImplementedError(\"It's not clear that FGM is a good inner loop\"\n",
    "                                  \" step for PGD when norm=1, because norm=1 FGM \"\n",
    "                                  \" changes only one pixel at a time. We need \"\n",
    "                                  \" to rigorously test a strong norm=1 PGD \"\n",
    "                                  \"before enabling this feature.\")\n",
    "    if norm not in [np.inf, 2]:\n",
    "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
    "        \n",
    "    x = x_test\n",
    "    # Obtain y\n",
    "    if obj_fn == 'test':\n",
    "        if y is None:\n",
    "            # Using model predictions as ground truth to avoid label leaking\n",
    "            x_labels = np.argmax(model_fn(kernel_fn, 'test', x_train, x_test, fx_train_0, fx_test_0)[1], 1)\n",
    "            y = one_hot(x_labels, 10)\n",
    "            \n",
    "    elif obj_fn == 'train':\n",
    "        if y is None:\n",
    "            # Compute NTK on training data\n",
    "            ntk_train_train = model_fn(kernel_fn=kernel_fn, obj_fn='train', x_train=x_train)\n",
    "            \n",
    "            # Construct diagonal\n",
    "            if phase == 'ordered':\n",
    "                y = np.ones(ntk_train_train.shape)*100\n",
    "            elif phase == 'chaotic':\n",
    "                y = np.eye(ntk_train_train.shape[0])*100\n",
    "            else:\n",
    "                raise ValueError(\"Phase must be either 'ordered' or 'critical'\")\n",
    "        \n",
    "    # Initialize loop variables\n",
    "    if rand_init:\n",
    "        rand_minmax = eps\n",
    "        eta = random.uniform(new_key, x.shape, minval=-rand_minmax, maxval=rand_minmax)\n",
    "    else:\n",
    "        eta = np.zeros_like(x)\n",
    "\n",
    "    # Clip eta\n",
    "    eta = clip_eta(eta, norm, eps)\n",
    "    adv_x = x + eta\n",
    "    if clip_min is not None or clip_max is not None:\n",
    "        adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "        \n",
    "    for i in range(nb_iter):\n",
    "        if update_c is not None and (i+1) % update_c == 0:\n",
    "            adv_x = fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, grads_c_fn, x_train, y_train, adv_x, \n",
    "                                         y, t, c, True, loss, loss_weighting, phase, fx_train_0, fx_test_0, eps_iter, norm, \n",
    "                                         clip_min, clip_max, targeted)\n",
    "        else:\n",
    "            adv_x = fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, grads_c_fn, x_train, y_train, adv_x, \n",
    "                                         y, t, c, False, loss, loss_weighting, phase, fx_train_0, fx_test_0, eps_iter, norm, \n",
    "                                         clip_min, clip_max, targeted)\n",
    "        \n",
    "        if obj_fn == 'test_c':\n",
    "            adv_x, c = adv_x\n",
    "\n",
    "        # Clipping perturbation eta to norm norm ball\n",
    "        eta = adv_x - x\n",
    "        eta = clip_eta(eta, norm, eps)\n",
    "        adv_x = x + eta\n",
    "\n",
    "        # Redo the clipping.\n",
    "        # FGM already did it, but subtracting and re-adding eta can add some\n",
    "        # small numerical error.\n",
    "        if clip_min is not None or clip_max is not None:\n",
    "            adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "\n",
    "    if obj_fn == 'test_c':\n",
    "        return adv_x, c\n",
    "    \n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = 0.05\n",
    "b = 0.18\n",
    "W = [1., 1.76, 2.5]\n",
    "phase_list = ['Ordered', 'Critical', 'Chaotic']\n",
    "layers = np.arange(19, 100, 20)\n",
    "layer = 50\n",
    "num_classes = 10\n",
    "\n",
    "kernel_list = []\n",
    "fx_train_0_list = []\n",
    "fx_test_0_list = []\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "b_std = np.sqrt(b)\n",
    "for w in W:\n",
    "    W_std = np.sqrt(w)\n",
    "\n",
    "    init_fn, apply_fn, kernel_fn = stax.serial(DenseGroup(layer, 1024, W_std, b_std))\n",
    "    \n",
    "    # Inference with a single infinite width / linearized network\n",
    "    apply_fn = jit(apply_fn)\n",
    "    kernel_fn = jit(kernel_fn, static_argnums=(2,))\n",
    "    kernel_list.append(kernel_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, kernel_fn in enumerate(kernel_list):\n",
    "#     print(phase_list[idx])\n",
    "#     print(kernel_fn(x_train, x_train, 'ntk')[:4, :4])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_type = [\"Clean\", \"FGSM\", \"PGD-10\", \"PGD-100\"]\n",
    "\n",
    "####### MNIST #######\n",
    "eps = 0.3\n",
    "eps_iter_10 = 0.04\n",
    "eps_iter_100 = 0.004\n",
    "####### MNIST #######\n",
    "\n",
    "####### CIFAR #######\n",
    "# eps = 16/255\n",
    "# eps_iter_10 = (eps/10)*1.1\n",
    "# eps_iter_100 = (eps/100)*1.1\n",
    "####### CIFAR #######\n",
    "\n",
    "val_size = 1200\n",
    "\n",
    "# x_train_all is on host device\n",
    "x_val = x_train_all[train_size:train_size+val_size]\n",
    "y_val = y_train_all[train_size:train_size+val_size]\n",
    "\n",
    "# to gpu\n",
    "x_val = np.asarray(x_val)\n",
    "y_val = np.asarray(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_predictor(x_train, x_test, kernel_fn, c=None):\n",
    "    \"\"\"\n",
    "    return Θ(test, train)Θ(train, train)^-1 and \n",
    "    || # Θ(test, train)Θ(train, train)^-1 - target ||\n",
    "    \n",
    "    \"\"\"\n",
    "    # Kernel\n",
    "    ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "    ntk_test_train = kernel_fn(x_test, x_train, 'ntk')\n",
    "    \n",
    "    # Θ(test, train)Θ(train, train)^-1\n",
    "    def inv(k):\n",
    "        return np.linalg.inv(k + diag_reg * np.eye(k.shape[0]))\n",
    "    mean_predictor = np.einsum('ij,jk->ik', ntk_test_train, inv(ntk_train_train))\n",
    "    \n",
    "    if c is None:\n",
    "        c = np.mean(mean_predictor)\n",
    "    \n",
    "    # Loss\n",
    "    loss = l2_loss_v1(mean_predictor, np.ones_like(mean_predictor)*c)\n",
    "    return loss, mean_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_predictor_row_col_wise(x_train, x_test, kernel_fn, c=None, row=False, col=False):\n",
    "    \"\"\"\n",
    "    return Θ(test, train)Θ(train, train)^-1 and \n",
    "    || # Θ(test, train)Θ(train, train)^-1 - target ||\n",
    "    \n",
    "    \"\"\"\n",
    "    if not row and not col:\n",
    "        raise ValueError(\"at least one of row or col should be true\")\n",
    "    # Kernel\n",
    "    ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "    ntk_test_train = kernel_fn(x_test, x_train, 'ntk')\n",
    "    \n",
    "    # Θ(test, train)Θ(train, train)^-1\n",
    "    def inv(k):\n",
    "        return np.linalg.inv(k + diag_reg * np.eye(k.shape[0]))\n",
    "    mean_predictor = np.einsum('ij,jk->ik', ntk_test_train, inv(ntk_train_train))\n",
    "    loss = 0.0\n",
    "    if c is None:\n",
    "        if row:\n",
    "            c = np.mean(mean_predictor, axis=1)\n",
    "            c = np.reshape(c, (1, -1))\n",
    "            loss = l2_loss_v1(mean_predictor, np.ones_like(mean_predictor)*c.T)\n",
    "        else:\n",
    "            c = np.mean(mean_predictor, axis=0)\n",
    "            loss = l2_loss_v1(mean_predictor, np.ones_like(mean_predictor)*c)\n",
    "    \n",
    "    return loss, mean_predictor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a = np.asarray([[1.,2.],\n",
    "                [3.,4.]])\n",
    "\n",
    "c = np.mean(a, axis=1)\n",
    "c = np.reshape(c, (1, -1))\n",
    "print(l2_loss_v1(a, np.ones_like(a)*c.T))\n",
    "a = np.asarray([[1.,2.],\n",
    "                [3.,4.]])\n",
    "\n",
    "c = np.mean(a, axis=0)\n",
    "print(l2_loss_v1(a, np.ones_like(a)*c))\n",
    "print(l2_loss_v1(a, np.ones_like(a)*np.mean(a)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "b = 0.05\n",
    "b = 0.18\n",
    "W = [1., 1.76, 2.5]\n",
    "phase_list = ['Ordered', 'Critical', 'Chaotic']\n",
    "layers = np.arange(19, 100, 20)\n",
    "layer = 100\n",
    "num_classes = 10\n",
    "\n",
    "kernel_list = []\n",
    "fx_train_0_list = []\n",
    "fx_test_0_list = []\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "\n",
    "b_std = np.sqrt(b)\n",
    "for w in W:\n",
    "    W_std = np.sqrt(w)\n",
    "\n",
    "    init_fn, apply_fn, kernel_fn = stax.serial(DenseGroup(layer, 1024, W_std, b_std))\n",
    "    \n",
    "    # Inference with a single infinite width / linearized network\n",
    "    apply_fn = jit(apply_fn)\n",
    "    kernel_fn = jit(kernel_fn, static_argnums=(2,))\n",
    "    kernel_list.append(kernel_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(x_train, x_test, model_fn, kernel_fn, t=None, c=0, attack_type=None):\n",
    "    y_train_predict, y_test_predict = model_fn(kernel_fn, 'test', x_train, x_test, t=t)\n",
    "    print(len(y_test_predict))\n",
    "    acc = accuracy(y_test_predict, y_test)\n",
    "    print(\"Robustness({:s}): {:.2f}\".format(attack_type, acc))\n",
    "    \n",
    "    # Mean predictor\n",
    "    l, m = mean_predictor(x_train, x_test, kernel_fn, c)\n",
    "    # print(\"c:{:.8f}, Mean: {:.8f}, Loss: {:.8f}\".format(c_list[idx], np.mean(m), l))\n",
    "    print(m[:5, :5])\n",
    "    print()\n",
    "    print('shape: ', m.shape)\n",
    "    print('mat: ', np.std(m))\n",
    "    print('row: ', np.mean(np.std(m, axis=0)))\n",
    "    print('col: ', np.mean(np.std(m, axis=1)))\n",
    "    return np.mean(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train list gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = None\n",
    "loss_type = ['cross-entropy', 'mse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "Robustness(Clean): 0.86\n",
      "[[ 0.00112736  0.01011385  0.00287164  0.001241    0.00544948]\n",
      " [-0.0015968   0.00132874  0.01696907 -0.00243109 -0.00083482]\n",
      " [-0.00199748  0.01193599 -0.00562409 -0.00149281  0.00288802]\n",
      " [-0.00396522  0.012832    0.0027059  -0.00103998  0.00695626]\n",
      " [ 0.04279609 -0.00182015 -0.00166272 -0.00105851 -0.00062796]]\n",
      "\n",
      "shape:  (256, 256)\n",
      "mat:  0.014842840670167338\n",
      "row:  0.013633210044645383\n",
      "col:  0.014114156922545015\n"
     ]
    }
   ],
   "source": [
    "x_test_list = []\n",
    "c_list = []\n",
    "\n",
    "c_sample = evaluate(x_train, x_test, model_fn=model_fn, kernel_fn=kernel_list[1], t=t, c=0, attack_type='Clean')\n",
    "x_test_list.append(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = c_sample.reshape((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[0.00387535]], dtype=float64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "Robustness(Clean): 0.86\n",
      "[[ 0.00112736  0.01011385  0.00287164  0.001241    0.00544948]\n",
      " [-0.0015968   0.00132874  0.01696907 -0.00243109 -0.00083482]\n",
      " [-0.00199748  0.01193599 -0.00562409 -0.00149281  0.00288802]\n",
      " [-0.00396522  0.012832    0.0027059  -0.00103998  0.00695626]\n",
      " [ 0.04279609 -0.00182015 -0.00166272 -0.00105851 -0.00062796]]\n",
      "\n",
      "shape:  (256, 256)\n",
      "mat:  0.014842840670167338\n",
      "row:  0.013633210044645383\n",
      "col:  0.014114156922545015\n",
      "256\n",
      "Robustness(FGSM): 0.76\n",
      "[[-7.31078699e-04  2.92903614e-03  5.43250651e-03  6.67594540e-04\n",
      "   2.25524591e-03]\n",
      " [-1.96649544e-03 -8.66053704e-04  1.40917228e-02 -1.44332758e-03\n",
      "  -1.68834972e-03]\n",
      " [-1.60995693e-03  8.38280497e-03 -1.84292881e-03 -2.41019468e-03\n",
      "   3.41287823e-03]\n",
      " [-9.78804255e-04  1.15932265e-02  6.06483282e-03 -3.99598156e-03\n",
      "   6.17756399e-03]\n",
      " [ 2.24459080e-02  5.36157801e-05 -2.52951109e-03 -2.44209142e-03\n",
      "  -3.94823496e-03]]\n",
      "\n",
      "shape:  (256, 256)\n",
      "mat:  0.009387926182652663\n",
      "row:  0.008874616558148524\n",
      "col:  0.009327391168835113\n",
      "256\n",
      "Robustness(PGD-10): 0.79\n",
      "[[-4.27901894e-03  5.11996813e-03  9.01032512e-03 -5.15367829e-05\n",
      "   3.60930627e-03]\n",
      " [-2.08809191e-03 -2.77960960e-03  1.54713734e-02 -1.79917349e-03\n",
      "  -1.44740405e-03]\n",
      " [-1.95834147e-03  9.16331273e-03  2.94486954e-03 -2.00664169e-03\n",
      "   8.58974457e-04]\n",
      " [-3.35598291e-03  9.05261816e-03  1.04781300e-02 -2.90572508e-03\n",
      "   5.41068106e-03]\n",
      " [ 4.66775449e-02 -4.74771012e-03 -3.87424073e-03  7.53414041e-04\n",
      "   5.25770315e-03]]\n",
      "\n",
      "shape:  (256, 256)\n",
      "mat:  0.011694672776934098\n",
      "row:  0.01122230402314705\n",
      "col:  0.011388990535636778\n",
      "256\n",
      "Robustness(PGD-100): 0.77\n",
      "[[-0.00378556  0.00500641  0.00938456 -0.00024259  0.00366424]\n",
      " [-0.00220485 -0.00333901  0.01590689 -0.00184902 -0.00117193]\n",
      " [-0.00206089  0.00982352  0.00360694 -0.00178879  0.00101314]\n",
      " [-0.00303937  0.00838751  0.00966503 -0.0028713   0.00474554]\n",
      " [ 0.04561773 -0.00516792 -0.00389606 -0.00019489  0.00524599]]\n",
      "\n",
      "shape:  (256, 256)\n",
      "mat:  0.011710710011113882\n",
      "row:  0.011231255003759827\n",
      "col:  0.011380751703352721\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1):\n",
    "    x_test_list = []\n",
    "    c_list = []\n",
    "    \n",
    "    evaluate(x_train, x_test, model_fn=model_fn, kernel_fn=kernel_list[1], t=t, c=0, attack_type='Clean')\n",
    "    x_test_list.append(x_test)\n",
    "    c_list.append(c)\n",
    "\n",
    "    # FGSM\n",
    "    adv_x, c = fast_gradient_method(model_fn=model_fn, kernel_fn=kernel_list[1], obj_fn='test_c', \n",
    "                                    grads_fn=test_grads_fn, grads_c_fn=test_grads_fn,\n",
    "                                    x_train=x_train, y_train=y_train, x_test=x_test, y=y_test, t=t, c=c,\n",
    "                                    loss=loss_type[0], eps=eps, clip_min=0, clip_max=1)\n",
    "    \n",
    "    evaluate(x_train, adv_x, model_fn=model_fn, kernel_fn=kernel_list[1], t=t, c=c, attack_type='FGSM')\n",
    "    x_test_list.append(adv_x)\n",
    "    c_list.append(c)\n",
    "    \n",
    "    # PGD 10\n",
    "    key, new_key = random.split(key)\n",
    "    c = c_sample.reshape((1,1))\n",
    "    adv_x, c = projected_gradient_descent(model_fn=model_fn, kernel_fn=kernel_list[1], obj_fn='test_c', \n",
    "                                          grads_fn=test_grads_fn, grads_c_fn=test_c_grads_fn, x_train=x_train, \n",
    "                                          y_train=y_train, x_test=x_test, y=y_test, t=t, c=c, update_c=3, \n",
    "                                          loss=loss_type[0], eps=eps, eps_iter=eps_iter_10, nb_iter=10, \n",
    "                                          clip_min=0, clip_max=1, rand_init=None, rand_minmax=eps)\n",
    "    \n",
    "    \n",
    "    evaluate(x_train, adv_x, model_fn=model_fn, kernel_fn=kernel_list[1], t=t, c=c, attack_type='PGD-10')\n",
    "    x_test_list.append(adv_x)\n",
    "    c_list.append(c)\n",
    "    \n",
    "    # PGD 100\n",
    "    key, new_key = random.split(key)\n",
    "    c = c_sample.reshape((1,1))\n",
    "    adv_x, c = projected_gradient_descent(model_fn=model_fn, kernel_fn=kernel_list[1], obj_fn='test_c', \n",
    "                                          grads_fn=test_grads_fn, grads_c_fn=test_c_grads_fn,\n",
    "                                          x_train=x_train, y_train=y_train, x_test=x_test, y=y_test, t=t, c=c, update_c=3, \n",
    "                                          loss=loss_type[0], eps=eps, eps_iter=eps_iter_100, nb_iter=100, \n",
    "                                          clip_min=0, clip_max=1, rand_init=None, rand_minmax=eps)\n",
    "    \n",
    "    evaluate(x_train, adv_x, model_fn=model_fn, kernel_fn=kernel_list[1], t=t, c=c, attack_type='PGD-100')\n",
    "    x_test_list.append(adv_x)\n",
    "    c_list.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f004bb28a20>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test_list[0][0].reshape((28 ,28)), cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f004b890fd0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAOvklEQVR4nO3df4xU9bnH8c8K3H9aAiKGEHB3uUgeQ4hSgtp4DYGYNHqDIv4g5SZXEm9oFX+kCX9ojIkYcxMT2yLxVpJaNkLSCiatZTGm9oYQtX9ICGgKhjxJF9lFAywNxuIfpq5s/5ih2eLO94xzzpkzu8/7lZidmWfPzLPH/XBmz3e+59s1OjoqAJPfFVU3AKA9CDsQBGEHgiDsQBCEHQhiajtfbOHChaMjIyPtfEkglKlTp2pgYKBr3FqeJzaz2yVtkzRF0q/c/fnU94+MjGhoaCjPSwJI6O7ublhr+W28mU2R9AtJd0haLGm9mS1u9fkAlCvP3+w3SfqLu59w979L2i1pTTFtAShanrDPk3RqzP1P6o8B6ECcjQeCyBP2TyVdM+b+/PpjADpQnrPxhyQtMrMFqoX8h5L+q5CuABSu5SO7u49IelTS25KOS3rd3T8qqjEAxco1zu7ub0l6q6BeAJSIE3RAEIQdCIKwA0EQdiAIwg4EQdiBINo6nx3lWL16dWnP/eabb1b22nll9R4NR3YgCMIOBEHYgSAIOxAEYQeCIOxAEF3tXNixp6dnlKvLdhaGziaX7u5uDQ4OjnspaY7sQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEU1wnubzj6Ix1j6/KacWt4sgOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0GEGWfv5HnbGN/27duT9blz5ybrX331VcPayy+/nNz2wIEDyfpE/PxBrrCb2UlJFyR9LWnE3ZcX0BOAEhRxZF/l7n8t4HkAlIi/2YEg8oZ9VNIfzeywmf2oiIYAlCNv2G9192WS7pD0iJmtKKAnACXIFXZ3/7T+dVjSG5JuKqIpAMVrOexm9h0zm37ptqQfSDpWVGMAipXnbPwcSW+Y2aXn+Y27/6GQriqQZ9w0awy/k8dk864bcOxY+t/3JUuWtPzc/f39yXpW79OmTWtY27RpU3Lb66+/Plnftm1bsp4lz+c+Wv19ajns7n5C0g2tbg+gvRh6A4Ig7EAQhB0IgrADQRB2IIgwU1yrHP6q8nLOeYeIsuQZWsuybt26XNs/++yzDWvXXXddctsXX3wxWR8YGGipp2ZwKWkAuRB2IAjCDgRB2IEgCDsQBGEHgiDsQBBhxtnLnIaatW3Zl7F+7LHHGtZ6enqS2+adRtrX15esnzhxomEta3psXlOmTGl528HBwQI76Qwc2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgiEkzzp53LLuTLwd9+PDhZH3ZsmWlvfZDDz2UrF+8eDFZ7+3tbalWhFmzZjWsdXV1Jbc9f/580e00razfRY7sQBCEHQiCsANBEHYgCMIOBEHYgSAIOxDEpBlnr1LZ89U/+uijZD11DfSsOePPPfdcSz11go0bNybrd911V8Pa3r17k9vu3r27pZ6alRorL+v3KTPsZtYnabWkYXdfUn9slqQ9knolnZS0zt0/K6VDAIVo5m38q5Juv+yxJyXtd/dFkvbX7wPoYJlhd/d3JV3+2cE1knbWb++UdHfBfQEoWKsn6Oa4++n67TOS5hTUD4CS5D4b7+6jktJXJQRQuVbDftbM5kpS/etwcS0BKEOrYe+XtKF+e4Ok9DgGgMp1ZV0X3Mxek7RS0mxJZyU9I+n3kl6X1C1pULWht8wJwD09PaNDQ0M5Wx5f2WPdZSpzrvxE3i9Z9u3bl6ynxtLXrFmT3Hbt2rXJ+sjISLJe1fUPuru7NTg4OO5k/cxxdndf36B0W66uALQVH5cFgiDsQBCEHQiCsANBEHYgCKa4TgKTdXjt5ptvTtZ37NiRrM+ePbthbdeuXclts4bWJiKO7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQxKQZZ69ySeWJPM6dtd/K/NmyXvu+++5L1mfOnJmsp6axZi3ZPJH/nzbCkR0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgsi8lHSROvlS0nnGm7O2feKJJ5L1rMsWZz3/jTfemKynXHFF+t/7ixcvJuupZZElKfX/+8CBA8ltr7rqqmQ96/951lj6ZJS6lDRHdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IYtLMZ88ra8z2/vvvb1hbtGhRcttbbrklWT9z5kyyvnz58mQ9z2clssbR834O44MPPmhYy5qPntXb9OnTW+opqsywm1mfpNWSht19Sf2xLZI2SjpX/7an3P2tspoEkF8zR/ZXJf2fpMuX0Njq7j8tvCMApcj8m93d35V0vg29AChRnhN0j5rZn82sz8yuLKwjAKVoNezbJS2UtFTSaUk/K6wjAKVo6Wy8u5+9dNvMXpFU3aVdATSlpSO7mc0dc3etpGPFtAOgLM0Mvb0maaWk2Wb2iaRnJK00s6WSRiWdlPTjEntsSt7rn997773J+gMPPNCwNmPGjOS2586dS9azxrKz1iH/8ssvG9Y+/PDD5LZZTp06lWv7Mn3xxRdVtzChZIbd3deP83D6tw9Ax+HjskAQhB0IgrADQRB2IAjCDgQRZopraoqqlB5ay9LX15es9/f3t/zcZTt06FCyPn/+/GR97969Lb92akllKXu/3Xnnncn6vn37vnVPkxlHdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IYkKNs+dZljnPOLqUHhMuexw96+eeN29ew9rTTz+d3PbIkSPJetY4+sGDB5P1BQsWNKw9+OCDyW2zPr+QNTX46NGjDWsnT55Mblu2PEuAt4ojOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EMaHG2VNWrFiRrGeNhbt7sp5aPjjP+L8k9fb2JuurVq1K1u+5556WX/v9999P1l944YVkPetnT+33rPHkrHH2rPnw77zzTsPa1q1bk9tORhzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiCIrqw5wUXq6ekZHRoaann71Jhu1jXCs+Zlb9myJVlPXT/92muvTW572223Jet5x+lTP9vbb7+d3Hb79u3Jet7eypqbLWXPZ0/5/PPPk/WZM2e2/NxV6u7u1uDgYNd4tWbWZ79G0i5Jc1Rbj/2X7r7NzGZJ2iOpV7U12te5+2dFNQ2gWM28jR+RtNndF0v6vqRHzGyxpCcl7Xf3RZL21+8D6FCZYXf30+5+pH77gqTjkuZJWiNpZ/3bdkq6u6wmAeT3rU7QmVmvpO9JOihpjrufrpfOqPY2H0CHajrsZvZdSb+V9BN3/9vYmruPqvb3PIAO1VTYzWyaakH/tbv/rv7wWTObW6/PlTRcTosAitDM2fguSTskHXf3n48p9UvaIOn5+tfW1+7tACtXrkzWq5wS+dJLLyXrjz/+eMvPnXdoLY+8rz0yMpKsT53a+Nd7xowZyW2zhiQffvjhZL0TNTOf/T8k/beko2b2Yf2xp1QL+etm9j+SBiWtK6dFAEXIDLu7/0nSuIP0ktKfFgHQMfi4LBAEYQeCIOxAEIQdCIKwA0FMqEtJp6ZLDg/n+0xP1jh7yscff5ysv/fee8n6hg0bWn7tLGWPo+eZwpq1bVbva9euTdZ37drVsHbllVcmt73hhhuS9U6e+tsIR3YgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCGJCjbOnbNq0KVlfuHBhruffs2dPw9qFCxeS25Y9plrmWHqZvZf9GYDNmzc3rF199dXJbQcGBopup3Ic2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgiAm1ZHMeE3UsWqr22u6TVRXzydshtWQzR3YgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCKKZ9dmvkbRL0hxJo5J+6e7bzGyLpI2SztW/9Sl3f6usRvPKO65a5lj3ZB5HT+33rJ8773Xl8a+auXjFiKTN7n7EzKZLOmxm/1+vbXX3n5bXHoCiNLM++2lJp+u3L5jZcUnzym4MQLG+1d/sZtYr6XuSDtYfetTM/mxmfWaWXk8HQKWaDruZfVfSbyX9xN3/Jmm7pIWSlqp25P9ZKR0CKERTF5w0s2mqBf3X7v47SXL3s2Pqr0ianDMLgEki88huZl2Sdkg67u4/H/P43DHftlbSseLbA1CUzCmuZnarpPckHZV0sf7wU5LWq/YWflTSSUk/rp/Ma6jKKa55VTlFliGm8U3Waap5pKa4NnM2/k+Sxtu4Y8fUAXwTn6ADgiDsQBCEHQiCsANBEHYgCMIOBBHmUtIYX9lj+IyFtxeXkgZA2IEoCDsQBGEHgiDsQBCEHQiCsANBtHWc3czOSRps2wsC8fS4+9XjFdoadgDV4W08EARhB4Ig7EAQhB0IgrADQRB2IIimVoQpmpndLmmbpCmSfuXuz1fRx3jM7KSkC5K+ljTi7ssr7KVP0mpJw+6+pP7YLEl7JPWqdr3+de7+WYf0tkUdsIx3YpnxSvdd1cuft/3IbmZTJP1C0h2SFktab2aL291HhlXuvrTKoNe9Kun2yx57UtJ+d18kaX/9fhVe1Td7k2rLeC+t/1fV2gKXlhlfLOn7kh6p/45Vve8a9SW1Yb9V8Tb+Jkl/cfcT7v53Sbslramgj47n7u9KOn/Zw2sk7azf3inp7rY2Vdegt47g7qfd/Uj99gVJl5YZr3TfJfpqiyrCPk/SqTH3P1Fnrfc+KumPZnbYzH5UdTPjmDNmma0zqr0l7CQdtYz3ZcuMd8y+q2L5c07QfdOt7r5MtT8zHjGzFVU31Ii7j6r2j1On6KhlvMdZZvyfqtx3VS1/XkXYP5V0zZj78+uPdQR3/7T+dVjSG6r92dFJzl5aQbf+dbjifv7J3c+6+9fuflHSK6pw3423zLg6YN81Wv68HfutirAfkrTIzBaY2b9J+qGk/gr6+AYz+46ZTb90W9IP1HlLUfdL2lC/vUHS3gp7+Redsox3o2XGVfG+q3r580pmvZnZf0p6UbWhtz53/9+2NzEOM/t31Y7mUm1Y8jdV9mZmr0laKWm2pLOSnpH0e0mvS+pWbbrwOndv+4myBr2t1Ldcxruk3hotM35QFe67Ipc/bwVTXIEgOEEHBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0H8A1VefU+v3K2WAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test_list[1][0].reshape((28, 28)), cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f004b8452e8>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test_list[2][0].reshape((28, 28)), cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f004b8730b8>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test_list[3][0].reshape((28, 28)), cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "Robustness(Clean): 0.86\n",
      "[[ 0.00112736  0.01011385  0.00287164  0.001241    0.00544948]\n",
      " [-0.0015968   0.00132874  0.01696907 -0.00243109 -0.00083482]\n",
      " [-0.00199748  0.01193599 -0.00562409 -0.00149281  0.00288802]\n",
      " [-0.00396522  0.012832    0.0027059  -0.00103998  0.00695626]\n",
      " [ 0.04279609 -0.00182015 -0.00166272 -0.00105851 -0.00062796]]\n",
      "\n",
      "256\n",
      "Robustness(FGSM): 0.76\n",
      "[[-7.31078699e-04  2.92903614e-03  5.43250651e-03  6.67594540e-04\n",
      "   2.25524591e-03]\n",
      " [-1.96649544e-03 -8.66053704e-04  1.40917228e-02 -1.44332758e-03\n",
      "  -1.68834972e-03]\n",
      " [-1.60995693e-03  8.38280497e-03 -1.84292881e-03 -2.41019468e-03\n",
      "   3.41287823e-03]\n",
      " [-9.78804255e-04  1.15932265e-02  6.06483282e-03 -3.99598156e-03\n",
      "   6.17756399e-03]\n",
      " [ 2.24459080e-02  5.36157801e-05 -2.52951109e-03 -2.44209142e-03\n",
      "  -3.94823496e-03]]\n",
      "\n",
      "update_c: 1\n",
      "256\n",
      "Robustness(PGD-10): 0.78\n",
      "[[-4.26988120e-03  5.04486080e-03  8.86506402e-03 -5.67124272e-05\n",
      "   3.58916334e-03]\n",
      " [-2.03793332e-03 -2.66613768e-03  1.48294663e-02 -1.70671905e-03\n",
      "  -1.44862185e-03]\n",
      " [-1.92157298e-03  8.91376827e-03  2.74549616e-03 -1.89186644e-03\n",
      "   7.56701282e-04]\n",
      " [-3.09007220e-03  9.22607686e-03  9.98459872e-03 -2.91234233e-03\n",
      "   5.38304388e-03]\n",
      " [ 4.65576209e-02 -4.69364258e-03 -3.97419663e-03  9.19339227e-04\n",
      "   5.44150136e-03]]\n",
      "\n",
      "update_c: 2\n",
      "256\n",
      "Robustness(PGD-10): 0.79\n",
      "[[-4.23890643e-03  5.00793209e-03  9.01556432e-03 -8.31752146e-05\n",
      "   3.54937612e-03]\n",
      " [-2.11184395e-03 -2.85493452e-03  1.52173603e-02 -1.81917791e-03\n",
      "  -1.44862247e-03]\n",
      " [-2.00062343e-03  9.05618524e-03  2.89466672e-03 -1.84878319e-03\n",
      "   8.08477212e-04]\n",
      " [-3.21715270e-03  9.13640904e-03  1.04618002e-02 -2.97854894e-03\n",
      "   5.52078185e-03]\n",
      " [ 4.67493395e-02 -4.75178897e-03 -3.88549446e-03  8.00766522e-04\n",
      "   5.30035378e-03]]\n",
      "\n",
      "update_c: 4\n",
      "256\n",
      "Robustness(PGD-10): 0.79\n",
      "[[-4.30449133e-03  5.09744317e-03  8.98970467e-03 -7.10082384e-05\n",
      "   3.62691650e-03]\n",
      " [-2.09970186e-03 -2.82112209e-03  1.52381998e-02 -1.81803101e-03\n",
      "  -1.44262621e-03]\n",
      " [-2.01511406e-03  9.07160562e-03  2.93156739e-03 -1.94548526e-03\n",
      "   8.12340118e-04]\n",
      " [-3.34015358e-03  9.14335360e-03  1.04837958e-02 -2.89827412e-03\n",
      "   5.42406386e-03]\n",
      " [ 4.66274211e-02 -4.78049277e-03 -3.91254671e-03  7.55464460e-04\n",
      "   5.20295050e-03]]\n",
      "\n",
      "update_c: 8\n",
      "256\n",
      "Robustness(PGD-10): 0.79\n",
      "[[-4.27901894e-03  5.11996813e-03  9.01032512e-03 -5.15367829e-05\n",
      "   3.60930627e-03]\n",
      " [-2.08461823e-03 -2.78891584e-03  1.53403469e-02 -1.79821123e-03\n",
      "  -1.45205895e-03]\n",
      " [-1.95834147e-03  9.16331273e-03  2.94486954e-03 -2.00664169e-03\n",
      "   8.58974457e-04]\n",
      " [-3.35762308e-03  9.05507455e-03  1.04768743e-02 -2.90605518e-03\n",
      "   5.41072238e-03]\n",
      " [ 4.66775449e-02 -4.74771012e-03 -3.87424073e-03  7.53414041e-04\n",
      "   5.25770315e-03]]\n",
      "\n",
      "update_c: 16\n",
      "256\n",
      "Robustness(PGD-10): 0.79\n",
      "[[-4.27901894e-03  5.11996813e-03  9.01032512e-03 -5.15367829e-05\n",
      "   3.60930627e-03]\n",
      " [-2.09015473e-03 -2.78919172e-03  1.53347966e-02 -1.79972713e-03\n",
      "  -1.45281543e-03]\n",
      " [-1.95834147e-03  9.16331273e-03  2.94486954e-03 -2.00664169e-03\n",
      "   8.58974457e-04]\n",
      " [-3.35598291e-03  9.05261816e-03  1.04781300e-02 -2.90572508e-03\n",
      "   5.41068106e-03]\n",
      " [ 4.66775449e-02 -4.74771012e-03 -3.87424073e-03  7.53414041e-04\n",
      "   5.25770315e-03]]\n",
      "\n",
      "update_c: 32\n",
      "256\n",
      "Robustness(PGD-10): 0.79\n",
      "[[-4.27901894e-03  5.11996813e-03  9.01032512e-03 -5.15367829e-05\n",
      "   3.60930627e-03]\n",
      " [-2.09015473e-03 -2.78919172e-03  1.53347966e-02 -1.79972713e-03\n",
      "  -1.45281543e-03]\n",
      " [-1.95834147e-03  9.16331273e-03  2.94486954e-03 -2.00664169e-03\n",
      "   8.58974457e-04]\n",
      " [-3.35598291e-03  9.05261816e-03  1.04781300e-02 -2.90572508e-03\n",
      "   5.41068106e-03]\n",
      " [ 4.66775449e-02 -4.74771012e-03 -3.87424073e-03  7.53414041e-04\n",
      "   5.25770315e-03]]\n",
      "\n",
      "update_c: 64\n",
      "256\n",
      "Robustness(PGD-10): 0.79\n",
      "[[-4.27901894e-03  5.11996813e-03  9.01032512e-03 -5.15367829e-05\n",
      "   3.60930627e-03]\n",
      " [-2.09015473e-03 -2.78919172e-03  1.53347966e-02 -1.79972713e-03\n",
      "  -1.45281543e-03]\n",
      " [-1.95834147e-03  9.16331273e-03  2.94486954e-03 -2.00664169e-03\n",
      "   8.58974457e-04]\n",
      " [-3.35598291e-03  9.05261816e-03  1.04781300e-02 -2.90572508e-03\n",
      "   5.41068106e-03]\n",
      " [ 4.66775449e-02 -4.74771012e-03 -3.87424073e-03  7.53414041e-04\n",
      "   5.25770315e-03]]\n",
      "\n",
      "update_c: 1\n",
      "256\n",
      "Robustness(PGD-100): 0.77\n",
      "[[-0.00379634  0.004873    0.00920179 -0.0001084   0.00369985]\n",
      " [-0.00205058 -0.00293253  0.0149287  -0.00153152 -0.00136389]\n",
      " [-0.00203944  0.00922     0.00358305 -0.00179971  0.00099833]\n",
      " [-0.0027543   0.00869902  0.00972733 -0.00280783  0.00487334]\n",
      " [ 0.04555639 -0.00518328 -0.0036908   0.00017563  0.00487758]]\n",
      "\n",
      "update_c: 2\n",
      "256\n",
      "Robustness(PGD-100): 0.77\n",
      "[[-0.0038381   0.00494069  0.00922167 -0.00018848  0.00363441]\n",
      " [-0.00215703 -0.0031069   0.0151841  -0.0016551  -0.00130205]\n",
      " [-0.00205443  0.00953181  0.00361906 -0.00186893  0.00099636]\n",
      " [-0.00299801  0.00866338  0.00987098 -0.00290066  0.00494965]\n",
      " [ 0.04589252 -0.00513145 -0.00390918 -0.00030252  0.00495618]]\n",
      "\n",
      "update_c: 4\n",
      "256\n",
      "Robustness(PGD-100): 0.77\n",
      "[[-0.00382899  0.00493865  0.00927264 -0.00020785  0.00363281]\n",
      " [-0.00218852 -0.0031856   0.01543218 -0.00171109 -0.00125164]\n",
      " [-0.00207775  0.00962425  0.00364682 -0.00183234  0.00096962]\n",
      " [-0.00302352  0.00849012  0.00973703 -0.00288255  0.00478772]\n",
      " [ 0.04549268 -0.00515077 -0.00390251 -0.0003046   0.00505462]]\n",
      "\n",
      "update_c: 8\n",
      "256\n",
      "Robustness(PGD-100): 0.77\n",
      "[[-0.00381911  0.00501875  0.00930746 -0.00023109  0.003635  ]\n",
      " [-0.00219616 -0.003303    0.0157598  -0.0017953  -0.00119907]\n",
      " [-0.0020495   0.00969527  0.00359163 -0.00176872  0.0009942 ]\n",
      " [-0.00304263  0.00843654  0.0098035  -0.00287393  0.00489788]\n",
      " [ 0.04576252 -0.00513862 -0.00394466 -0.00013327  0.00516404]]\n",
      "\n",
      "update_c: 16\n",
      "256\n",
      "Robustness(PGD-100): 0.77\n",
      "[[-0.00382373  0.00497752  0.00936606 -0.00022318  0.00365272]\n",
      " [-0.00220579 -0.00331299  0.01585182 -0.00182191 -0.00118829]\n",
      " [-0.00204699  0.00976228  0.00361869 -0.0017937   0.00101154]\n",
      " [-0.00305109  0.00829408  0.00953219 -0.00288099  0.00468737]\n",
      " [ 0.04561516 -0.005193   -0.00395485 -0.00017675  0.00518039]]\n",
      "\n",
      "update_c: 32\n",
      "256\n",
      "Robustness(PGD-100): 0.77\n",
      "[[-0.00381086  0.00496071  0.00935992 -0.00022003  0.00365876]\n",
      " [-0.00221109 -0.00332996  0.01585289 -0.00184543 -0.00118151]\n",
      " [-0.00207583  0.00980521  0.00362698 -0.00178276  0.00101474]\n",
      " [-0.00303913  0.00833587  0.00960523 -0.00288978  0.00478575]\n",
      " [ 0.04556293 -0.00516235 -0.00395271 -0.00016509  0.00512274]]\n",
      "\n",
      "update_c: 64\n",
      "256\n",
      "Robustness(PGD-100): 0.77\n",
      "[[-0.00379231  0.00499878  0.00935022 -0.00023672  0.00367542]\n",
      " [-0.00221304 -0.00333053  0.01585969 -0.00184167 -0.001178  ]\n",
      " [-0.00207505  0.00982344  0.00361201 -0.00178757  0.00101577]\n",
      " [-0.00307438  0.00837018  0.00967654 -0.00285265  0.00483599]\n",
      " [ 0.04555642 -0.00515465 -0.003959   -0.00015984  0.0051606 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = None\n",
    "c = np.zeros((1, train_size)) + 0.00387526\n",
    "update_c_list = [1, 2, 4, 8, 16, 32, 64]\n",
    "loss_type = ['cross-entropy', 'mse']\n",
    "\n",
    "x_test_list = []\n",
    "c_list = []\n",
    "    \n",
    "\n",
    "# Clean    \n",
    "evaluate(x_train, x_test, model_fn=model_fn, kernel_fn=kernel_list[1], t=t, c=0, attack_type='Clean')\n",
    "x_test_list.append(x_test)\n",
    "c_list.append(c)\n",
    "\n",
    "\n",
    "# FGSM\n",
    "adv_x, c = fast_gradient_method(model_fn=model_fn, kernel_fn=kernel_list[1], obj_fn='test_c', \n",
    "                                grads_fn=test_col_wise_grads_fn, grads_c_fn=test_col_wise_c_grads_fn,\n",
    "                                x_train=x_train, y_train=y_train, x_test=x_test, y=y_test, t=t, c=c,\n",
    "                                loss=loss_type[0], eps=eps, clip_min=0, clip_max=1)\n",
    "\n",
    "evaluate(x_train, adv_x, model_fn=model_fn, kernel_fn=kernel_list[1], t=t, c=c, attack_type='FGSM')\n",
    "x_test_list.append(adv_x)\n",
    "c_list.append(c)\n",
    "\n",
    "for update_c in update_c_list:\n",
    "    # PGD 10\n",
    "    print('update_c:', update_c)\n",
    "    key, new_key = random.split(key)\n",
    "    c = np.zeros((1, train_size)) + 0.00387526\n",
    "    adv_x, c = projected_gradient_descent(model_fn=model_fn, kernel_fn=kernel_list[1], obj_fn='test_c', \n",
    "                                          grads_fn=test_col_wise_grads_fn, grads_c_fn=test_col_wise_c_grads_fn,\n",
    "                                          x_train=x_train, y_train=y_train, x_test=x_test, y=y_test, t=t, c=c, update_c=update_c, \n",
    "                                          loss=loss_type[0], eps=eps, eps_iter=eps_iter_10, nb_iter=10, \n",
    "                                          clip_min=0, clip_max=1, rand_init=None, rand_minmax=eps)\n",
    "    \n",
    "    evaluate(x_train, adv_x, model_fn=model_fn, kernel_fn=kernel_list[1], t=t, c=c, attack_type='PGD-10')\n",
    "    x_test_list.append(adv_x)\n",
    "    c_list.append(c)\n",
    "\n",
    "for update_c in update_c_list:\n",
    "    # PGD 100\n",
    "    print('update_c:', update_c)\n",
    "    key, new_key = random.split(key)\n",
    "    c = np.zeros((1, train_size)) + 0.00387526\n",
    "    adv_x, c = projected_gradient_descent(model_fn=model_fn, kernel_fn=kernel_list[1], obj_fn='test_c', \n",
    "                                          grads_fn=test_col_wise_grads_fn, grads_c_fn=test_col_wise_c_grads_fn,\n",
    "                                          x_train=x_train, y_train=y_train, x_test=x_test, y=y_test, t=t, c=c, update_c=update_c, \n",
    "                                          loss=loss_type[0], eps=eps, eps_iter=eps_iter_100, nb_iter=100, \n",
    "                                          clip_min=0, clip_max=1, rand_init=None, rand_minmax=eps)\n",
    "    \n",
    "    evaluate(x_train, adv_x, model_fn=model_fn, kernel_fn=kernel_list[1], t=t, c=c, attack_type='PGD-100')\n",
    "    x_test_list.append(adv_x)\n",
    "    c_list.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
