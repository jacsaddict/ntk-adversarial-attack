{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as onp\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "\n",
    "from jax import lax, random\n",
    "from jax.api import grad, jit, vmap\n",
    "from jax.config import config\n",
    "from jax.experimental import optimizers\n",
    "from jax.experimental.stax import logsoftmax\n",
    "\n",
    "config.update('jax_enable_x64', True)\n",
    "\n",
    "from neural_tangents import stax\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "# Attacking\n",
    "from cleverhans.utils import clip_eta, one_hot\n",
    "\n",
    "# Plotting\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from utils import *\n",
    "\n",
    "sns.set_style(style='white')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "\"\"\"\n",
    "diag_reg:\n",
    "    a scalar representing the strength of the diagonal regularization for\n",
    "    `k_train_train`, i.e. computing `k_train_train + diag_reg * I` during\n",
    "    Cholesky factorization or eigendecomposition.\n",
    "\"\"\"\n",
    "diag_reg = 1e-5\n",
    "\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.0\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1, 4, 16, 64, 256, 1024, 4096, 16384, 65536]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'cifar10'\n",
    "class_num   = 10\n",
    "image_shape = None\n",
    "\n",
    "train_size = 4096\n",
    "test_size = 64\n",
    "test_batch_size  = 16\n",
    "time = 65536\n",
    "eps = 0.1\n",
    "\n",
    "if DATASET =='mnist':\n",
    "    image_shape = (28, 28)\n",
    "elif DATASET == 'cifar10':\n",
    "    image_shape = (32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_all, y_train_all, x_test_all, y_test_all = tuple(onp.array(x) for x in get_dataset(DATASET, None, None, \n",
    "                                                                                  do_flatten_and_normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "seed = 0\n",
    "x_train_all, y_train_all = shaffle(x_train_all, y_train_all, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# down sample\n",
    "x_train = x_train_all[:train_size]\n",
    "y_train = y_train_all[:train_size]\n",
    "\n",
    "x_test = x_test_all[:test_size]\n",
    "y_test = y_test_all[:test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train.reshape((-1, *image_shape)), x_test.reshape((-1, *image_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(mean, ys):\n",
    "    return onp.argmax(mean, axis=-1) == onp.argmax(ys, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvBlock(channels, W_std, b_std, strides=(1,1)):\n",
    "    return stax.serial(stax.Conv(out_chan=channels, filter_shape=(3,3), strides=strides, padding='SAME',\n",
    "                                 W_std=W_std, b_std=b_std), \n",
    "                       stax.Relu(do_backprop=True))\n",
    "\n",
    "def ConvGroup(n, channels, stride, W_std, b_std, last_stride=False):\n",
    "    blocks = []\n",
    "    if last_stride:\n",
    "        for i in range(n-1):\n",
    "            blocks += [ConvBlock(channels, W_std, b_std, stride)]\n",
    "        blocks += [ConvBlock(channels, W_std, b_std, (2, 2))]\n",
    "    \n",
    "    else:\n",
    "        for i in range(n):\n",
    "            blocks += [ConvBlock(channels, W_std, b_std, stride)]\n",
    "        \n",
    "    return stax.serial(*blocks)\n",
    "        \n",
    "def VGG19_stride(class_num=class_num):\n",
    "    \n",
    "    return stax.serial(\n",
    "        ConvGroup(n=2, channels=64 , stride=(1,1), W_std=1.76, b_std=0.18, last_stride=True),\n",
    "        ConvGroup(n=2, channels=128, stride=(1,1), W_std=1.76, b_std=0.18, last_stride=True),\n",
    "        ConvGroup(n=4, channels=256, stride=(1,1), W_std=1.76, b_std=0.18, last_stride=True),\n",
    "        ConvGroup(n=4, channels=512, stride=(1,1), W_std=1.76, b_std=0.18, last_stride=True),\n",
    "        ConvGroup(n=4, channels=512, stride=(1,1), W_std=1.76, b_std=0.18, last_stride=True),\n",
    "        stax.Flatten(),\n",
    "        stax.Dense(4096), stax.Relu(do_backprop=True),\n",
    "        stax.Dense(4096), stax.Relu(do_backprop=True),\n",
    "        stax.Dense(class_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_fn, apply_fn, kernel_fn = VGG19_stride(class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_kernel_fn = nt.batch(kernel_fn, batch_size=256, store_on_device=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel_train_m = batch_kernel_fn(x_train, None, 'ntk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_train_m = onp.load('../kernel_vgg_4096.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(kernel_fn, x_train=None, x_test=None, fx_train_0=0., fx_test_0=0., t=None, ntk_train_train=None):\n",
    "    # Kernel\n",
    "    if ntk_train_train is None:\n",
    "        ntk_train_train = kernel_fn(x_train, x_train, 'ntk')\n",
    "    \n",
    "    ntk_test_train = kernel_fn(x_test, x_train, 'ntk')\n",
    "    # Prediction\n",
    "    predict_fn = nt.predict.gradient_descent_mse(ntk_train_train, y_train, diag_reg=diag_reg) # no convariance\n",
    "    \n",
    "    return predict_fn(t, fx_train_0, fx_test_0, ntk_test_train) # fx_train_0, fx_test_0 = (0, 0) for infinite width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def l2_loss_v1(logits, labels, weighting=1):\n",
    "    \"\"\"\n",
    "    Tensorflow version of L2 loss (without sqrt)\n",
    "    \"\"\"\n",
    "    return np.sum(((logits - labels)**2) * weighting) / 2\n",
    "    \n",
    "@jit\n",
    "def l2_loss_v2(logits, lables):\n",
    "    \"\"\"\n",
    "    Normal L2 loss\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(logits - labels)\n",
    "\n",
    "@jit\n",
    "def cross_entropy_loss(logits, lables):\n",
    "    return -np.sum(logsoftmax(logits) * lables)\n",
    "    \n",
    "@jit\n",
    "def mse_loss(logits, lables):\n",
    "    return 0.5 * np.mean((logits - lables) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attack algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, x_train=None, y_train=None, x_test=None, \n",
    "                         y=None, t=None, loss_weighting=None, fx_train_0=0., fx_test_0=0., eps=0.3, \n",
    "                         norm=np.inf, clip_min=None, clip_max=None, targeted=False):\n",
    "    \"\"\"\n",
    "    JAX implementation of the Fast Gradient Method.\n",
    "    :param model_fn: a callable that takes an input tensor and returns the model logits.\n",
    "    :param x: input tensor.\n",
    "    :param eps: epsilon (input variation parameter); see https://arxiv.org/abs/1412.6572.\n",
    "    :param norm: Order of the norm (mimics NumPy). Possible values: np.inf or 2.\n",
    "    :param clip_min: (optional) float. Minimum float value for adversarial example components.\n",
    "    :param clip_max: (optional) float. Maximum float value for adversarial example components.\n",
    "    :param y: (optional) Tensor with one-hot true labels. If targeted is true, then provide the\n",
    "            target one-hot label. Otherwise, only provide this parameter if you'd like to use true\n",
    "            labels when crafting adversarial samples. Otherwise, model predictions are used\n",
    "            as labels to avoid the \"label leaking\" effect (explained in this paper:\n",
    "            https://arxiv.org/abs/1611.01236). Default is None. This argument does not have\n",
    "            to be a binary one-hot label (e.g., [0, 1, 0, 0]), it can be floating points values\n",
    "            that sum up to 1 (e.g., [0.05, 0.85, 0.05, 0.05]).\n",
    "    :param targeted: (optional) bool. Is the attack targeted or untargeted?\n",
    "            Untargeted, the default, will try to make the label incorrect.\n",
    "            Targeted will instead try to move in the direction of being more like y.\n",
    "    :return: a tensor for the adversarial example\n",
    "    \"\"\"\n",
    "    if norm not in [np.inf, 2]:\n",
    "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
    "        \n",
    "    x = x_test\n",
    "        \n",
    "    # test independent\n",
    "    if obj_fn == 'untargeted':\n",
    "        grads = grads_fn(x_train, x_test, y_train, y, kernel_fn, t)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Objective function must be either train(ntk_train_train) or test(predict_fn)\")\n",
    "\n",
    "    axis = list(range(1, len(grads.shape)))\n",
    "    eps_div = 1e-12\n",
    "    \n",
    "    if norm == np.inf:\n",
    "        perturbation = eps * np.sign(grads)\n",
    "    elif norm == 1:\n",
    "        raise NotImplementedError(\"L_1 norm has not been implemented yet.\")\n",
    "    elif norm == 2:\n",
    "        square = np.maximum(eps_div, np.sum(np.square(grads), axis=axis, keepdims=True))\n",
    "        perturbation = grads / np.sqrt(square)\n",
    "    \n",
    "    # TODO\n",
    "    adv_x = x + perturbation\n",
    "    \n",
    "    # If clipping is needed, reset all values outside of [clip_min, clip_max]\n",
    "    if (clip_min is not None) or (clip_max is not None):\n",
    "        # We don't currently support one-sided clipping\n",
    "        assert clip_min is not None and clip_max is not None\n",
    "        adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "    \n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, x_train=None, y_train=None,\n",
    "                               x_test=None, y=None, t=None, loss_weighting=None, fx_train_0=0., fx_test_0=0., \n",
    "                               eps=0.3, eps_iter=0.03, nb_iter=10, norm=np.inf, clip_min=None, clip_max=None, \n",
    "                               targeted=False, rand_init=None, rand_minmax=0.3):\n",
    "    \"\"\"\n",
    "    This class implements either the Basic Iterative Method\n",
    "    (Kurakin et al. 2016) when rand_init is set to 0. or the\n",
    "    Madry et al. (2017) method when rand_minmax is larger than 0.\n",
    "    Paper link (Kurakin et al. 2016): https://arxiv.org/pdf/1607.02533.pdf\n",
    "    Paper link (Madry et al. 2017): https://arxiv.org/pdf/1706.06083.pdf\n",
    "    :param model_fn: a callable that takes an input tensor and returns the model logits.\n",
    "    :param x: input tensor.\n",
    "    :param eps: epsilon (input variation parameter); see https://arxiv.org/abs/1412.6572.\n",
    "    :param eps_iter: step size for each attack iteration\n",
    "    :param nb_iter: Number of attack iterations.\n",
    "    :param norm: Order of the norm (mimics NumPy). Possible values: np.inf or 2.\n",
    "    :param clip_min: (optional) float. Minimum float value for adversarial example components.\n",
    "    :param clip_max: (optional) float. Maximum float value for adversarial example components.\n",
    "    :param y: (optional) Tensor with true labels. If targeted is true, then provide the\n",
    "            target label. Otherwise, only provide this parameter if you'd like to use true\n",
    "            labels when crafting adversarial samples. Otherwise, model predictions are used\n",
    "            as labels to avoid the \"label leaking\" effect (explained in this paper:\n",
    "            https://arxiv.org/abs/1611.01236). Default is None.\n",
    "    :param targeted: (optional) bool. Is the attack targeted or untargeted?\n",
    "            Untargeted, the default, will try to make the label incorrect.\n",
    "            Targeted will instead try to move in the direction of being more like y.\n",
    "    :return: a tensor for the adversarial example\n",
    "    \"\"\"\n",
    "\n",
    "    assert eps_iter <= eps, (eps_iter, eps)\n",
    "    if norm == 1:\n",
    "        raise NotImplementedError(\"It's not clear that FGM is a good inner loop\"\n",
    "                                  \" step for PGD when norm=1, because norm=1 FGM \"\n",
    "                                  \" changes only one pixel at a time. We need \"\n",
    "                                  \" to rigorously test a strong norm=1 PGD \"\n",
    "                                  \"before enabling this feature.\")\n",
    "    if norm not in [np.inf, 2]:\n",
    "        raise ValueError(\"Norm order must be either np.inf or 2.\")\n",
    "        \n",
    "    x = x_test\n",
    "    \n",
    "    # Initialize loop variables\n",
    "    if rand_init:\n",
    "        rand_minmax = eps\n",
    "        eta = random.uniform(new_key, x.shape, minval=-rand_minmax, maxval=rand_minmax)\n",
    "    else:\n",
    "        eta = np.zeros_like(x)\n",
    "\n",
    "    # Clip eta\n",
    "    eta = clip_eta(eta, norm, eps)\n",
    "    adv_x = x + eta\n",
    "    if clip_min is not None or clip_max is not None:\n",
    "        adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "        \n",
    "    for i in range(nb_iter):\n",
    "        adv_x = fast_gradient_method(model_fn, kernel_fn, obj_fn, grads_fn, x_train, y_train, adv_x, \n",
    "                                        y, t, loss_weighting, fx_train_0, fx_test_0, eps_iter, norm, \n",
    "                                        clip_min, clip_max, targeted)\n",
    "\n",
    "        # Clipping perturbation eta to norm norm ball\n",
    "        eta = adv_x - x\n",
    "        eta = clip_eta(eta, norm, eps)\n",
    "        adv_x = x + eta\n",
    "\n",
    "        # Redo the clipping.\n",
    "        # FGM already did it, but subtracting and re-adding eta can add some\n",
    "        # small numerical error.\n",
    "        if clip_min is not None or clip_max is not None:\n",
    "            adv_x = np.clip(adv_x, a_min=clip_min, a_max=clip_max)\n",
    "    \n",
    "    return adv_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == 'mnist':\n",
    "    eps = 0.3\n",
    "    eps_iter_10 = (eps/10)*1.1\n",
    "    eps_iter_100 = (eps/100)*1.1\n",
    "    eps_iter_1000 = (eps/1000)*1.1\n",
    "    \n",
    "elif DATASET == 'cifar10':\n",
    "    eps = eps\n",
    "    eps_iter_10 = (eps/10)*1.1\n",
    "    # eps_iter_100 = (eps/100)*1.1\n",
    "    eps_iter_100 = 1/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(x_train, x_test, y_test, model_fn, kernel_fn, t=None, attack_type=None, ntk_train_train=None):\n",
    "    \n",
    "    y_train_predict, y_test_predict = model_fn(kernel_fn, x_train, x_test, \n",
    "                                               t=t, ntk_train_train=ntk_train_train)\n",
    "    \n",
    "    selected_table = correct(y_test_predict, y_test)\n",
    "    print(\"Accuray({:s}): {:.2f}\".format(attack_type, onp.mean(selected_table)))\n",
    "    print(\"Loss({:s}): {:.4f}\".format('cross entropy', cross_entropy_loss(y_test, y_test_predict)))\n",
    "    print(\"Loss({:s}): {:.4f}\".format('mse', mse_loss(y_test, y_test_predict)))\n",
    "    return selected_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_robustness(x_train, x_test, y_test, model_fn, kernel_fn, selected_table, t=None, \n",
    "                        attack_type=None, ntk_train_train=None):\n",
    "    \n",
    "    y_train_predict, y_test_predict = model_fn(kernel_fn, x_train, x_test,\n",
    "                                               t=t, ntk_train_train=ntk_train_train)\n",
    "    \n",
    "    y_test_predict = onp.asarray(y_test_predict)\n",
    "    y_test_predict_select = y_test_predict[onp.asarray(selected_table)]\n",
    "    y_test_select = y_test[onp.asarray(selected_table)]\n",
    "    print(\"Robustness({:s}): {:.2f}\".format(attack_type, onp.mean(correct(y_test_predict_select, y_test_select))))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adv_x generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv(k):\n",
    "        #inverse with diag_reg\n",
    "        return onp.linalg.inv(k + diag_reg * onp.eye(k.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_train_inv_m = inv(kernel_train_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_adv_x(kernel_fn, x_test, y_test, t=None):\n",
    "    \n",
    "    kernel_fn = jit(kernel_fn, static_argnums=(2,))\n",
    "    \n",
    "    def inv(k):\n",
    "        #inverse with diag_reg\n",
    "        return onp.linalg.inv(k + diag_reg * onp.eye(k.shape[0]))\n",
    "    \n",
    "    ntk_train_train     = kernel_train_m\n",
    "    ntk_train_train_inv = kernel_train_inv_m\n",
    "    \n",
    "    \n",
    "    def test_loss_adv_mse(x_train, x_test, y_train, y, kernel_fn, t=None, diag_reg=diag_reg,\n",
    "                          ntk_train_train=ntk_train_train):\n",
    "        \n",
    "        ntk_test_train = kernel_fn(x_test[None], x_train, 'ntk')\n",
    "\n",
    "        predict_fn = nt.predict.gradient_descent_mse(ntk_train_train, y_train, diag_reg=diag_reg)\n",
    "        # predict_fn(t, train_0, test_0, kernel_matrix)\n",
    "        pred = predict_fn(t, 0., 0., ntk_test_train)[1]\n",
    "        \n",
    "        # loss = -mse_loss(pred, y)\n",
    "        loss = -np.sum(logsoftmax(pred) * y)\n",
    "        return loss\n",
    "    \n",
    "    test_mse_grads_fn = jit(vmap(grad(test_loss_adv_mse, argnums=1), in_axes=(None, 0, None, 0, None, None), \n",
    "                                 out_axes=0), static_argnums=(4,))\n",
    "    \n",
    "    #clean\n",
    "    # print('evaluateing clean...')\n",
    "    # selected_table = evaluate_accuracy(x_train, x_test, y_test, model_fn=model_fn,\n",
    "    #                                    kernel_fn=kernel_fn, t=t, attack_type='Clean', ntk_train_train=ntk_train_train)\n",
    "\n",
    "    # FGSM\n",
    "    print('generating FGSM data...')\n",
    "    adv_x_FGSM = fast_gradient_method(model_fn=model_fn, kernel_fn=kernel_fn, obj_fn='untargeted', \n",
    "                                      grads_fn=test_mse_grads_fn, x_train=x_train, y_train=y_train, \n",
    "                                      x_test=x_test, y=y_test, t=t, eps=eps, clip_min=0, clip_max=1)\n",
    "    # print('evaluateing FGSM...')\n",
    "    # evaluate_accuracy(x_train, adv_x_FGSM, y_test, model_fn=model_fn,\n",
    "    #                     kernel_fn=kernel_fn, t=t, attack_type='FGSM', ntk_train_train=ntk_train_train)\n",
    "    # \n",
    "    # evaluate_robustness(x_train, adv_x_FGSM, y_test, model_fn=model_fn,\n",
    "    #                     kernel_fn=kernel_fn, selected_table=selected_table, t=t, \n",
    "    #                     attack_type='FGSM', ntk_train_train=ntk_train_train)\n",
    "\n",
    "    # I-FGSM 100\n",
    "    print('generating I-FGSM-100 data...')\n",
    "    adv_x_IFGSM_100 = iter_fast_gradient_method(model_fn=model_fn, kernel_fn=kernel_fn, obj_fn='untargeted', \n",
    "                                                 grads_fn=test_mse_grads_fn, x_train=x_train, \n",
    "                                                 y_train=y_train, x_test=x_test, y=y_test, t=t, \n",
    "                                                 eps=eps, eps_iter=eps_iter_100, nb_iter=100, \n",
    "                                                 clip_min=0, clip_max=1, rand_init=None, rand_minmax=eps)\n",
    "    # print('evaluateing I-FGSM-100...')\n",
    "    # evaluate_accuracy(x_train, adv_x_IFGSM_100, y_test, model_fn=model_fn,\n",
    "    #                     kernel_fn=kernel_fn, t=t, attack_type='I-FGSM-100', ntk_train_train=ntk_train_train)\n",
    "    # \n",
    "    # evaluate_robustness(x_train, adv_x_IFGSM_100, y_test, model_fn=model_fn,\n",
    "    #                     kernel_fn=kernel_fn, selected_table=selected_table, t=t, \n",
    "    #                     attack_type='I-FGSM-100', ntk_train_train=ntk_train_train)\n",
    "\n",
    "    return adv_x_FGSM, adv_x_IFGSM_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch number 0\n",
      "generating FGSM data...\n",
      "generating I-FGSM-100 data...\n",
      "-------------------------\n",
      "batch number 1\n",
      "generating FGSM data...\n",
      "generating I-FGSM-100 data...\n",
      "-------------------------\n",
      "batch number 2\n",
      "generating FGSM data...\n",
      "generating I-FGSM-100 data...\n",
      "-------------------------\n",
      "batch number 3\n",
      "generating FGSM data...\n",
      "generating I-FGSM-100 data...\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "adv_x_FGSM, adv_x_IFGSM_100 = [], []\n",
    "for batch_id in range(test_size//test_batch_size):\n",
    "    print(\"batch number %d\"%(batch_id))\n",
    "    fgsm, ifgsm = gen_adv_x(kernel_fn, \n",
    "                            x_test[batch_id*test_batch_size:(batch_id+1)*test_batch_size], \n",
    "                            y_test[batch_id*test_batch_size:(batch_id+1)*test_batch_size],\n",
    "                            time)\n",
    "    adv_x_FGSM.append(fgsm)\n",
    "    adv_x_IFGSM_100.append(ifgsm)\n",
    "    print(\"-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "FGSM = onp.concatenate(adv_x_FGSM, axis=0)\n",
    "IFGSM_100 = onp.concatenate(adv_x_IFGSM_100, axis=0)\n",
    "\n",
    "onp.save(\"cifar-FGSM-eps=%s-t=%s.npy\"%(eps, time), FGSM)\n",
    "onp.save(\"cifar-IFGSM-eps=%s-t=%s.npy\"%(eps, time), IFGSM_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluateing clean...\n",
      "Accuray(Clean): 0.50\n",
      "Loss(cross entropy): 136.8915\n",
      "Loss(mse): 0.0332\n",
      "-------------------------\n",
      "evaluateing FGSM...\n",
      "Accuray(FGSM): 0.00\n",
      "Loss(cross entropy): 173.2489\n",
      "Loss(mse): 0.0943\n",
      "Robustness(FGSM): 0.00\n",
      "-------------------------\n",
      "evaluateing I-FGSM-100...\n",
      "Accuray(I-FGSM-100): 0.00\n",
      "Loss(cross entropy): 176.7436\n",
      "Loss(mse): 0.1056\n",
      "Robustness(I-FGSM-100): 0.00\n"
     ]
    }
   ],
   "source": [
    "print('evaluateing clean...')\n",
    "selected_table = evaluate_accuracy(x_train, x_test, y_test, model_fn=model_fn,\n",
    "                                   kernel_fn=kernel_fn, t=time, attack_type='Clean', ntk_train_train=kernel_train_m)\n",
    "print(\"-------------------------\")\n",
    "print('evaluateing FGSM...')\n",
    "evaluate_accuracy(x_train, FGSM, y_test, model_fn=model_fn,\n",
    "                    kernel_fn=kernel_fn, t=time, attack_type='FGSM', ntk_train_train=kernel_train_m)\n",
    "\n",
    "evaluate_robustness(x_train, FGSM, y_test, model_fn=model_fn,\n",
    "                    kernel_fn=kernel_fn, selected_table=selected_table, t=time, \n",
    "                    attack_type='FGSM', ntk_train_train=kernel_train_m)\n",
    "print(\"-------------------------\")\n",
    "print('evaluateing I-FGSM-100...')\n",
    "evaluate_accuracy(x_train, IFGSM_100, y_test, model_fn=model_fn,\n",
    "                    kernel_fn=kernel_fn, t=time, attack_type='I-FGSM-100', ntk_train_train=kernel_train_m)\n",
    "\n",
    "evaluate_robustness(x_train, IFGSM_100, y_test, model_fn=model_fn,\n",
    "                    kernel_fn=kernel_fn, selected_table=selected_table, t=time, \n",
    "                    attack_type='I-FGSM-100', ntk_train_train=kernel_train_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntk-env",
   "language": "python",
   "name": "ntk-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
